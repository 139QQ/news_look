
## PROJECT OVERVIEW

This is the NewsLook project, a financial news crawler system designed to collect news from various financial websites, including Eastmoney, Sina Finance, Tencent Finance, Netease Finance, and Ifeng Finance.  The project uses a modular design with clear interfaces and functional separation for easy expansion and maintenance. It supports three main operating modes: crawler mode, scheduler mode, and web application mode, all accessible through the unified entry script `run.py`.  Recent updates include significant database optimizations and enhanced encoding handling to improve stability and performance.  The project now intelligently handles multiple database files for each news source, allowing for the storage and retrieval of historical data.  The system now uses a single database file per news source, simplifying data access and management.


## CODE STYLE

*   Follow PEP 8 guidelines.
*   Use type annotations for all functions.
*   Maintain consistent coding style throughout the project.


## FOLDER ORGANIZATION

```
NewsLook
├── api/              # API 接口
├── app/              # 应用逻辑
│   ├── crawlers/     # 爬虫模块
│   ├── tasks/        # 任务调度模块
│   ├── utils/        # 工具函数
│   └── web/          # Web 应用
├── data/             # 数据存储
│   └── db/           # 数据库文件
├── docs/             # 文档
├── requirements/    # 权限包
├── scripts/          # 运行脚本
├── tests/            # 测试
├── static/           # 静态文件
└── templates/        # 模板文件

```

## TECH STACK

*   Python 3.13.0
*   Flask
*   SQLAlchemy (or sqlite3)
*   Requests
*   BeautifulSoup
*   Selenium
*   jieba
*   fake-useragent
*   webdriver-manager


## PROJECT-SPECIFIC STANDARDS

*   All news data must be stored in a consistent format.
*   Use UTF-8 encoding for all files.
*   All functions must have docstrings.
*   All database interactions must be handled within try-except blocks.

## WORKFLOW & RELEASE RULES

*   Use Git for version control.
*   Create new branches for each feature.
*   Write clear and concise commit messages.
*   Conduct thorough testing before releasing new versions.
*   Implement a robust logging system to track errors and debug issues.

## REFERENCE EXAMPLES

*   [Example of a function with type annotations](#example-a-function-with-type-annotations)

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

*   All documentation should be written in Markdown format.
*   Use consistent formatting throughout the documentation.
*   Maintain a README.md file in the root directory.
*   Document all functions and classes.
*   Use diagrams to illustrate complex systems.
*   Regularly update the project documentation to reflect changes in the codebase.
*   [Database Connection Details](docs/数据库连接说明.md)


## DEBUGGING

*   Use a logging system to track program execution.
*   Use a debugger to step through code.
*   Test frequently to catch errors early.
*   The system now uses a single database file per news source, simplifying data access and management.

## FINAL DOs AND DON'Ts

*   **Do:** Use type hints, write clear and concise code, and test thoroughly.
*   **Don't:** Commit broken code, use hardcoded values unless necessary, or ignore warnings.



# Example: A function with type annotations

```python
from typing import List, Dict

def get_news(source: str, days: int) -> List[Dict]:
    """Retrieves news articles from a specified source for the given number of days."""
    #Implementation goes here.
    pass
```

## Database Connection Details

The system uses a single database file per news source.  The system intelligently handles multiple database files for each news source, allowing for the storage and retrieval of historical data.  The system now uses a single database file per news source, simplifying data access and management.  The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management.

## PROJECT STRUCTURE

This section details the project's file and folder structure.  The structure has been recently optimized for improved maintainability and modularity.  Specific changes include the consolidation of database files and a more logical grouping of crawler modules.  The `today3` directory, while containing a mirrored structure, is now considered deprecated in favor of the optimized structure in the root directory. The project now uses a single database per source.  The structure is now more streamlined, with a clear separation of concerns between the API, the application logic, the data storage, and the web interface.  The root directory now contains the following:

```
NewsLook
├── api/              # API 接口
├── app/              # 应用逻辑
│   ├── crawlers/     # 爬虫模块
│   ├── tasks/        # 任务调度模块
│   ├── utils/        # 工具函数
│   └── web/          # Web 应用
├── data/             # 数据存储
│   └── db/           # 数据库文件
├── docs/             # 文档
├── requirements/    # 权限包
├── scripts/          # 运行脚本
├── tests/            # 测试
├── static/           # 静态文件
└── templates/        # 模板文件
```

## README.md UPDATES

The README.md file has been updated to include the following sections:

### Database Optimization

The system intelligently identifies and uses multiple database files, ensuring that you can view all historically crawled news data.

#### Multi-Database Support Features

- **Automatic Database Discovery:** The system automatically searches for possible database directories, finds, and uses all available database files.
- **Flexible Database Locations:** Supports multiple possible database locations, searching in order of priority:
    - `./data/db/` (the data/db directory under the project root)
    - `./db/` (the db directory under the project root)
    - The corresponding directories under the current working directory
- **Command-line Arguments:** Explicitly specify the database directory using the `--db-dir` argument.
- **Merged Query Results:** Queries and merges results from multiple database files to provide a complete data view.

#### How to Use the Multi-Database Feature

The system enables multi-database functionality by default. When starting the web application, it automatically searches for and uses all available database files:

```bash
# Use automatically detected database directory
python run.py web

# Specify database directory
python run.py web --db-dir path/to/your/db
```

In the console output, you can see which database files the system found:

```
Found main database: /path/to/your/db/finance_news.db
Found X database files
```

#### Database Search Logic

1. First, check if the command-line argument `--db-dir` specifies the database directory.
2. If not specified, check the environment variable `DB_DIR`.
3. If the environment variable is not set, search for the first valid directory containing `.db` files in the possible directories in order of priority.
4. If all directories do not exist or do not contain database files, create and use the default directory.

### Encoding Optimization and Garbled Character Handling

To address encoding and garbled character issues encountered when crawling financial websites, we have made the following key optimizations:

- **Multiple Encoding Handling:** Enhanced intelligent identification and conversion capabilities for multiple encodings such as UTF-8, GBK, and GB18030.
- **Unicode Escape Sequence Decoding:** Added the `decode_unicode_escape` function to handle Unicode escape sequences such as `\u4e2d\u56fd` -> 中国.
- **URL-Encoded Character Decoding:** Added the `decode_url_encoded` function to handle URL-encoded characters such as `%E4%B8%AD%E5%9B%BD` -> 中国.
- **Enhanced HTML Entity Decoding:** Expanded the capabilities of the `decode_html_entities` function, adding more common HTML entity handling.
- **Encoding Process Optimization:** Implemented multi-stage encoding detection and conversion during the crawling, parsing, and storage processes.
- **Automatic Garbled Character Repair:** Implemented customized garbled character repair mechanisms for specific websites (such as Sina Finance and Tencent Finance).

These optimizations significantly improve the system's ability to handle Chinese content and resolve Chinese garbled character issues, especially when dealing with mixed encodings on financial websites.  Refer to `app/utils/text_cleaner.py` and `app/crawlers/sina.py` files for implementation details.

### Database File Storage Optimization

To optimize database storage and access, the system has been updated as follows:

**Fixed Database Filenames:** The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management.

**Direct Database Access:** Crawler classes now directly use `sqlite3` connections to save news data to the database. This reduces overhead and improves reliability.

**Automatic Table Initialization:** The system automatically checks for and creates necessary database tables upon startup, ensuring data consistency.

**Intelligent Path Handling:** The `NewsDatabase` class in the web application intelligently handles both relative and absolute database paths, allowing for flexible deployment.

These optimizations significantly improve data storage efficiency, reduce data fragmentation, and simplify system maintenance.

### Database File Storage Optimization

The system has been updated to use a single database file per news source, simplifying data access and management.  The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management. Crawler classes directly use `sqlite3` connections, reducing overhead and improving reliability. The system automatically checks for and creates necessary database tables upon startup, ensuring data consistency. The system now uses a single database file per news source, simplifying data access and management.

## DATABASE CONNECTION DETAILS

The system now uses a single database file per news source, simplifying data access and management. The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management.

## README.md UPDATES

The README.md file has been updated to include the following sections:

### Database Optimization

The system intelligently identifies and uses multiple database files, ensuring that you can view all historically crawled news data.

#### Multi-Database Support Features

- **Automatic Database Discovery:** The system automatically searches for possible database directories, finds, and uses all available database files.
- **Flexible Database Locations:** Supports multiple possible database locations, searching in order of priority:
    - `./data/db/` (the data/db directory under the project root)
    - `./db/` (the db directory under the project root)
    - The corresponding directories under the current working directory
- **Command-line Arguments:** Explicitly specify the database directory using the `--db-dir` argument.
- **Merged Query Results:** Queries and merges results from multiple database files to provide a complete data view.

#### How to Use the Multi-Database Feature

The system enables multi-database functionality by default. When starting the web application, it automatically searches for and uses all available database files:

```bash
# Use automatically detected database directory
python run.py web

# Specify database directory
python run.py web --db-dir path/to/your/db
```

In the console output, you can see which database files the system found:

```
Found main database: /path/to/your/db/finance_news.db
Found X database files
```

#### Database Search Logic

1. First, check if the command-line argument `--db-dir` specifies the database directory.
2. If not specified, check the environment variable `DB_DIR`.
3. If the environment variable is not set, search for the first valid directory containing `.db` files in the possible directories in order of priority.
4. If all directories do not exist or do not contain database files, create and use the default directory.

### Encoding Optimization and Garbled Character Handling

To address encoding and garbled character issues encountered when crawling financial websites, we have made the following key optimizations:

- **Multiple Encoding Handling:** Enhanced intelligent identification and conversion capabilities for multiple encodings such as UTF-8, GBK, and GB18030.
- **Unicode Escape Sequence Decoding:** Added the `decode_unicode_escape` function to handle Unicode escape sequences such as `\u4e2d\u56fd` -> 中国.
- **URL-Encoded Character Decoding:** Added the `decode_url_encoded` function to handle URL-encoded characters such as `%E4%B8%AD%E5%9B%BD` -> 中国.
- **Enhanced HTML Entity Decoding:** Expanded the capabilities of the `decode_html_entities` function, adding more common HTML entity handling.
- **Encoding Process Optimization:** Implemented multi-stage encoding detection and conversion during the crawling, parsing, and storage processes.
- **Automatic Garbled Character Repair:** Implemented customized garbled character repair mechanisms for specific websites (such as Sina Finance and Tencent Finance).

These optimizations significantly improve the system's ability to handle Chinese content and resolve Chinese garbled character issues, especially when dealing with mixed encodings on financial websites.  Refer to `app/utils/text_cleaner.py` and `app/crawlers/sina.py` files for implementation details.

### Database File Storage Optimization

To optimize database storage and access, the system has been updated as follows:

**Fixed Database Filenames:** The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management.

**Direct Database Access:** Crawler classes now directly use `sqlite3` connections to save news data to the database. This reduces overhead and improves reliability.

**Automatic Table Initialization:** The system automatically checks for and creates necessary database tables upon startup, ensuring data consistency.

**Intelligent Path Handling:** The `NewsDatabase` class in the web application intelligently handles both relative and absolute database paths, allowing for flexible deployment.

These optimizations significantly improve data storage efficiency, reduce data fragmentation, and simplify system maintenance.

### Database File Storage Optimization

The system has been updated to use a single database file per news source, simplifying data access and management.  The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management. Crawler classes directly use `sqlite3` connections, reducing overhead and improving reliability. The system automatically checks for and creates necessary database tables upon startup, ensuring data consistency. The system now uses a single database file per news source, simplifying data access and management.

## PROJECT STRUCTURE

This section details the project's file and folder structure.  The structure has been recently optimized for improved maintainability and modularity.  Specific changes include the consolidation of database files and a more logical grouping of crawler modules.  The `today3` directory, while containing a mirrored structure, is now considered deprecated in favor of the optimized structure in the root directory. The project now uses a single database per source.  The structure is now more streamlined, with a clear separation of concerns between the API, the application logic, the data storage, and the web interface.

```
NewsLook
├── api/              # API 接口
├── app/              # 应用逻辑
│   ├── crawlers/     # 爬虫模块
│   ├── tasks/        # 任务调度模块
│   ├── utils/        # 工具函数
│   └── web/          # Web 应用
├── data/             # 数据存储
│   └── db/           # 数据库文件
├── docs/             # 文档
├── requirements/    # 权限包
├── scripts/          # 运行脚本
├── tests/            # 测试
├── static/           # 静态文件
└── templates/        # 模板文件
```

## README.md UPDATES

The README.md file has been updated to include the following sections:

### Database Optimization

The system intelligently identifies and uses multiple database files, ensuring that you can view all historically crawled news data.

#### Multi-Database Support Features

- **Automatic Database Discovery:** The system automatically searches for possible database directories, finds, and uses all available database files.
- **Flexible Database Locations:** Supports multiple possible database locations, searching in order of priority:
    - `./data/db/` (the data/db directory under the project root)
    - `./db/` (the db directory under the project root)
    - The corresponding directories under the current working directory
- **Command-line Arguments:** Explicitly specify the database directory using the `--db-dir` argument.
- **Merged Query Results:** Queries and merges results from multiple database files to provide a complete data view.

#### How to Use the Multi-Database Feature

The system enables multi-database functionality by default. When starting the web application, it automatically searches for and uses all available database files:

```bash
# Use automatically detected database directory
python run.py web

# Specify database directory
python run.py web --db-dir path/to/your/db
```

In the console output, you can see which database files the system found:

```
Found main database: /path/to/your/db/finance_news.db
Found X database files
```

#### Database Search Logic

1. First, check if the command-line argument `--db-dir` specifies the database directory.
2. If not specified, check the environment variable `DB_DIR`.
3. If the environment variable is not set, search for the first valid directory containing `.db` files in the possible directories in order of priority.
4. If all directories do not exist or do not contain database files, create and use the default directory.

### Encoding Optimization and Garbled Character Handling

To address encoding and garbled character issues encountered when crawling financial websites, we have made the following key optimizations:

- **Multiple Encoding Handling:** Enhanced intelligent identification and conversion capabilities for multiple encodings such as UTF-8, GBK, and GB18030.
- **Unicode Escape Sequence Decoding:** Added the `decode_unicode_escape` function to handle Unicode escape sequences such as `\u4e2d\u56fd` -> 中国.
- **URL-Encoded Character Decoding:** Added the `decode_url_encoded` function to handle URL-encoded characters such as `%E4%B8%AD%E5%9B%BD` -> 中国.
- **Enhanced HTML Entity Decoding:** Expanded the capabilities of the `decode_html_entities` function, adding more common HTML entity handling.
- **Encoding Process Optimization:** Implemented multi-stage encoding detection and conversion during the crawling, parsing, and storage processes.
- **Automatic Garbled Character Repair:** Implemented customized garbled character repair mechanisms for specific websites (such as Sina Finance and Tencent Finance).

These optimizations significantly improve the system's ability to handle Chinese content and resolve Chinese garbled character issues, especially when dealing with mixed encodings on financial websites.  Refer to `app/utils/text_cleaner.py` and `app/crawlers/sina.py` files for implementation details.

### Database File Storage Optimization

To optimize database storage and access, the system has been updated as follows:

**Fixed Database Filenames:** The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management.

**Direct Database Access:** Crawler classes now directly use `sqlite3` connections to save news data to the database. This reduces overhead and improves reliability.

**Automatic Table Initialization:** The system automatically checks for and creates necessary database tables upon startup, ensuring data consistency.

**Intelligent Path Handling:** The `NewsDatabase` class in the web application intelligently handles both relative and absolute database paths, allowing for flexible deployment.

These optimizations significantly improve data storage efficiency, reduce data fragmentation, and simplify system maintenance.

### Database File Storage Optimization

The system has been updated to use a single database file per news source, simplifying data access and management.  The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management. Crawler classes directly use `sqlite3` connections, reducing overhead and improving reliability. The system automatically checks for and creates necessary database tables upon startup, ensuring data consistency. The system now uses a single database file per news source, simplifying data access and management.

### Database File Storage Optimization

The system now uses a single database file per news source, simplifying data access and management.  The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management. Crawler classes directly use `sqlite3` connections, reducing overhead and improving reliability. The system automatically checks for and creates necessary database tables upon startup, ensuring data consistency. The system now uses a single database file per news source, simplifying data access and management.

### Encoding Optimization and Garbled Character Handling

To address encoding and garbled character issues encountered when crawling financial websites, we have made the following key optimizations:

- **Multiple Encoding Handling:** Enhanced intelligent identification and conversion capabilities for multiple encodings such as UTF-8, GBK, and GB18030.
- **Unicode Escape Sequence Decoding:** Added the `decode_unicode_escape` function to handle Unicode escape sequences such as `\u4e2d\u56fd` -> 中国.
- **URL-Encoded Character Decoding:** Added the `decode_url_encoded` function to handle URL-encoded characters such as `%E4%B8%AD%E5%9B%BD` -> 中国.
- **Enhanced HTML Entity Decoding:** Expanded the capabilities of the `decode_html_entities` function, adding more common HTML entity handling.
- **Encoding Process Optimization:** Implemented multi-stage encoding detection and conversion during the crawling, parsing, and storage processes.
- **Automatic Garbled Character Repair:** Implemented customized garbled character repair mechanisms for specific websites (such as Sina Finance and Tencent Finance).

These optimizations significantly improve the system's ability to handle Chinese content and resolve Chinese garbled character issues, especially when dealing with mixed encodings on financial websites.  Refer to `app/utils/text_cleaner.py` and `app/crawlers/sina.py` files for implementation details.

### 2025年4月 - 编码优化与乱码处理

为解决爬取财经网站时遇到的编码和乱码问题，我们进行了以下关键优化：

- **多重编码处理**：增强了对UTF-8、GBK、GB18030等多种编码的智能识别和转换能力
- **Unicode转义序列解码**：新增`decode_unicode_escape`函数处理类似`\u4e2d\u56fd`的Unicode转义序列
- **URL编码字符解码**：新增`decode_url_encoded`函数处理类似`%E4%B8%AD%E5%9B%BD`的URL编码字符
- **HTML实体解码增强**：扩展了`decode_html_entities`函数的能力，添加更多常见HTML实体的处理
- **编码流程优化**：在爬取、解析和存储过程中实施多阶段编码检测和转换
- **乱码自动修复**：对于特定网站（如新浪财经、腾讯财经）实现了定制化的乱码修复机制

这些优化显著提高了系统处理中文内容的能力，解决了中文乱码问题，特别是在处理财经网站混合编码的情况时。具体实现可查看`app/utils/text_cleaner.py`和`app/crawlers/sina.py`等文件。

### 2025年5月 - 数据库存储优化

为提高数据管理效率和简化系统架构，我们对数据库存储机制进行了以下优化：

- **固定数据库文件名**：改用固定命名格式`[source_name].db`（如`腾讯财经.db`），替代之前的时间戳命名方式
- **直接数据库访问**：爬虫类实现了直接使用sqlite3连接将新闻保存到数据库，减少了中间层次
- **自动表初始化**：系统运行时自动检查并创建所需的数据库表结构，确保数据一致性
- **智能路径处理**：增强了数据库路径处理逻辑，支持多种可能的数据库位置
- **出错自动重试**：优化了数据库操作的错误处理和重试机制

这些优化带来了显著改进：
1. 减少了数据库文件数量，避免了数据库文件爆炸性增长
2. 简化了数据访问流程，网页应用始终能找到最新数据
3. 减少了数据碎片化，提高了查询效率
4. 降低了系统维护难度，数据备份更加便捷

详细说明可参阅 `docs/数据库连接说明.md` 文件中的"数据库文件存储优化"部分.

## PROJECT STRUCTURE

This section details the project's file and folder structure.  The structure has been recently optimized for improved maintainability and modularity.  Specific changes include the consolidation of database files and a more logical grouping of crawler modules.  The `today3` directory, while containing a mirrored structure, is now considered deprecated in favor of the optimized structure in the root directory. The project now uses a single database per source.  The structure is now more streamlined, with a clear separation of concerns between the API, the application logic, the data storage, and the web interface.

```
NewsLook
├── api/              # API 接口
├── app/              # 应用逻辑
│   ├── crawlers/     # 爬虫模块
│   ├── tasks/        # 任务调度模块
│   ├── utils/        # 工具函数
│   └── web/          # Web 应用
├── data/             # 数据存储
│   └── db/           # 数据库文件
├── docs/             # 文档
├── requirements/    # 权限包
├── scripts/          # 运行脚本
├── tests/            # 测试
├── static/           # 静态文件
└── templates/        # 模板文件
```

## DEBUGGING

*   Use a logging system to track program execution.
*   Use a debugger to step through code.
*   Test frequently to catch errors early.
*   The system now uses a single database file per news source, simplifying data access and management.

## DATABASE CONNECTION DETAILS

The system now uses a single database file per news source, simplifying data access and management. The system now uses a fixed filename for each news source's database (e.g., `tencent_finance.db`, `sina_finance.db`, `eastmoney.db`), instead of the previous timestamp-based naming convention (`[source_name]_YYYYMMDD_HHMMSS.db`). This simplifies data access and management.


## 来源专用数据库实现说明

## 概述

为了实现每个爬虫来源使用独立的数据库文件，我们对系统进行了一系列修改和优化。本文档描述了实现细节和使用方法。

## 数据库架构

系统现在使用以下数据库架构：

1. **主数据库**: `finance_news.db` - 用于存储聚合的新闻数据
2. **来源专用数据库**: 每个来源对应一个独立的数据库文件，如 `腾讯财经.db`、`新浪财经.db` 等

## 文件命名约定

来源专用数据库使用来源名称作为文件名，例如：
- 腾讯财经 -> `腾讯财经.db`
- 新浪财经 -> `新浪财经.db`
- 东方财富 -> `东方财富.db`
- 网易财经 -> `网易财经.db`
- 凤凰财经 -> `凤凰财经.db`

## 实现细节

### 1. 为每个爬虫设置专用数据库路径

在 `CrawlerManager` 类中，我们为每个爬虫实例设置了对应的数据库路径：

```python
# 为每个爬虫设置固定的数据库路径
self._set_db_path(ifeng_crawler, "凤凰财经")
self._set_db_path(sina_crawler, "新浪财经")
self._set_db_path(tencent_crawler, "腾讯财经")
self._set_db_path(netease_crawler, "网易财经")
self._set_db_path(eastmoney_crawler, "东方财富")
```

### 2. 改进 NewsDatabase 类

我们对 `NewsDatabase` 类进行了以下改进：

- 添加 `source_db_map` 属性，建立来源名称到数据库文件的映射
- 修改 `query_news` 方法，支持按来源名称选择对应的数据库
- 修改 `get_news_count` 方法，支持按来源统计新闻数量
- 修改 `get_news_by_id` 方法，支持在所有数据库中查找指定ID的新闻
- 修改 `get_sources` 和 `get_categories` 方法，从所有数据库中收集数据

### 3. 实现数据分离和查询整合

- 每个爬虫将新闻存储到对应的来源专用数据库
- 查询时，系统根据是否指定来源来决定查询哪些数据库
- 对于来源专用查询，直接使用对应的数据库文件
- 对于无来源查询，合并所有数据库的结果并去重

## 数据库初始化流程

1. 当创建 `NewsDatabase` 实例时，系统会自动扫描数据库目录
2. 识别所有 `.db` 文件，并根据文件名建立来源映射
3. 对于每个来源专用数据库，记录在 `source_db_map` 中
4. 基于 `use_all_dbs` 参数决定是否使用所有数据库

## Web应用中的使用

在Web应用中，我们始终使用参数 `use_all_dbs=True` 初始化 `NewsDatabase`，确保能访问所有可用的数据库：

```python
# 查询数据库 - 始终使用所有可用的数据库
db = NewsDatabase(use_all_dbs=True)
```

## 来源筛选逻辑

当用户在Web界面选择特定来源筛选新闻时：

1. 系统首先检查是否存在该来源的专用数据库
2. 如果存在，则优先使用专用数据库进行查询
3. 如果不存在，则在所有数据库中使用条件筛选

## 优势

1. **数据隔离**: 每个来源的数据存储在独立文件中，便于管理和备份
2. **性能优化**: 按来源查询时直接使用专用数据库，提高查询效率
3. **扩展性**: 易于添加新的来源，只需创建新的数据库文件
4. **维护简便**: 可以单独备份、修复或优化某个来源的数据库

## 测试

我们创建了以下测试工具脚本：

1. `create_test_dbs.py` - 创建测试用的来源专用数据库
2. `test_db_access.py` - 测试来源专用数据库访问功能

## 注意事项

1. 所有数据库文件必须存放在配置的 `DB_DIR` 目录中
2. 数据库文件名必须使用来源名称，以确保正确映射
3. 当有新的爬虫添加到系统时，需要在 `CrawlerManager` 中设置其数据库路径