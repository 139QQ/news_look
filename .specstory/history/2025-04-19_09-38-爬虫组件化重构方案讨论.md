<!-- Generated by SpecStory -->

# 爬虫组件化重构方案讨论 (2025-04-19 17:38:53)

_**User**_

@crawlers 组件化重构
将爬虫功能拆分为更小的独立组件
建议划分：请求器、解析器、存储器、监控器(使用合适的设计模式）
接口标准化
统一所有爬虫组件的接口设计
实现可插拔的组件架构
配置与代码分离
将爬虫配置从代码中抽离出来
使用YAML或JSON管理配置
中间件体系
实现类似Django中间件的请求处理链
在请求前后添加通用处理逻辑
通用爬虫模板
实现通用爬虫模板，只需配置特定规则
减少重复代码，提高维护性



---

_**Assistant**_

我需要了解现有的爬虫代码结构，然后提出重构方案。

---

Read file: app/crawlers/base.py

---

Read file: app/crawlers/manager.py

---

Read file: app/crawlers/ifeng.py

---

Read file: app/crawlers/async_crawler.py

---

<details>
            <summary>Searched codebase "组件化设计或插件系统在爬虫代码中的实现" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `app/utils/crawler_optimizer.py` | L45-92 |
| `app/crawlers/optimized_crawler.py` | L30-77 |
| `app/crawlers/optimized_crawler.py` | L77-116 |
| `app/crawlers/manager.py` | L537-551 |
| `app/crawlers/base.py` | L170-197 |
| `app/crawlers/base.py` | L38-77 |
| `_archived_today3_20250324200011/app/crawlers/ifeng.py` | L1-58 |
| `app/crawlers/netease.py` | L1-65 |
| `run.py` | L562-613 |
| `newslook/crawlers/base.py` | L32-71 |
| `scripts/temp/run.py.bak.20250324200011` | L1-43 |
| `_archived_today3_20250324200011/app/crawlers/__init__.py` | L1-47 |
| `_archived_today3_20250324200011/爬虫迁移总结.md` | L1-32 |
| `newslook/crawlers/ifeng.py` | L1-49 |
| `scripts/run_crawler.py` | L47-70 |
| `app/crawlers/optimized_netease.py` | L445-496 |
| `_archived_today3_20250324200011/run.py` | L1-45 |
| `app/crawlers/base.py` | L341-377 |
| `_archived_today3_20250324200011/docs/脚本整合说明.md` | L1-110 |
| `app/crawlers/base.py` | L312-341 |
| `app/crawlers/optimized_crawler.py` | L116-165 |
| `newslook/crawlers/manager.py` | L420-434 |
| `app/crawlers/base.py` | L277-312 |
| `docs/脚本整合说明.md` | L1-110 |
| `app/crawlers/async_ifeng.py` | L1-45 |

</details>

---

在查看了现有的爬虫代码后，我需要设计一个更加组件化和可插拔的架构。现在我将提出一个重构方案：

---

```diff
- 
+ """
+ 财经新闻爬虫系统 - 组件化架构
+ 提供可插拔的爬虫组件
+ """
+ 
+ __all__ = [] 
```

---

Cancelled

---

_**User**_

继续

---

_**Assistant**_

继续进行爬虫组件化重构：

---

```diff
- 
+ #!/usr/bin/env python3
+ # -*- coding: utf-8 -*-
+ 
+ """
+ 财经新闻爬虫系统 - 请求器组件
+ 负责处理HTTP请求，支持同步和异步模式
+ """
+ 
+ import time
+ import random
+ import logging
+ import requests
+ import aiohttp
+ import asyncio
+ from typing import Dict, Optional, Any, Union, List, Tuple
+ from abc import ABC, abstractmethod
+ from fake_useragent import UserAgent
+ 
+ from app.utils.logger import get_crawler_logger
+ 
+ # 使用专门的爬虫日志记录器
+ logger = get_crawler_logger('requesters')
+ 
+ class BaseRequester(ABC):
+     """请求器基类，定义请求器的接口"""
+     
+     @abstractmethod
+     def fetch(self, url: str, params: Optional[Dict] = None, 
+               headers: Optional[Dict] = None, timeout: Optional[int] = None, 
+               max_retries: int = 3) -> Optional[str]:
+         """获取页面内容"""
+         pass
+     
+     @abstractmethod
+     def fetch_json(self, url: str, params: Optional[Dict] = None, 
+                   headers: Optional[Dict] = None, timeout: Optional[int] = None,
+                   max_retries: int = 3) -> Optional[Dict]:
+         """获取JSON内容"""
+         pass
+     
+     @abstractmethod
+     def get_headers(self) -> Dict[str, str]:
+         """获取HTTP请求头"""
+         pass
+     
+     @abstractmethod
+     def set_proxy(self, proxy_url: str) -> None:
+         """设置代理"""
+         pass
+ 
+ 
+ class SyncRequester(BaseRequester):
+     """同步请求器，使用requests库处理HTTP请求"""
+     
+     def __init__(self, use_proxy: bool = False, proxy_url: Optional[str] = None, 
+                  timeout: int = 30, random_delay: bool = True):
+         """
+         初始化同步请求器
+         
+         Args:
+             use_proxy: 是否使用代理
+             proxy_url: 代理URL
+             timeout: 默认超时时间（秒）
+             random_delay: 是否随机延迟请求
+         """
+         self.use_proxy = use_proxy
+         self.proxy_url = proxy_url
+         self.timeout = timeout
+         self.random_delay = random_delay
+         self.proxies = self._set_proxies(proxy_url) if use_proxy and proxy_url else None
+         
+         # 尝试初始化UserAgent
+         try:
+             self.ua = UserAgent()
+         except Exception as e:
+             logger.warning(f"初始化UserAgent失败: {str(e)}，将使用默认User-Agent")
+             self.ua = None
+             
+         # 请求统计
+         self.request_stats = {
+             "total": 0,
+             "success": 0,
+             "failure": 0,
+             "avg_response_time": 0,
+             "total_response_time": 0
+         }
+     
+     def fetch(self, url: str, params: Optional[Dict] = None, 
+               headers: Optional[Dict] = None, timeout: Optional[int] = None, 
+               max_retries: int = 3) -> Optional[str]:
+         """
+         获取页面内容
+         
+         Args:
+             url: 页面URL
+             params: 请求参数
+             headers: 自定义请求头
+             timeout: 超时时间（秒）
+             max_retries: 最大重试次数
+             
+         Returns:
+             str: 页面内容或None
+         """
+         if not url:
+             return None
+             
+         # 设置超时
+         if timeout is None:
+             timeout = self.timeout
+             
+         # 合并请求头
+         merged_headers = self.get_headers()
+         if headers:
+             merged_headers.update(headers)
+             
+         # 添加随机延迟
+         if self.random_delay:
+             self._random_sleep()
+             
+         # 更新请求统计
+         self.request_stats["total"] += 1
+         start_time = time.time()
+         
+         # 尝试请求
+         for attempt in range(max_retries):
+             try:
+                 response = requests.get(
+                     url,
+                     params=params,
+                     headers=merged_headers,
+                     proxies=self.proxies,
+                     timeout=timeout
+                 )
+                 
+                 # 检查响应状态
+                 if response.status_code != 200:
+                     logger.warning(f"请求失败: {url}, 状态码: {response.status_code}, 重试 {attempt+1}/{max_retries}")
+                     if attempt < max_retries - 1:
+                         time.sleep(2 * (attempt + 1))  # 指数退避
+                         continue
+                     self.request_stats["failure"] += 1
+                     return None
+                 
+                 # 更新统计信息
+                 end_time = time.time()
+                 request_time = end_time - start_time
+                 self.request_stats["success"] += 1
+                 self.request_stats["total_response_time"] += request_time
+                 self.request_stats["avg_response_time"] = (
+                     self.request_stats["total_response_time"] / self.request_stats["success"]
+                 )
+                 
+                 # 尝试检测并修正编码
+                 content = self._handle_encoding(response)
+                 return content
+                 
+             except requests.exceptions.RequestException as e:
+                 logger.error(f"请求异常: {url}, 错误: {str(e)}, 重试 {attempt+1}/{max_retries}")
+                 if attempt < max_retries - 1:
+                     time.sleep(2 * (attempt + 1))  # 指数退避
+                     continue
+                 self.request_stats["failure"] += 1
+                 return None
+         
+         return None
+     
+     def fetch_json(self, url: str, params: Optional[Dict] = None, 
+                   headers: Optional[Dict] = None, timeout: Optional[int] = None,
+                   max_retries: int = 3) -> Optional[Dict]:
+         """
+         获取JSON内容
+         
+         Args:
+             url: 页面URL
+             params: 请求参数
+             headers: 自定义请求头
+             timeout: 超时时间（秒）
+             max_retries: 最大重试次数
+             
+         Returns:
+             Dict: JSON响应或None
+         """
+         # 设置默认的JSON请求头
+         json_headers = {
+             'Accept': 'application/json, text/javascript, */*; q=0.01',
+             'X-Requested-With': 'XMLHttpRequest',
+             'Content-Type': 'application/json'
+         }
+         
+         # 合并请求头
+         if headers:
+             json_headers.update(headers)
+             
+         # 发起请求
+         content = self.fetch(url, params, json_headers, timeout, max_retries)
+         if not content:
+             return None
+             
+         # 尝试解析JSON
+         try:
+             return requests.utils.json_loads(content)
+         except ValueError as e:
+             logger.error(f"JSON解析失败: {url}, 错误: {str(e)}")
+             return None
+     
+     def get_headers(self) -> Dict[str, str]:
+         """获取随机的HTTP请求头"""
+         user_agent = self._get_random_user_agent()
+         return {
+             'User-Agent': user_agent,
+             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
+             'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
+             'Accept-Encoding': 'gzip, deflate, br',
+             'Connection': 'keep-alive',
+             'Upgrade-Insecure-Requests': '1',
+             'Cache-Control': 'max-age=0',
+             'sec-ch-ua': '"Google Chrome";v="93", " Not;A Brand";v="99", "Chromium";v="93"',
+             'sec-ch-ua-mobile': '?0',
+             'sec-ch-ua-platform': '"Windows"'
+         }
+     
+     def set_proxy(self, proxy_url: str) -> None:
+         """
+         设置代理
+         
+         Args:
+             proxy_url: 代理URL (例如: http://user:pass@10.10.1.10:3128/)
+         """
+         self.proxy_url = proxy_url
+         self.proxies = self._set_proxies(proxy_url)
+         self.use_proxy = bool(self.proxies)
+         
+         if self.use_proxy:
+             logger.info(f"代理已设置: {proxy_url}")
+         else:
+             logger.warning("代理设置失败")
+     
+     def _set_proxies(self, proxy_url: str) -> Dict[str, str]:
+         """
+         根据代理URL设置proxies字典
+         
+         Args:
+             proxy_url: 代理URL
+             
+         Returns:
+             Dict: 代理配置字典
+         """
+         if not proxy_url:
+             return {}
+             
+         return {
+             'http': proxy_url,
+             'https': proxy_url
+         }
+     
+     def _random_sleep(self, min_seconds: float = 0.5, max_seconds: float = 2.0) -> None:
+         """
+         随机休眠，避免频繁请求
+         
+         Args:
+             min_seconds: 最小休眠时间（秒）
+             max_seconds: 最大休眠时间（秒）
+         """
+         sleep_time = random.uniform(min_seconds, max_seconds)
+         time.sleep(sleep_time)
+     
+     def _get_random_user_agent(self) -> str:
+         """
+         获取随机User-Agent
+         
+         Returns:
+             str: User-Agent字符串
+         """
+         # 如果有fake_useragent，使用它
+         if self.ua:
+             try:
+                 return self.ua.random
+             except Exception:
+                 pass
+         
+         # 否则使用预定义的User-Agent列表
+         user_agents = [
+             # Chrome最新版本UA
+             "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
+             "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
+             # Edge最新版本UA
+             "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
+             # Firefox最新版本UA
+             "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/118.0",
+             # Safari最新版本UA
+             "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
+         ]
+         return random.choice(user_agents)
+     
+     def _handle_encoding(self, response: requests.Response) -> str:
+         """
+         处理响应编码
+         
+         Args:
+             response: requests响应对象
+             
+         Returns:
+             str: 解码后的响应内容
+         """
+         # 尝试从响应头获取编码
+         content_type = response.headers.get('Content-Type', '')
+         if 'charset=' in content_type:
+             encoding = content_type.split('charset=')[-1].split(';')[0].strip()
+             try:
+                 return response.content.decode(encoding, errors='replace')
+             except (LookupError, UnicodeDecodeError):
+                 pass
+         
+         # 尝试使用response.encoding
+         if response.encoding:
+             try:
+                 return response.content.decode(response.encoding, errors='replace')
+             except (LookupError, UnicodeDecodeError):
+                 pass
+         
+         # 尝试使用apparent_encoding
+         if response.apparent_encoding:
+             try:
+                 return response.content.decode(response.apparent_encoding, errors='replace')
+             except (LookupError, UnicodeDecodeError):
+                 pass
+         
+         # 常见的中文编码
+         for encoding in ['utf-8', 'gb2312', 'gbk', 'gb18030']:
+             try:
+                 return response.content.decode(encoding, errors='replace')
+             except (LookupError, UnicodeDecodeError):
+                 continue
+         
+         # 最后尝试使用默认编码，并忽略错误
+         return response.text
+ 
+ 
+ class AsyncRequester(BaseRequester):
+     """异步请求器，使用aiohttp库处理HTTP请求"""
+     
+     def __init__(self, use_proxy: bool = False, proxy_url: Optional[str] = None, 
+                  timeout: int = 30, max_concurrency: int = 5, random_delay: bool = True):
+         """
+         初始化异步请求器
+         
+         Args:
+             use_proxy: 是否使用代理
+             proxy_url: 代理URL
+             timeout: 默认超时时间（秒）
+             max_concurrency: 最大并发请求数
+             random_delay: 是否随机延迟请求
+         """
+         self.use_proxy = use_proxy
+         self.proxy_url = proxy_url
+         self.timeout = timeout
+         self.max_concurrency = max_concurrency
+         self.random_delay = random_delay
+         self.session = None
+         self.semaphore = None
+         
+         # 尝试初始化UserAgent
+         try:
+             self.ua = UserAgent()
+         except Exception as e:
+             logger.warning(f"初始化UserAgent失败: {str(e)}，将使用默认User-Agent")
+             self.ua = None
+             
+         # 请求统计
+         self.request_stats = {
+             "total": 0,
+             "success": 0,
+             "failure": 0,
+             "avg_response_time": 0,
+             "total_response_time": 0
+         }
+     
+     def fetch(self, url: str, params: Optional[Dict] = None, 
+               headers: Optional[Dict] = None, timeout: Optional[int] = None, 
+               max_retries: int = 3) -> Optional[str]:
+         """
+         同步获取页面内容（该方法是为了兼容性而提供的）
+         
+         Args:
+             url: 页面URL
+             params: 请求参数
+             headers: 自定义请求头
+             timeout: 超时时间（秒）
+             max_retries: 最大重试次数
+             
+         Returns:
+             str: 页面内容或None
+         """
+         # 创建一个事件循环并运行异步fetch
+         loop = asyncio.new_event_loop()
+         asyncio.set_event_loop(loop)
+         try:
+             return loop.run_until_complete(
+                 self.async_fetch(url, params, headers, timeout, max_retries)
+             )
+         finally:
+             loop.close()
+     
+     def fetch_json(self, url: str, params: Optional[Dict] = None, 
+                   headers: Optional[Dict] = None, timeout: Optional[int] = None,
+                   max_retries: int = 3) -> Optional[Dict]:
+         """
+         同步获取JSON内容（该方法是为了兼容性而提供的）
+         
+         Args:
+             url: 页面URL
+             params: 请求参数
+             headers: 自定义请求头
+             timeout: 超时时间（秒）
+             max_retries: 最大重试次数
+             
+         Returns:
+             Dict: JSON响应或None
+         """
+         # 创建一个事件循环并运行异步fetch_json
+         loop = asyncio.new_event_loop()
+         asyncio.set_event_loop(loop)
+         try:
+             return loop.run_until_complete(
+                 self.async_fetch_json(url, params, headers, timeout, max_retries)
+             )
+         finally:
+             loop.close()
+     
+     async def async_fetch(self, url: str, params: Optional[Dict] = None, 
+                          headers: Optional[Dict] = None, timeout: Optional[int] = None,
+                          max_retries: int = 3) -> Optional[str]:
+         """
+         异步获取页面内容
+         
+         Args:
+             url: 页面URL
+             params: 请求参数
+             headers: 自定义请求头
+             timeout: 超时时间（秒）
+             max_retries: 最大重试次数
+             
+         Returns:
+             str: 页面内容或None
+         """
+         if not url:
+             return None
+             
+         # 初始化会话
+         await self._init_session()
+         
+         # 设置超时
+         if timeout is None:
+             timeout = self.timeout
+             
+         # 合并请求头
+         merged_headers = self.get_headers()
+         if headers:
+             merged_headers.update(headers)
+             
+         # 限制并发
+         async with self.semaphore:
+             # 添加随机延迟
+             if self.random_delay:
+                 await self._async_random_sleep()
+                 
+             # 更新请求统计
+             self.request_stats["total"] += 1
+             start_time = time.time()
+             
+             # 尝试请求
+             for attempt in range(max_retries):
+                 try:
+                     # 设置代理
+                     proxy = self.proxy_url if self.use_proxy else None
+                     
+                     async with self.session.get(
+                         url,
+                         params=params,
+                         headers=merged_headers,
+                         proxy=proxy,
+                         timeout=aiohttp.ClientTimeout(total=timeout)
+                     ) as response:
+                         # 检查响应状态
+                         if response.status != 200:
+                             logger.warning(f"请求失败: {url}, 状态码: {response.status}, 重试 {attempt+1}/{max_retries}")
+                             if attempt < max_retries - 1:
+                                 await asyncio.sleep(2 * (attempt + 1))  # 指数退避
+                                 continue
+                             self.request_stats["failure"] += 1
+                             return None
+                         
+                         # 更新统计信息
+                         end_time = time.time()
+                         request_time = end_time - start_time
+                         self.request_stats["success"] += 1
+                         self.request_stats["total_response_time"] += request_time
+                         self.request_stats["avg_response_time"] = (
+                             self.request_stats["total_response_time"] / self.request_stats["success"]
+                         )
+                         
+                         # 获取响应内容
+                         content = await response.text(errors='replace')
+                         return content
+                         
+                 except (aiohttp.ClientError, asyncio.TimeoutError) as e:
+                     logger.error(f"请求异常: {url}, 错误: {str(e)}, 重试 {attempt+1}/{max_retries}")
+                     if attempt < max_retries - 1:
+                         await asyncio.sleep(2 * (attempt + 1))  # 指数退避
+                         continue
+                     self.request_stats["failure"] += 1
+                     return None
+             
+             return None
+     
+     async def async_fetch_json(self, url: str, params: Optional[Dict] = None, 
+                               headers: Optional[Dict] = None, timeout: Optional[int] = None,
+                               max_retries: int = 3) -> Optional[Dict]:
+         """
+         异步获取JSON内容
+         
+         Args:
+             url: 页面URL
+             params: 请求参数
+             headers: 自定义请求头
+             timeout: 超时时间（秒）
+             max_retries: 最大重试次数
+             
+         Returns:
+             Dict: JSON响应或None
+         """
+         # 设置默认的JSON请求头
+         json_headers = {
+             'Accept': 'application/json, text/javascript, */*; q=0.01',
+             'X-Requested-With': 'XMLHttpRequest',
+             'Content-Type': 'application/json'
+         }
+         
+         # 合并请求头
+         if headers:
+             json_headers.update(headers)
+             
+         # 发起请求
+         content = await self.async_fetch(url, params, json_headers, timeout, max_retries)
+         if not content:
+             return None
+             
+         # 尝试解析JSON
+         try:
+             import json
+             return json.loads(content)
+         except ValueError as e:
+             logger.error(f"JSON解析失败: {url}, 错误: {str(e)}")
+             return None
+     
+     def get_headers(self) -> Dict[str, str]:
+         """获取随机的HTTP请求头"""
+         user_agent = self._get_random_user_agent()
+         return {
+             'User-Agent': user_agent,
+             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
+             'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
+             'Accept-Encoding': 'gzip, deflate, br',
+             'Connection': 'keep-alive',
+             'Upgrade-Insecure-Requests': '1',
+             'Cache-Control': 'max-age=0',
+             'sec-ch-ua': '"Google Chrome";v="93", " Not;A Brand";v="99", "Chromium";v="93"',
+             'sec-ch-ua-mobile': '?0',
+             'sec-ch-ua-platform': '"Windows"'
+         }
+     
+     def set_proxy(self, proxy_url: str) -> None:
+         """
+         设置代理
+         
+         Args:
+             proxy_url: 代理URL (例如: http://user:pass@10.10.1.10:3128/)
+         """
+         self.proxy_url = proxy_url
+         self.use_proxy = bool(proxy_url)
+         
+         # 重置会话，让新的代理设置生效
+         if self.session and not self.session.closed:
+             asyncio.create_task(self.session.close())
+             self.session = None
+         
+         if self.use_proxy:
+             logger.info(f"代理已设置: {proxy_url}")
+         else:
+             logger.warning("代理已清除")
+     
+     async def _init_session(self) -> None:
+         """初始化异步会话"""
+         if self.session is None or self.session.closed:
+             timeout = aiohttp.ClientTimeout(total=self.timeout)
+             self.session = aiohttp.ClientSession(headers=self.get_headers(), timeout=timeout)
+         
+         if self.semaphore is None:
+             self.semaphore = asyncio.Semaphore(self.max_concurrency)
+     
+     async def close(self) -> None:
+         """关闭异步会话"""
+         if self.session and not self.session.closed:
+             await self.session.close()
+             self.session = None
+             logger.debug("异步会话已关闭")
+     
+     def __del__(self) -> None:
+         """析构函数，确保会话被关闭"""
+         if self.session and not self.session.closed:
+             # 创建一个事件循环来关闭会话
+             try:
+                 loop = asyncio.get_event_loop()
+                 if loop.is_running():
+                     loop.create_task(self.close())
+                 else:
+                     loop.run_until_complete(self.close())
+             except RuntimeError:
+                 # 如果没有运行中的事件循环
+                 try:
+                     loop = asyncio.new_event_loop()
+                     asyncio.set_event_loop(loop)
+                     loop.run_until_complete(self.close())
+                     loop.close()
+                 except Exception as e:
+                     logger.error(f"关闭异步会话时出错: {str(e)}")
+     
+     async def _async_random_sleep(self, min_seconds: float = 0.5, max_seconds: float = 2.0) -> None:
+         """
+         异步随机休眠，避免频繁请求
+         
+         Args:
+             min_seconds: 最小休眠时间（秒）
+             max_seconds: 最大休眠时间（秒）
+         """
+         sleep_time = random.uniform(min_seconds, max_seconds)
+         await asyncio.sleep(sleep_time)
+     
+     def _get_random_user_agent(self) -> str:
+         """
+         获取随机User-Agent
+         
+         Returns:
+             str: User-Agent字符串
+         """
+         # 如果有fake_useragent，使用它
+         if self.ua:
+             try:
+                 return self.ua.random
+             except Exception:
+                 pass
+         
+         # 否则使用预定义的User-Agent列表
+         user_agents = [
+             # Chrome最新版本UA
+             "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
+             "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
+             # Edge最新版本UA
+             "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
+             # Firefox最新版本UA
+             "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/118.0",
+             # Safari最新版本UA
+             "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
+         ]
+         return random.choice(user_agents)
+ 
+ 
+ def create_requester(async_mode: bool = False, **kwargs) -> BaseRequester:
+     """
+     创建请求器工厂函数
+     
+     Args:
+         async_mode: 是否创建异步请求器
+         **kwargs: 其他参数
+         
+     Returns:
+         BaseRequester: 请求器实例
+     """
+     if async_mode:
+         return AsyncRequester(**kwargs)
+     else:
+         return SyncRequester(**kwargs) 
```

---

```diff
- 
+ #!/usr/bin/env python3
+ # -*- coding: utf-8 -*-
+ 
+ """
+ 财经新闻爬虫系统 - 解析器组件
+ 负责解析HTML和提取内容
+ """
+ 
+ import re
+ import json
+ import logging
+ import datetime
+ from typing import Dict, List, Optional, Any, Tuple, Union
+ from abc import ABC, abstractmethod
+ from bs4 import BeautifulSoup
+ 
+ from app.utils.logger import get_crawler_logger
+ from app.utils.text_cleaner import (
+     clean_html_content, 
+     extract_keywords, 
+     decode_html_entities, 
+     decode_unicode_escape, 
+     decode_url_encoded
+ )
+ 
+ # 使用专门的爬虫日志记录器
+ logger = get_crawler_logger('parsers')
+ 
+ class BaseParser(ABC):
+     """解析器基类，定义解析器的接口"""
+     
+     @abstractmethod
+     def parse_list(self, content: str, url: str = None) -> List[Dict]:
+         """解析列表页面，提取新闻列表"""
+         pass
+     
+     @abstractmethod
+     def parse_detail(self, content: str, url: str = None) -> Dict:
+         """解析详情页面，提取新闻详情"""
+         pass
+     
+     @abstractmethod
+     def clean_content(self, content: str) -> str:
+         """清理内容，去除广告、无用标签等"""
+         pass
+ 
+ 
+ class HTMLParser(BaseParser):
+     """HTML解析器，使用BeautifulSoup解析HTML内容"""
+     
+     def __init__(self, list_selector: Optional[Dict] = None, 
+                  detail_selector: Optional[Dict] = None,
+                  date_format: str = '%Y-%m-%d %H:%M:%S'):
+         """
+         初始化HTML解析器
+         
+         Args:
+             list_selector: 列表页面选择器
+             detail_selector: 详情页面选择器
+             date_format: 日期格式
+         """
+         # 默认列表选择器
+         self.list_selector = {
+             'container': 'div.news-list, ul.news-list, div.article-list',
+             'item': 'div.news-item, li.news-item, div.article-item',
+             'title': 'h2, h3, a.title',
+             'link': 'a',
+             'date': 'span.date, time.date, span.time',
+             'summary': 'div.summary, p.summary, div.desc'
+         }
+         
+         # 默认详情选择器
+         self.detail_selector = {
+             'title': 'h1.title, h1.article-title, div.article-title h1',
+             'content': 'div.article-content, div.content, article.content, div.article',
+             'date': 'span.date, time.date, span.time, div.article-info time',
+             'author': 'span.author, div.author, span.editor, div.editor',
+             'source': 'span.source, div.source',
+             'category': 'span.category, div.category, span.channel, div.channel'
+         }
+         
+         # 更新选择器
+         if list_selector:
+             self.list_selector.update(list_selector)
+         if detail_selector:
+             self.detail_selector.update(detail_selector)
+             
+         self.date_format = date_format
+         
+     def parse_list(self, content: str, url: str = None) -> List[Dict]:
+         """
+         解析列表页面，提取新闻列表
+         
+         Args:
+             content: HTML内容
+             url: 页面URL
+             
+         Returns:
+             List[Dict]: 新闻列表
+         """
+         if not content:
+             logger.warning("解析列表页面失败：内容为空")
+             return []
+             
+         # 尝试修复编码问题
+         content = self._fix_encoding(content)
+         
+         try:
+             soup = BeautifulSoup(content, 'html.parser')
+             
+             # 尝试查找容器
+             containers = soup.select(self.list_selector['container'])
+             if not containers:
+                 # 如果找不到容器，尝试直接查找条目
+                 items = soup.select(self.list_selector['item'])
+             else:
+                 # 如果找到容器，在容器中查找条目
+                 container = containers[0]
+                 items = container.select(self.list_selector['item'])
+             
+             news_list = []
+             
+             for item in items:
+                 try:
+                     # 提取标题
+                     title_elem = item.select_one(self.list_selector['title'])
+                     title = title_elem.get_text(strip=True) if title_elem else ""
+                     
+                     # 提取链接
+                     link_elem = item.select_one(self.list_selector['link'])
+                     link = ""
+                     if link_elem and link_elem.has_attr('href'):
+                         link = link_elem['href']
+                         # 处理相对URL
+                         if link.startswith('/') and url:
+                             from urllib.parse import urljoin
+                             link = urljoin(url, link)
+                     
+                     # 提取日期
+                     date_elem = item.select_one(self.list_selector['date'])
+                     date_str = date_elem.get_text(strip=True) if date_elem else ""
+                     date = self._parse_date(date_str)
+                     
+                     # 提取摘要
+                     summary_elem = item.select_one(self.list_selector['summary'])
+                     summary = summary_elem.get_text(strip=True) if summary_elem else ""
+                     
+                     # 创建新闻字典
+                     news = {
+                         'title': title,
+                         'url': link,
+                         'publish_time': date,
+                         'summary': summary
+                     }
+                     
+                     # 只添加有效的新闻（至少有标题和链接）
+                     if title and link:
+                         news_list.append(news)
+                         
+                 except Exception as e:
+                     logger.error(f"解析新闻列表项时出错: {str(e)}")
+                     continue
+             
+             return news_list
+             
+         except Exception as e:
+             logger.error(f"解析列表页面失败: {str(e)}")
+             return []
+     
+     def parse_detail(self, content: str, url: str = None) -> Dict:
+         """
+         解析详情页面，提取新闻详情
+         
+         Args:
+             content: HTML内容
+             url: 页面URL
+             
+         Returns:
+             Dict: 新闻详情
+         """
+         if not content:
+             logger.warning("解析详情页面失败：内容为空")
+             return {}
+             
+         # 尝试修复编码问题
+         content = self._fix_encoding(content)
+         
+         try:
+             soup = BeautifulSoup(content, 'html.parser')
+             
+             # 提取标题
+             title_elem = soup.select_one(self.detail_selector['title'])
+             title = title_elem.get_text(strip=True) if title_elem else ""
+             
+             # 提取内容
+             content_elem = soup.select_one(self.detail_selector['content'])
+             if content_elem:
+                 # 清理内容
+                 content_html = str(content_elem)
+                 content_text = self.clean_content(content_html)
+             else:
+                 content_html = ""
+                 content_text = ""
+             
+             # 提取日期
+             date_elem = soup.select_one(self.detail_selector['date'])
+             date_str = date_elem.get_text(strip=True) if date_elem else ""
+             date = self._parse_date(date_str)
+             
+             # 提取作者
+             author_elem = soup.select_one(self.detail_selector['author'])
+             author = author_elem.get_text(strip=True) if author_elem else ""
+             
+             # 提取来源
+             source_elem = soup.select_one(self.detail_selector['source'])
+             source = source_elem.get_text(strip=True) if source_elem else ""
+             
+             # 提取分类
+             category_elem = soup.select_one(self.detail_selector['category'])
+             category = category_elem.get_text(strip=True) if category_elem else ""
+             
+             # 提取关键词
+             keywords = extract_keywords(content_text)
+             
+             # 提取图片
+             images = self._extract_images(soup, self.detail_selector['content'])
+             
+             # 创建新闻详情字典
+             news_detail = {
+                 'title': title,
+                 'content': content_text,
+                 'content_html': content_html,
+                 'publish_time': date,
+                 'author': author,
+                 'source': source,
+                 'category': category,
+                 'url': url,
+                 'keywords': keywords,
+                 'images': images,
+                 'crawl_time': datetime.datetime.now().strftime(self.date_format)
+             }
+             
+             return news_detail
+             
+         except Exception as e:
+             logger.error(f"解析详情页面失败: {str(e)}")
+             return {}
+     
+     def clean_content(self, content: str) -> str:
+         """
+         清理内容，去除广告、无用标签等
+         
+         Args:
+             content: HTML内容
+             
+         Returns:
+             str: 清理后的文本内容
+         """
+         if not content:
+             return ""
+             
+         try:
+             # 使用BeautifulSoup解析HTML
+             soup = BeautifulSoup(content, 'html.parser')
+             
+             # 移除不需要的标签
+             for tag in soup(['script', 'style', 'iframe', 'form', 'ins', 'div.advertisement', 'div.ad']):
+                 tag.decompose()
+             
+             # 移除空节点
+             for tag in soup.find_all(lambda t: t.name and not t.contents and not t.string):
+                 tag.decompose()
+             
+             # 去除评论区
+             for tag in soup.select('div.comment, div.comments, div#comments, div.comment-area'):
+                 tag.decompose()
+             
+             # 清理属性
+             for tag in soup.find_all(True):
+                 # 保留部分有用属性
+                 allowed_attrs = ['href', 'src', 'alt', 'title']
+                 for attr in list(tag.attrs):
+                     if attr not in allowed_attrs:
+                         del tag.attrs[attr]
+             
+             # 获取清理后的HTML
+             cleaned_html = str(soup)
+             
+             # 使用内置的HTML清理功能
+             cleaned_text = clean_html_content(cleaned_html)
+             
+             return cleaned_text.strip()
+             
+         except Exception as e:
+             logger.error(f"清理内容时出错: {str(e)}")
+             return ""
+     
+     def _parse_date(self, date_str: str) -> str:
+         """
+         解析日期字符串
+         
+         Args:
+             date_str: 日期字符串
+             
+         Returns:
+             str: 标准格式的日期字符串
+         """
+         if not date_str:
+             return datetime.datetime.now().strftime(self.date_format)
+             
+         # 清理日期字符串
+         date_str = date_str.strip()
+         date_str = re.sub(r'[\[\]()]', '', date_str)
+         date_str = re.sub(r'发布时间：|日期：|时间：|来源：|作者：', '', date_str)
+         date_str = date_str.strip()
+         
+         # 常见的日期格式
+         formats = [
+             '%Y-%m-%d %H:%M:%S',
+             '%Y-%m-%d %H:%M',
+             '%Y年%m月%d日 %H:%M:%S',
+             '%Y年%m月%d日 %H:%M',
+             '%Y年%m月%d日 %H时%M分',
+             '%Y/%m/%d %H:%M:%S',
+             '%Y/%m/%d %H:%M',
+             '%Y.%m.%d %H:%M:%S',
+             '%Y.%m.%d %H:%M',
+             '%m-%d %H:%M',
+             '%m月%d日 %H:%M',
+         ]
+         
+         # 尝试不同的格式解析日期
+         for fmt in formats:
+             try:
+                 # 尝试解析日期
+                 dt = datetime.datetime.strptime(date_str, fmt)
+                 
+                 # 如果年份缺失（如月日时间格式），添加当前年份
+                 if dt.year == 1900:
+                     dt = dt.replace(year=datetime.datetime.now().year)
+                 
+                 # 返回标准格式的日期字符串
+                 return dt.strftime(self.date_format)
+             except ValueError:
+                 continue
+         
+         # 尝试识别相对日期表达
+         try:
+             # 处理"X分钟前"、"X小时前"等表达
+             if '分钟前' in date_str:
+                 minutes = int(re.search(r'(\d+)分钟前', date_str).group(1))
+                 dt = datetime.datetime.now() - datetime.timedelta(minutes=minutes)
+                 return dt.strftime(self.date_format)
+             elif '小时前' in date_str:
+                 hours = int(re.search(r'(\d+)小时前', date_str).group(1))
+                 dt = datetime.datetime.now() - datetime.timedelta(hours=hours)
+                 return dt.strftime(self.date_format)
+             elif '天前' in date_str:
+                 days = int(re.search(r'(\d+)天前', date_str).group(1))
+                 dt = datetime.datetime.now() - datetime.timedelta(days=days)
+                 return dt.strftime(self.date_format)
+         except Exception:
+             pass
+         
+         # 如果无法解析，使用当前时间
+         logger.warning(f"无法解析日期: {date_str}，使用当前时间")
+         return datetime.datetime.now().strftime(self.date_format)
+     
+     def _extract_images(self, soup: BeautifulSoup, content_selector: str) -> List[str]:
+         """
+         提取图片URL
+         
+         Args:
+             soup: BeautifulSoup对象
+             content_selector: 内容选择器
+             
+         Returns:
+             List[str]: 图片URL列表
+         """
+         images = []
+         
+         try:
+             # 尝试查找内容区域
+             content_elem = soup.select_one(content_selector)
+             if content_elem:
+                 # 在内容区域中查找图片
+                 for img in content_elem.find_all('img'):
+                     if img.has_attr('src'):
+                         img_url = img['src']
+                         # 处理图片URL
+                         img_url = decode_url_encoded(img_url)
+                         images.append(img_url)
+             
+             # 去重
+             images = list(set(images))
+             
+             return images
+             
+         except Exception as e:
+             logger.error(f"提取图片时出错: {str(e)}")
+             return []
+     
+     def _fix_encoding(self, content: str) -> str:
+         """
+         修复编码问题
+         
+         Args:
+             content: HTML内容
+             
+         Returns:
+             str: 修复编码后的内容
+         """
+         # 应用各种解码方法
+         content = decode_html_entities(content)
+         content = decode_unicode_escape(content)
+         content = decode_url_encoded(content)
+         
+         return content
+ 
+ 
+ class JSONParser(BaseParser):
+     """JSON解析器，解析JSON格式数据"""
+     
+     def __init__(self, list_mapping: Optional[Dict] = None, 
+                  detail_mapping: Optional[Dict] = None,
+                  date_format: str = '%Y-%m-%d %H:%M:%S'):
+         """
+         初始化JSON解析器
+         
+         Args:
+             list_mapping: 列表字段映射
+             detail_mapping: 详情字段映射
+             date_format: 日期格式
+         """
+         # 默认列表映射
+         self.list_mapping = {
+             'container': 'data.list',
+             'title': 'title',
+             'link': 'url',
+             'date': 'publishTime',
+             'summary': 'summary'
+         }
+         
+         # 默认详情映射
+         self.detail_mapping = {
+             'title': 'data.title',
+             'content': 'data.content',
+             'date': 'data.publishTime',
+             'author': 'data.author',
+             'source': 'data.source',
+             'category': 'data.category'
+         }
+         
+         # 更新映射
+         if list_mapping:
+             self.list_mapping.update(list_mapping)
+         if detail_mapping:
+             self.detail_mapping.update(detail_mapping)
+             
+         self.date_format = date_format
+         
+     def parse_list(self, content: str, url: str = None) -> List[Dict]:
+         """
+         解析列表页面，提取新闻列表
+         
+         Args:
+             content: JSON内容
+             url: 页面URL
+             
+         Returns:
+             List[Dict]: 新闻列表
+         """
+         if not content:
+             logger.warning("解析列表页面失败：内容为空")
+             return []
+             
+         try:
+             # 尝试解析JSON
+             data = json.loads(content)
+             
+             # 获取数据容器
+             container_path = self.list_mapping['container'].split('.')
+             container = data
+             for key in container_path:
+                 if key in container:
+                     container = container[key]
+                 else:
+                     logger.warning(f"找不到容器路径: {self.list_mapping['container']}")
+                     return []
+             
+             # 确保容器是列表
+             if not isinstance(container, list):
+                 logger.warning("数据容器不是列表")
+                 return []
+             
+             news_list = []
+             
+             for item in container:
+                 try:
+                     # 提取标题
+                     title = self._get_value_by_path(item, self.list_mapping['title'])
+                     
+                     # 提取链接
+                     link = self._get_value_by_path(item, self.list_mapping['link'])
+                     
+                     # 提取日期
+                     date_str = self._get_value_by_path(item, self.list_mapping['date'])
+                     date = self._parse_date(date_str)
+                     
+                     # 提取摘要
+                     summary = self._get_value_by_path(item, self.list_mapping['summary'])
+                     
+                     # 创建新闻字典
+                     news = {
+                         'title': title,
+                         'url': link,
+                         'publish_time': date,
+                         'summary': summary
+                     }
+                     
+                     # 只添加有效的新闻（至少有标题和链接）
+                     if title and link:
+                         news_list.append(news)
+                         
+                 except Exception as e:
+                     logger.error(f"解析新闻列表项时出错: {str(e)}")
+                     continue
+             
+             return news_list
+             
+         except json.JSONDecodeError as e:
+             logger.error(f"JSON解析失败: {str(e)}")
+             return []
+         except Exception as e:
+             logger.error(f"解析列表页面失败: {str(e)}")
+             return []
+     
+     def parse_detail(self, content: str, url: str = None) -> Dict:
+         """
+         解析详情页面，提取新闻详情
+         
+         Args:
+             content: JSON内容
+             url: 页面URL
+             
+         Returns:
+             Dict: 新闻详情
+         """
+         if not content:
+             logger.warning("解析详情页面失败：内容为空")
+             return {}
+             
+         try:
+             # 尝试解析JSON
+             data = json.loads(content)
+             
+             # 提取各字段
+             title = self._get_value_by_path(data, self.detail_mapping['title'])
+             content_html = self._get_value_by_path(data, self.detail_mapping['content'])
+             content_text = self.clean_content(content_html)
+             date_str = self._get_value_by_path(data, self.detail_mapping['date'])
+             date = self._parse_date(date_str)
+             author = self._get_value_by_path(data, self.detail_mapping['author'])
+             source = self._get_value_by_path(data, self.detail_mapping['source'])
+             category = self._get_value_by_path(data, self.detail_mapping['category'])
+             
+             # 提取关键词
+             keywords = extract_keywords(content_text)
+             
+             # 提取图片
+             images = self._extract_images_from_html(content_html)
+             
+             # 创建新闻详情字典
+             news_detail = {
+                 'title': title,
+                 'content': content_text,
+                 'content_html': content_html,
+                 'publish_time': date,
+                 'author': author,
+                 'source': source,
+                 'category': category,
+                 'url': url,
+                 'keywords': keywords,
+                 'images': images,
+                 'crawl_time': datetime.datetime.now().strftime(self.date_format)
+             }
+             
+             return news_detail
+             
+         except json.JSONDecodeError as e:
+             logger.error(f"JSON解析失败: {str(e)}")
+             return {}
+         except Exception as e:
+             logger.error(f"解析详情页面失败: {str(e)}")
+             return {}
+     
+     def clean_content(self, content: str) -> str:
+         """
+         清理内容，去除HTML标签等
+         
+         Args:
+             content: HTML内容
+             
+         Returns:
+             str: 清理后的文本内容
+         """
+         if not content:
+             return ""
+             
+         # 去除HTML标签
+         return clean_html_content(content)
+     
+     def _get_value_by_path(self, data: Any, path: str) -> Any:
+         """
+         根据路径获取数据
+         
+         Args:
+             data: 数据对象
+             path: 路径（如'data.list.0.title'）
+             
+         Returns:
+             Any: 路径对应的数据
+         """
+         if not path:
+             return None
+             
+         # 处理数组索引
+         parts = []
+         for part in path.split('.'):
+             # 检查是否是数组索引
+             if part.isdigit():
+                 parts.append(int(part))
+             else:
+                 parts.append(part)
+         
+         # 获取值
+         value = data
+         for part in parts:
+             if isinstance(value, dict) and part in value:
+                 value = value[part]
+             elif isinstance(value, list) and isinstance(part, int) and 0 <= part < len(value):
+                 value = value[part]
+             else:
+                 return None
+         
+         return value
+     
+     def _parse_date(self, date_str: str) -> str:
+         """
+         解析日期字符串
+         
+         Args:
+             date_str: 日期字符串
+             
+         Returns:
+             str: 标准格式的日期字符串
+         """
+         # 如果是时间戳
+         if isinstance(date_str, (int, float)):
+             # 假设是毫秒级时间戳
+             if date_str > 1000000000000:
+                 date_str = date_str / 1000
+             return datetime.datetime.fromtimestamp(date_str).strftime(self.date_format)
+             
+         # 如果是字符串
+         if isinstance(date_str, str):
+             # 尝试解析，使用与HTMLParser相同的方法
+             html_parser = HTMLParser(date_format=self.date_format)
+             return html_parser._parse_date(date_str)
+         
+         # 默认返回当前时间
+         return datetime.datetime.now().strftime(self.date_format)
+     
+     def _extract_images_from_html(self, html: str) -> List[str]:
+         """
+         从HTML内容中提取图片URL
+         
+         Args:
+             html: HTML内容
+             
+         Returns:
+             List[str]: 图片URL列表
+         """
+         if not html:
+             return []
+             
+         images = []
+         
+         try:
+             # 使用BeautifulSoup解析HTML
+             soup = BeautifulSoup(html, 'html.parser')
+             
+             # 提取所有图片
+             for img in soup.find_all('img'):
+                 if img.has_attr('src'):
+                     img_url = img['src']
+                     # 处理图片URL
+                     img_url = decode_url_encoded(img_url)
+                     images.append(img_url)
+             
+             # 去重
+             images = list(set(images))
+             
+             return images
+             
+         except Exception as e:
+             logger.error(f"从HTML中提取图片时出错: {str(e)}")
+             return []
+ 
+ 
+ def create_parser(parser_type: str = 'html', **kwargs) -> BaseParser:
+     """
+     创建解析器工厂函数
+     
+     Args:
+         parser_type: 解析器类型（'html'或'json'）
+         **kwargs: 其他参数
+         
+     Returns:
+         BaseParser: 解析器实例
+     """
+     if parser_type.lower() == 'json':
+         return JSONParser(**kwargs)
+     else:
+         return HTMLParser(**kwargs) 
```

---

```diff
- 
+ #!/usr/bin/env python3
+ # -*- coding: utf-8 -*-
+ 
+ """
+ 财经新闻爬虫系统 - 存储器组件
+ 负责存储爬取的新闻数据
+ """
+ 
+ import os
+ import json
+ import sqlite3
+ import logging
+ import datetime
+ from typing import Dict, List, Optional, Any, Union, Tuple
+ from abc import ABC, abstractmethod
+ 
+ from app.utils.logger import get_crawler_logger
+ 
+ # 使用专门的爬虫日志记录器
+ logger = get_crawler_logger('storers')
+ 
+ class BaseStorer(ABC):
+     """存储器基类，定义存储器的接口"""
+     
+     @abstractmethod
+     def save(self, data: Union[Dict, List[Dict]]) -> bool:
+         """保存数据"""
+         pass
+     
+     @abstractmethod
+     def get(self, query: Dict) -> List[Dict]:
+         """获取数据"""
+         pass
+     
+     @abstractmethod
+     def update(self, query: Dict, data: Dict) -> bool:
+         """更新数据"""
+         pass
+     
+     @abstractmethod
+     def delete(self, query: Dict) -> bool:
+         """删除数据"""
+         pass
+ 
+ 
+ class SQLiteStorer(BaseStorer):
+     """SQLite存储器，将数据存储到SQLite数据库"""
+     
+     def __init__(self, db_path: str, table_name: str = 'news'):
+         """
+         初始化SQLite存储器
+         
+         Args:
+             db_path: 数据库路径
+             table_name: 表名
+         """
+         self.db_path = db_path
+         self.table_name = table_name
+         
+         # 确保数据库目录存在
+         os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
+         
+         # 初始化数据库连接
+         self.conn = None
+         try:
+             self.conn = sqlite3.connect(self.db_path)
+             self._init_table()
+             logger.info(f"SQLite存储器初始化成功: {self.db_path}, 表: {self.table_name}")
+         except Exception as e:
+             logger.error(f"SQLite存储器初始化失败: {str(e)}")
+     
+     def __del__(self):
+         """析构函数，关闭数据库连接"""
+         try:
+             if self.conn:
+                 self.conn.close()
+                 logger.debug("数据库连接已关闭")
+         except Exception as e:
+             logger.error(f"关闭数据库连接时出错: {str(e)}")
+     
+     def _init_table(self):
+         """初始化数据库表结构"""
+         try:
+             cursor = self.conn.cursor()
+             
+             # 创建新闻表
+             cursor.execute(f'''
+             CREATE TABLE IF NOT EXISTS {self.table_name} (
+                 id TEXT PRIMARY KEY,
+                 title TEXT NOT NULL,
+                 content TEXT,
+                 content_html TEXT,
+                 url TEXT UNIQUE,
+                 publish_time TEXT,
+                 crawl_time TEXT,
+                 source TEXT,
+                 category TEXT,
+                 author TEXT,
+                 keywords TEXT,
+                 sentiment REAL,
+                 summary TEXT,
+                 images TEXT
+             )
+             ''')
+             
+             # 创建索引
+             cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_{self.table_name}_url ON {self.table_name}(url)')
+             cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_{self.table_name}_source ON {self.table_name}(source)')
+             cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_{self.table_name}_category ON {self.table_name}(category)')
+             cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_{self.table_name}_publish_time ON {self.table_name}(publish_time)')
+             
+             self.conn.commit()
+             logger.info(f"数据库表 {self.table_name} 初始化成功")
+         except Exception as e:
+             logger.error(f"初始化数据库表失败: {str(e)}")
+     
+     def save(self, data: Union[Dict, List[Dict]]) -> bool:
+         """
+         保存数据到数据库
+         
+         Args:
+             data: 单条新闻数据或新闻数据列表
+             
+         Returns:
+             bool: 保存是否成功
+         """
+         # 确保连接可用
+         if not self.conn:
+             try:
+                 self.conn = sqlite3.connect(self.db_path)
+             except Exception as e:
+                 logger.error(f"连接数据库失败: {str(e)}")
+                 return False
+         
+         # 转换单条数据为列表
+         if isinstance(data, dict):
+             data_list = [data]
+         else:
+             data_list = data
+         
+         # 开始事务
+         self.conn.isolation_level = 'DEFERRED'
+         
+         try:
+             cursor = self.conn.cursor()
+             
+             # 准备SQL语句
+             sql = f'''
+             INSERT OR REPLACE INTO {self.table_name} 
+             (id, title, content, content_html, url, publish_time, crawl_time, 
+              source, category, author, keywords, sentiment, summary, images)
+             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
+             '''
+             
+             # 批量插入数据
+             count = 0
+             for item in data_list:
+                 try:
+                     # 如果没有ID，生成一个
+                     if 'id' not in item or not item['id']:
+                         import hashlib
+                         id_str = f"{item.get('url', '')}{item.get('title', '')}"
+                         item['id'] = hashlib.md5(id_str.encode('utf-8')).hexdigest()
+                     
+                     # 处理列表类型的字段（关键词、图片）
+                     keywords = item.get('keywords', [])
+                     if isinstance(keywords, list):
+                         keywords = ','.join(keywords)
+                     
+                     images = item.get('images', [])
+                     if isinstance(images, list):
+                         images = json.dumps(images)
+                     
+                     # 添加当前时间为爬取时间
+                     if 'crawl_time' not in item or not item['crawl_time']:
+                         item['crawl_time'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
+                     
+                     # 执行插入
+                     cursor.execute(sql, (
+                         item.get('id', ''),
+                         item.get('title', ''),
+                         item.get('content', ''),
+                         item.get('content_html', ''),
+                         item.get('url', ''),
+                         item.get('publish_time', ''),
+                         item.get('crawl_time', ''),
+                         item.get('source', ''),
+                         item.get('category', ''),
+                         item.get('author', ''),
+                         keywords,
+                         item.get('sentiment', 0.0),
+                         item.get('summary', ''),
+                         images
+                     ))
+                     count += 1
+                 except Exception as e:
+                     logger.error(f"插入数据时出错: {str(e)}, 数据: {item}")
+                     continue
+             
+             # 提交事务
+             self.conn.commit()
+             logger.info(f"成功保存 {count}/{len(data_list)} 条数据到 {self.table_name}")
+             return count > 0
+             
+         except Exception as e:
+             # 回滚事务
+             self.conn.rollback()
+             logger.error(f"保存数据失败: {str(e)}")
+             return False
+     
+     def get(self, query: Dict = None) -> List[Dict]:
+         """
+         获取数据
+         
+         Args:
+             query: 查询条件
+             
+         Returns:
+             List[Dict]: 查询结果
+         """
+         # 确保连接可用
+         if not self.conn:
+             try:
+                 self.conn = sqlite3.connect(self.db_path)
+             except Exception as e:
+                 logger.error(f"连接数据库失败: {str(e)}")
+                 return []
+         
+         try:
+             cursor = self.conn.cursor()
+             
+             # 构建SQL查询
+             sql = f'SELECT * FROM {self.table_name}'
+             params = []
+             
+             if query:
+                 conditions = []
+                 for key, value in query.items():
+                     if key in ['publish_time_start', 'publish_time_end']:
+                         continue
+                     conditions.append(f'{key} = ?')
+                     params.append(value)
+                 
+                 # 处理日期范围查询
+                 if 'publish_time_start' in query and query['publish_time_start']:
+                     conditions.append('publish_time >= ?')
+                     params.append(query['publish_time_start'])
+                 
+                 if 'publish_time_end' in query and query['publish_time_end']:
+                     conditions.append('publish_time <= ?')
+                     params.append(query['publish_time_end'])
+                 
+                 if conditions:
+                     sql += ' WHERE ' + ' AND '.join(conditions)
+             
+             # 添加排序
+             sql += ' ORDER BY publish_time DESC'
+             
+             # 执行查询
+             if params:
+                 cursor.execute(sql, params)
+             else:
+                 cursor.execute(sql)
+             
+             # 获取列名
+             columns = [col[0] for col in cursor.description]
+             
+             # 解析结果
+             result = []
+             for row in cursor.fetchall():
+                 item = dict(zip(columns, row))
+                 
+                 # 处理JSON字段
+                 if 'images' in item and item['images']:
+                     try:
+                         item['images'] = json.loads(item['images'])
+                     except json.JSONDecodeError:
+                         item['images'] = []
+                 
+                 if 'keywords' in item and item['keywords']:
+                     item['keywords'] = item['keywords'].split(',')
+                 
+                 result.append(item)
+             
+             return result
+             
+         except Exception as e:
+             logger.error(f"查询数据失败: {str(e)}")
+             return []
+     
+     def update(self, query: Dict, data: Dict) -> bool:
+         """
+         更新数据
+         
+         Args:
+             query: 查询条件
+             data: 更新数据
+             
+         Returns:
+             bool: 更新是否成功
+         """
+         # 确保连接可用
+         if not self.conn:
+             try:
+                 self.conn = sqlite3.connect(self.db_path)
+             except Exception as e:
+                 logger.error(f"连接数据库失败: {str(e)}")
+                 return False
+         
+         if not query or not data:
+             logger.warning("更新数据失败：查询条件或更新数据为空")
+             return False
+         
+         try:
+             cursor = self.conn.cursor()
+             
+             # 构建SQL更新
+             set_clauses = []
+             set_params = []
+             
+             for key, value in data.items():
+                 # 处理特殊字段
+                 if key in ['keywords', 'images'] and isinstance(value, list):
+                     if key == 'keywords':
+                         value = ','.join(value)
+                     elif key == 'images':
+                         value = json.dumps(value)
+                 
+                 set_clauses.append(f'{key} = ?')
+                 set_params.append(value)
+             
+             # 构建WHERE条件
+             where_clauses = []
+             where_params = []
+             
+             for key, value in query.items():
+                 where_clauses.append(f'{key} = ?')
+                 where_params.append(value)
+             
+             # 合并参数
+             params = set_params + where_params
+             
+             # 构建完整SQL
+             sql = f'''
+             UPDATE {self.table_name} 
+             SET {', '.join(set_clauses)} 
+             WHERE {' AND '.join(where_clauses)}
+             '''
+             
+             # 执行更新
+             cursor.execute(sql, params)
+             self.conn.commit()
+             
+             affected_rows = cursor.rowcount
+             logger.info(f"成功更新 {affected_rows} 条数据")
+             return affected_rows > 0
+             
+         except Exception as e:
+             self.conn.rollback()
+             logger.error(f"更新数据失败: {str(e)}")
+             return False
+     
+     def delete(self, query: Dict) -> bool:
+         """
+         删除数据
+         
+         Args:
+             query: 查询条件
+             
+         Returns:
+             bool: 删除是否成功
+         """
+         # 确保连接可用
+         if not self.conn:
+             try:
+                 self.conn = sqlite3.connect(self.db_path)
+             except Exception as e:
+                 logger.error(f"连接数据库失败: {str(e)}")
+                 return False
+         
+         if not query:
+             logger.warning("删除数据失败：查询条件为空")
+             return False
+         
+         try:
+             cursor = self.conn.cursor()
+             
+             # 构建WHERE条件
+             where_clauses = []
+             params = []
+             
+             for key, value in query.items():
+                 where_clauses.append(f'{key} = ?')
+                 params.append(value)
+             
+             # 构建完整SQL
+             sql = f'''
+             DELETE FROM {self.table_name} 
+             WHERE {' AND '.join(where_clauses)}
+             '''
+             
+             # 执行删除
+             cursor.execute(sql, params)
+             self.conn.commit()
+             
+             affected_rows = cursor.rowcount
+             logger.info(f"成功删除 {affected_rows} 条数据")
+             return affected_rows > 0
+             
+         except Exception as e:
+             self.conn.rollback()
+             logger.error(f"删除数据失败: {str(e)}")
+             return False
+     
+     def get_count(self, query: Dict = None) -> int:
+         """
+         获取数据数量
+         
+         Args:
+             query: 查询条件
+             
+         Returns:
+             int: 数据数量
+         """
+         # 确保连接可用
+         if not self.conn:
+             try:
+                 self.conn = sqlite3.connect(self.db_path)
+             except Exception as e:
+                 logger.error(f"连接数据库失败: {str(e)}")
+                 return 0
+         
+         try:
+             cursor = self.conn.cursor()
+             
+             # 构建SQL查询
+             sql = f'SELECT COUNT(*) FROM {self.table_name}'
+             params = []
+             
+             if query:
+                 conditions = []
+                 for key, value in query.items():
+                     if key in ['publish_time_start', 'publish_time_end']:
+                         continue
+                     conditions.append(f'{key} = ?')
+                     params.append(value)
+                 
+                 # 处理日期范围查询
+                 if 'publish_time_start' in query and query['publish_time_start']:
+                     conditions.append('publish_time >= ?')
+                     params.append(query['publish_time_start'])
+                 
+                 if 'publish_time_end' in query and query['publish_time_end']:
+                     conditions.append('publish_time <= ?')
+                     params.append(query['publish_time_end'])
+                 
+                 if conditions:
+                     sql += ' WHERE ' + ' AND '.join(conditions)
+             
+             # 执行查询
+             if params:
+                 cursor.execute(sql, params)
+             else:
+                 cursor.execute(sql)
+             
+             # 获取结果
+             count = cursor.fetchone()[0]
+             return count
+             
+         except Exception as e:
+             logger.error(f"查询数据数量失败: {str(e)}")
+             return 0
+ 
+ 
+ class JSONFileStorer(BaseStorer):
+     """JSON文件存储器，将数据存储到JSON文件"""
+     
+     def __init__(self, file_path: str, encoding: str = 'utf-8'):
+         """
+         初始化JSON文件存储器
+         
+         Args:
+             file_path: 文件路径
+             encoding: 文件编码
+         """
+         self.file_path = file_path
+         self.encoding = encoding
+         
+         # 确保目录存在
+         os.makedirs(os.path.dirname(self.file_path), exist_ok=True)
+         
+         # 初始化文件
+         if not os.path.exists(self.file_path):
+             with open(self.file_path, 'w', encoding=self.encoding) as f:
+                 json.dump([], f, ensure_ascii=False, indent=2)
+             logger.info(f"JSON文件已创建: {self.file_path}")
+         
+         logger.info(f"JSON文件存储器初始化成功: {self.file_path}")
+     
+     def save(self, data: Union[Dict, List[Dict]]) -> bool:
+         """
+         保存数据到JSON文件
+         
+         Args:
+             data: 单条新闻数据或新闻数据列表
+             
+         Returns:
+             bool: 保存是否成功
+         """
+         try:
+             # 读取现有数据
+             existing_data = []
+             try:
+                 with open(self.file_path, 'r', encoding=self.encoding) as f:
+                     existing_data = json.load(f)
+             except (json.JSONDecodeError, FileNotFoundError):
+                 existing_data = []
+             
+             # 准备新数据
+             if isinstance(data, dict):
+                 data_list = [data]
+             else:
+                 data_list = data
+             
+             # 添加ID
+             for item in data_list:
+                 if 'id' not in item or not item['id']:
+                     import hashlib
+                     id_str = f"{item.get('url', '')}{item.get('title', '')}"
+                     item['id'] = hashlib.md5(id_str.encode('utf-8')).hexdigest()
+                 
+                 # 添加当前时间为爬取时间
+                 if 'crawl_time' not in item or not item['crawl_time']:
+                     item['crawl_time'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
+             
+             # 找出已存在的数据
+             existing_ids = {item.get('id') for item in existing_data}
+             
+             # 合并数据
+             for item in data_list:
+                 if item.get('id') not in existing_ids:
+                     existing_data.append(item)
+                 else:
+                     # 更新已存在的数据
+                     for i, existing_item in enumerate(existing_data):
+                         if existing_item.get('id') == item.get('id'):
+                             existing_data[i] = item
+                             break
+             
+             # 写入文件
+             with open(self.file_path, 'w', encoding=self.encoding) as f:
+                 json.dump(existing_data, f, ensure_ascii=False, indent=2)
+             
+             logger.info(f"成功保存 {len(data_list)} 条数据到 {self.file_path}")
+             return True
+             
+         except Exception as e:
+             logger.error(f"保存数据到JSON文件失败: {str(e)}")
+             return False
+     
+     def get(self, query: Dict = None) -> List[Dict]:
+         """
+         获取数据
+         
+         Args:
+             query: 查询条件
+             
+         Returns:
+             List[Dict]: 查询结果
+         """
+         try:
+             # 读取所有数据
+             with open(self.file_path, 'r', encoding=self.encoding) as f:
+                 data = json.load(f)
+             
+             # 如果没有查询条件，返回所有数据
+             if not query:
+                 return data
+             
+             # 过滤数据
+             result = []
+             for item in data:
+                 match = True
+                 
+                 for key, value in query.items():
+                     # 处理日期范围查询
+                     if key == 'publish_time_start' and 'publish_time' in item:
+                         if item['publish_time'] < value:
+                             match = False
+                             break
+                     elif key == 'publish_time_end' and 'publish_time' in item:
+                         if item['publish_time'] > value:
+                             match = False
+                             break
+                     elif key in item and item[key] != value:
+                         match = False
+                         break
+                 
+                 if match:
+                     result.append(item)
+             
+             # 按发布时间排序
+             result.sort(key=lambda x: x.get('publish_time', ''), reverse=True)
+             
+             return result
+             
+         except (json.JSONDecodeError, FileNotFoundError) as e:
+             logger.error(f"读取JSON文件失败: {str(e)}")
+             return []
+     
+     def update(self, query: Dict, data: Dict) -> bool:
+         """
+         更新数据
+         
+         Args:
+             query: 查询条件
+             data: 更新数据
+             
+         Returns:
+             bool: 更新是否成功
+         """
+         try:
+             # 读取所有数据
+             with open(self.file_path, 'r', encoding=self.encoding) as f:
+                 all_data = json.load(f)
+             
+             # 查找并更新数据
+             updated = False
+             for i, item in enumerate(all_data):
+                 match = True
+                 
+                 for key, value in query.items():
+                     if key not in item or item[key] != value:
+                         match = False
+                         break
+                 
+                 if match:
+                     # 更新数据
+                     all_data[i].update(data)
+                     updated = True
+             
+             # 如果有更新，写入文件
+             if updated:
+                 with open(self.file_path, 'w', encoding=self.encoding) as f:
+                     json.dump(all_data, f, ensure_ascii=False, indent=2)
+                 
+                 logger.info(f"成功更新数据")
+                 return True
+             else:
+                 logger.warning(f"未找到匹配的数据进行更新")
+                 return False
+                 
+         except Exception as e:
+             logger.error(f"更新数据失败: {str(e)}")
+             return False
+     
+     def delete(self, query: Dict) -> bool:
+         """
+         删除数据
+         
+         Args:
+             query: 查询条件
+             
+         Returns:
+             bool: 删除是否成功
+         """
+         try:
+             # 读取所有数据
+             with open(self.file_path, 'r', encoding=self.encoding) as f:
+                 all_data = json.load(f)
+             
+             # 过滤数据
+             original_length = len(all_data)
+             filtered_data = []
+             
+             for item in all_data:
+                 match = True
+                 
+                 for key, value in query.items():
+                     if key not in item or item[key] != value:
+                         match = False
+                         break
+                 
+                 if not match:
+                     filtered_data.append(item)
+             
+             # 如果有数据被删除，写入文件
+             if len(filtered_data) < original_length:
+                 with open(self.file_path, 'w', encoding=self.encoding) as f:
+                     json.dump(filtered_data, f, ensure_ascii=False, indent=2)
+                 
+                 deleted_count = original_length - len(filtered_data)
+                 logger.info(f"成功删除 {deleted_count} 条数据")
+                 return True
+             else:
+                 logger.warning("未找到匹配的数据进行删除")
+                 return False
+                 
+         except Exception as e:
+             logger.error(f"删除数据失败: {str(e)}")
+             return False
+ 
+ 
+ def create_storer(storer_type: str = 'sqlite', **kwargs) -> BaseStorer:
+     """
+     创建存储器工厂函数
+     
+     Args:
+         storer_type: 存储器类型（'sqlite'或'json'）
+         **kwargs: 其他参数
+         
+     Returns:
+         BaseStorer: 存储器实例
+     """
+     if storer_type.lower() == 'json':
+         return JSONFileStorer(**kwargs)
+     else:
+         return SQLiteStorer(**kwargs) 
```

---

```diff
- 
+ #!/usr/bin/env python3
+ # -*- coding: utf-8 -*-
+ 
+ """
+ 财经新闻爬虫系统 - 监控器组件
+ 负责监控爬虫运行状态
+ """
+ 
+ import os
+ import time
+ import json
+ import logging
+ import threading
+ import datetime
+ from typing import Dict, List, Optional, Any, Callable
+ from abc import ABC, abstractmethod
+ 
+ from app.utils.logger import get_crawler_logger
+ 
+ # 使用专门的爬虫日志记录器
+ logger = get_crawler_logger('monitors')
+ 
+ class BaseMonitor(ABC):
+     """监控器基类，定义监控器的接口"""
+     
+     @abstractmethod
+     def start(self):
+         """开始监控"""
+         pass
+     
+     @abstractmethod
+     def stop(self):
+         """停止监控"""
+         pass
+     
+     @abstractmethod
+     def get_status(self) -> Dict:
+         """获取监控状态"""
+         pass
+ 
+ 
+ class PerformanceMonitor(BaseMonitor):
+     """性能监控器，监控爬虫性能数据"""
+     
+     def __init__(self, name: str, log_dir: Optional[str] = None, 
+                  interval: float = 60.0, max_records: int = 1000):
+         """
+         初始化性能监控器
+         
+         Args:
+             name: 监控器名称
+             log_dir: 日志目录
+             interval: 监控间隔（秒）
+             max_records: 最大记录数
+         """
+         self.name = name
+         self.interval = interval
+         self.max_records = max_records
+         self.running = False
+         self.thread = None
+         
+         # 监控数据
+         self.stats = {
+             'start_time': None,
+             'end_time': None,
+             'total_requests': 0,
+             'successful_requests': 0,
+             'failed_requests': 0,
+             'avg_response_time': 0.0,
+             'total_items_processed': 0,
+             'items_saved': 0,
+             'items_filtered': 0,
+             'memory_usage': 0,
+             'cpu_usage': 0,
+             'request_rate': 0.0,  # 每秒请求数
+             'throughput': 0.0,    # 每秒处理项目数
+         }
+         
+         # 时间序列数据
+         self.time_series = []
+         
+         # 设置日志路径
+         if log_dir:
+             self.log_dir = log_dir
+         else:
+             self.log_dir = os.path.join('logs', 'performance', self.name)
+         
+         os.makedirs(self.log_dir, exist_ok=True)
+         self.log_file = os.path.join(self.log_dir, f"{self.name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
+         
+         logger.info(f"性能监控器初始化成功: {self.name}, 间隔: {self.interval}秒, 日志: {self.log_file}")
+         
+     def start(self):
+         """开始监控"""
+         if self.running:
+             logger.warning(f"监控器 {self.name} 已经在运行")
+             return
+             
+         self.running = True
+         self.stats['start_time'] = datetime.datetime.now().isoformat()
+         
+         # 启动监控线程
+         self.thread = threading.Thread(target=self._monitor_task, daemon=True)
+         self.thread.start()
+         
+         logger.info(f"监控器 {self.name} 已启动")
+     
+     def stop(self):
+         """停止监控"""
+         if not self.running:
+             logger.warning(f"监控器 {self.name} 已经停止")
+             return
+             
+         self.running = False
+         self.stats['end_time'] = datetime.datetime.now().isoformat()
+         
+         # 等待线程结束
+         if self.thread and self.thread.is_alive():
+             self.thread.join(timeout=5)
+         
+         # 保存最终报告
+         self._save_report()
+         
+         logger.info(f"监控器 {self.name} 已停止")
+     
+     def get_status(self) -> Dict:
+         """
+         获取监控状态
+         
+         Returns:
+             Dict: 当前监控状态
+         """
+         # 计算运行时间
+         if self.stats['start_time']:
+             start_time = datetime.datetime.fromisoformat(self.stats['start_time'])
+             current_time = datetime.datetime.now()
+             runtime = (current_time - start_time).total_seconds()
+         else:
+             runtime = 0
+         
+         # 计算请求率和吞吐量
+         if runtime > 0:
+             self.stats['request_rate'] = self.stats['total_requests'] / runtime
+             self.stats['throughput'] = self.stats['total_items_processed'] / runtime
+         
+         # 获取内存和CPU使用情况
+         try:
+             import psutil
+             process = psutil.Process(os.getpid())
+             self.stats['memory_usage'] = process.memory_info().rss / 1024 / 1024  # MB
+             self.stats['cpu_usage'] = process.cpu_percent(interval=0.1)
+         except (ImportError, Exception) as e:
+             logger.debug(f"获取系统资源使用情况失败: {str(e)}")
+         
+         # 返回当前状态
+         status = self.stats.copy()
+         status['running'] = self.running
+         
+         # 添加最近的时间序列数据点(最多10个)
+         if self.time_series:
+             status['recent_data'] = self.time_series[-10:]
+         
+         return status
+     
+     def update_stats(self, new_stats: Dict):
+         """
+         更新监控数据
+         
+         Args:
+             new_stats: 新的统计数据
+         """
+         # 更新累计值
+         for key in ['total_requests', 'successful_requests', 'failed_requests', 
+                    'total_items_processed', 'items_saved', 'items_filtered']:
+             if key in new_stats:
+                 self.stats[key] += new_stats[key]
+         
+         # 更新平均响应时间（加权平均）
+         if 'response_time' in new_stats and 'successful_requests' in new_stats and new_stats['successful_requests'] > 0:
+             total_success = self.stats['successful_requests']
+             old_avg = self.stats['avg_response_time']
+             new_success = new_stats['successful_requests']
+             new_avg = new_stats['response_time']
+             
+             if total_success > 0:
+                 self.stats['avg_response_time'] = ((total_success - new_success) * old_avg + new_success * new_avg) / total_success
+         
+         # 添加时间戳
+         timestamp = datetime.datetime.now().isoformat()
+         data_point = {
+             'timestamp': timestamp,
+             **self.get_status()
+         }
+         
+         # 添加到时间序列
+         self.time_series.append(data_point)
+         
+         # 限制时间序列大小
+         if len(self.time_series) > self.max_records:
+             self.time_series = self.time_series[-self.max_records:]
+     
+     def _monitor_task(self):
+         """监控任务，定期收集和保存性能数据"""
+         last_save_time = time.time()
+         
+         while self.running:
+             try:
+                 # 获取当前状态
+                 status = self.get_status()
+                 
+                 # 记录到时间序列
+                 timestamp = datetime.datetime.now().isoformat()
+                 data_point = {
+                     'timestamp': timestamp,
+                     **status
+                 }
+                 self.time_series.append(data_point)
+                 
+                 # 限制时间序列大小
+                 if len(self.time_series) > self.max_records:
+                     self.time_series = self.time_series[-self.max_records:]
+                 
+                 # 定期保存报告
+                 current_time = time.time()
+                 if current_time - last_save_time >= 300:  # 每5分钟保存一次
+                     self._save_report()
+                     last_save_time = current_time
+                 
+                 # 暂停指定间隔
+                 time.sleep(self.interval)
+                 
+             except Exception as e:
+                 logger.error(f"监控任务出错: {str(e)}")
+                 time.sleep(self.interval)
+     
+     def _save_report(self):
+         """保存监控报告"""
+         try:
+             # 准备报告数据
+             report = {
+                 'stats': self.stats,
+                 'time_series': self.time_series
+             }
+             
+             # 写入JSON文件
+             with open(self.log_file, 'w', encoding='utf-8') as f:
+                 json.dump(report, f, ensure_ascii=False, indent=2)
+                 
+             logger.debug(f"监控报告已保存: {self.log_file}")
+             
+         except Exception as e:
+             logger.error(f"保存监控报告失败: {str(e)}")
+ 
+ 
+ class StatusMonitor(BaseMonitor):
+     """状态监控器，监控爬虫状态和警报"""
+     
+     def __init__(self, name: str, alert_callbacks: Optional[List[Callable]] = None):
+         """
+         初始化状态监控器
+         
+         Args:
+             name: 监控器名称
+             alert_callbacks: 警报回调函数列表
+         """
+         self.name = name
+         self.alert_callbacks = alert_callbacks or []
+         self.running = False
+         self.thread = None
+         
+         # 爬虫状态
+         self.status = {
+             'state': 'stopped',  # 'stopped', 'running', 'paused', 'error'
+             'last_activity': None,
+             'last_error': None,
+             'error_count': 0,
+             'warnings': [],
+             'messages': []
+         }
+         
+         logger.info(f"状态监控器初始化成功: {self.name}")
+     
+     def start(self):
+         """开始监控"""
+         if self.running:
+             logger.warning(f"状态监控器 {self.name} 已经在运行")
+             return
+             
+         self.running = True
+         self.status['state'] = 'running'
+         self.status['last_activity'] = datetime.datetime.now().isoformat()
+         
+         # 启动监控线程
+         self.thread = threading.Thread(target=self._monitor_task, daemon=True)
+         self.thread.start()
+         
+         logger.info(f"状态监控器 {self.name} 已启动")
+     
+     def stop(self):
+         """停止监控"""
+         if not self.running:
+             logger.warning(f"状态监控器 {self.name} 已经停止")
+             return
+             
+         self.running = False
+         self.status['state'] = 'stopped'
+         
+         # 等待线程结束
+         if self.thread and self.thread.is_alive():
+             self.thread.join(timeout=5)
+         
+         logger.info(f"状态监控器 {self.name} 已停止")
+     
+     def get_status(self) -> Dict:
+         """
+         获取监控状态
+         
+         Returns:
+             Dict: 当前状态
+         """
+         return self.status.copy()
+     
+     def update_activity(self):
+         """更新活动时间"""
+         self.status['last_activity'] = datetime.datetime.now().isoformat()
+     
+     def record_error(self, error: str):
+         """
+         记录错误
+         
+         Args:
+             error: 错误信息
+         """
+         timestamp = datetime.datetime.now().isoformat()
+         self.status['last_error'] = {'time': timestamp, 'message': error}
+         self.status['error_count'] += 1
+         
+         # 如果错误过多，更新状态为error
+         if self.status['error_count'] >= 10 and self.status['state'] != 'error':
+             self.status['state'] = 'error'
+             self._send_alert(f"爬虫 {self.name} 连续出现 {self.status['error_count']} 个错误，已标记为错误状态")
+     
+     def record_warning(self, warning: str):
+         """
+         记录警告
+         
+         Args:
+             warning: 警告信息
+         """
+         timestamp = datetime.datetime.now().isoformat()
+         self.status['warnings'].append({'time': timestamp, 'message': warning})
+         
+         # 限制警告数量
+         if len(self.status['warnings']) > 50:
+             self.status['warnings'] = self.status['warnings'][-50:]
+     
+     def record_message(self, message: str):
+         """
+         记录消息
+         
+         Args:
+             message: 消息内容
+         """
+         timestamp = datetime.datetime.now().isoformat()
+         self.status['messages'].append({'time': timestamp, 'message': message})
+         
+         # 限制消息数量
+         if len(self.status['messages']) > 100:
+             self.status['messages'] = self.status['messages'][-100:]
+     
+     def _monitor_task(self):
+         """监控任务，检查状态和发送警报"""
+         while self.running:
+             try:
+                 # 检查最后活动时间
+                 if self.status['last_activity']:
+                     last_activity = datetime.datetime.fromisoformat(self.status['last_activity'])
+                     current_time = datetime.datetime.now()
+                     inactive_duration = (current_time - last_activity).total_seconds()
+                     
+                     # 如果5分钟没有活动，发送警告
+                     if inactive_duration > 300 and self.status['state'] == 'running':
+                         warning = f"爬虫 {self.name} 已经 {int(inactive_duration)} 秒没有活动"
+                         self.record_warning(warning)
+                         logger.warning(warning)
+                         
+                         # 如果15分钟没有活动，视为错误
+                         if inactive_duration > 900:
+                             error = f"爬虫 {self.name} 已经 {int(inactive_duration)} 秒没有活动，可能已经卡住"
+                             self.record_error(error)
+                             self._send_alert(error)
+                 
+                 # 暂停一会儿
+                 time.sleep(30)
+                 
+             except Exception as e:
+                 logger.error(f"状态监控任务出错: {str(e)}")
+                 time.sleep(30)
+     
+     def _send_alert(self, message: str):
+         """
+         发送警报
+         
+         Args:
+             message: 警报消息
+         """
+         # 记录警报
+         logger.warning(f"警报: {message}")
+         
+         # 调用警报回调
+         for callback in self.alert_callbacks:
+             try:
+                 callback(message)
+             except Exception as e:
+                 logger.error(f"调用警报回调失败: {str(e)}")
+ 
+ 
+ class ResourceMonitor(BaseMonitor):
+     """资源监控器，监控系统资源使用情况"""
+     
+     def __init__(self, name: str, interval: float = 60.0, 
+                  alert_threshold: Dict[str, float] = None):
+         """
+         初始化资源监控器
+         
+         Args:
+             name: 监控器名称
+             interval: 监控间隔（秒）
+             alert_threshold: 警报阈值
+         """
+         self.name = name
+         self.interval = interval
+         self.running = False
+         self.thread = None
+         
+         # 默认警报阈值
+         self.alert_threshold = {
+             'cpu': 90.0,  # CPU使用率超过90%
+             'memory': 85.0,  # 内存使用率超过85%
+             'disk': 90.0  # 磁盘使用率超过90%
+         }
+         
+         # 更新自定义阈值
+         if alert_threshold:
+             self.alert_threshold.update(alert_threshold)
+         
+         # 监控数据
+         self.resources = {
+             'cpu_usage': 0.0,
+             'memory_usage': 0.0,
+             'memory_percent': 0.0,
+             'disk_usage': 0.0,
+             'disk_percent': 0.0,
+             'process_count': 0,
+             'thread_count': 0,
+             'network_io': {'sent_bytes': 0, 'recv_bytes': 0},
+             'io_counters': {'read_bytes': 0, 'write_bytes': 0}
+         }
+         
+         # 时间序列数据
+         self.time_series = []
+         
+         logger.info(f"资源监控器初始化成功: {self.name}, 间隔: {self.interval}秒")
+         
+         # 检查psutil可用性
+         try:
+             import psutil
+             self.psutil_available = True
+         except ImportError:
+             logger.warning("psutil模块不可用，资源监控功能受限")
+             self.psutil_available = False
+     
+     def start(self):
+         """开始监控"""
+         if self.running:
+             logger.warning(f"资源监控器 {self.name} 已经在运行")
+             return
+             
+         self.running = True
+         
+         # 启动监控线程
+         self.thread = threading.Thread(target=self._monitor_task, daemon=True)
+         self.thread.start()
+         
+         logger.info(f"资源监控器 {self.name} 已启动")
+     
+     def stop(self):
+         """停止监控"""
+         if not self.running:
+             logger.warning(f"资源监控器 {self.name} 已经停止")
+             return
+             
+         self.running = False
+         
+         # 等待线程结束
+         if self.thread and self.thread.is_alive():
+             self.thread.join(timeout=5)
+         
+         logger.info(f"资源监控器 {self.name} 已停止")
+     
+     def get_status(self) -> Dict:
+         """
+         获取监控状态
+         
+         Returns:
+             Dict: 当前资源使用情况
+         """
+         # 如果psutil可用，获取最新的资源使用情况
+         if self.psutil_available:
+             try:
+                 self._update_resources()
+             except Exception as e:
+                 logger.error(f"更新资源信息失败: {str(e)}")
+         
+         # 返回当前状态
+         status = {
+             'resources': self.resources,
+             'running': self.running,
+             'alert_threshold': self.alert_threshold
+         }
+         
+         # 添加最近的时间序列数据点(最多10个)
+         if self.time_series:
+             status['recent_data'] = self.time_series[-10:]
+         
+         return status
+     
+     def _update_resources(self):
+         """更新资源使用情况"""
+         if not self.psutil_available:
+             return
+             
+         try:
+             import psutil
+             
+             # 获取当前进程
+             process = psutil.Process(os.getpid())
+             
+             # 更新CPU使用率
+             self.resources['cpu_usage'] = psutil.cpu_percent(interval=0.1)
+             
+             # 更新内存使用情况
+             memory = psutil.virtual_memory()
+             self.resources['memory_usage'] = memory.used / 1024 / 1024  # MB
+             self.resources['memory_percent'] = memory.percent
+             
+             # 更新磁盘使用情况
+             disk = psutil.disk_usage('/')
+             self.resources['disk_usage'] = disk.used / 1024 / 1024 / 1024  # GB
+             self.resources['disk_percent'] = disk.percent
+             
+             # 更新进程和线程数量
+             self.resources['process_count'] = len(psutil.pids())
+             self.resources['thread_count'] = len(process.threads())
+             
+             # 更新网络IO
+             net_io = psutil.net_io_counters()
+             self.resources['network_io'] = {
+                 'sent_bytes': net_io.bytes_sent,
+                 'recv_bytes': net_io.bytes_recv
+             }
+             
+             # 更新IO计数器
+             io_counters = psutil.disk_io_counters()
+             self.resources['io_counters'] = {
+                 'read_bytes': io_counters.read_bytes,
+                 'write_bytes': io_counters.write_bytes
+             }
+             
+         except Exception as e:
+             logger.error(f"获取资源使用情况失败: {str(e)}")
+     
+     def _monitor_task(self):
+         """监控任务，定期收集和检查资源使用情况"""
+         while self.running:
+             try:
+                 # 更新资源使用情况
+                 self._update_resources()
+                 
+                 # 检查资源使用是否超过阈值
+                 self._check_thresholds()
+                 
+                 # 记录到时间序列
+                 timestamp = datetime.datetime.now().isoformat()
+                 data_point = {
+                     'timestamp': timestamp,
+                     **self.resources
+                 }
+                 self.time_series.append(data_point)
+                 
+                 # 限制时间序列大小
+                 if len(self.time_series) > 100:
+                     self.time_series = self.time_series[-100:]
+                 
+                 # 暂停指定间隔
+                 time.sleep(self.interval)
+                 
+             except Exception as e:
+                 logger.error(f"资源监控任务出错: {str(e)}")
+                 time.sleep(self.interval)
+     
+     def _check_thresholds(self):
+         """检查资源使用是否超过阈值"""
+         # 检查CPU使用率
+         if self.resources['cpu_usage'] > self.alert_threshold['cpu']:
+             logger.warning(f"CPU使用率过高: {self.resources['cpu_usage']}% > {self.alert_threshold['cpu']}%")
+         
+         # 检查内存使用率
+         if self.resources['memory_percent'] > self.alert_threshold['memory']:
+             logger.warning(f"内存使用率过高: {self.resources['memory_percent']}% > {self.alert_threshold['memory']}%")
+         
+         # 检查磁盘使用率
+         if self.resources['disk_percent'] > self.alert_threshold['disk']:
+             logger.warning(f"磁盘使用率过高: {self.resources['disk_percent']}% > {self.alert_threshold['disk']}%")
+ 
+ 
+ def create_monitor(monitor_type: str = 'performance', **kwargs) -> BaseMonitor:
+     """
+     创建监控器工厂函数
+     
+     Args:
+         monitor_type: 监控器类型（'performance', 'status', 'resource'）
+         **kwargs: 其他参数
+         
+     Returns:
+         BaseMonitor: 监控器实例
+     """
+     if monitor_type == 'status':
+         return StatusMonitor(**kwargs)
+     elif monitor_type == 'resource':
+         return ResourceMonitor(**kwargs)
+     else:
+         return PerformanceMonitor(**kwargs) 
```

---

```diff
- 
+ #!/usr/bin/env python3
+ # -*- coding: utf-8 -*-
+ 
+ """
+ 财经新闻爬虫系统 - 中间件组件
+ 实现类似Django的请求处理链
+ """
+ 
+ import time
+ import logging
+ from typing import Dict, List, Optional, Any, Callable, Union
+ from abc import ABC, abstractmethod
+ 
+ from app.utils.logger import get_crawler_logger
+ 
+ # 使用专门的爬虫日志记录器
+ logger = get_crawler_logger('middlewares')
+ 
+ # 中间件类型定义
+ RequestMiddleware = Callable[[Dict], Dict]  # 处理请求的中间件
+ ResponseMiddleware = Callable[[Dict, Dict], Dict]  # 处理响应的中间件
+ ItemMiddleware = Callable[[Dict], Dict]  # 处理数据项的中间件
+ 
+ 
+ class MiddlewareManager:
+     """中间件管理器，管理请求、响应和数据处理中间件"""
+     
+     def __init__(self):
+         """初始化中间件管理器"""
+         # 请求中间件列表
+         self.request_middlewares = []
+         
+         # 响应中间件列表
+         self.response_middlewares = []
+         
+         # 数据项中间件列表
+         self.item_middlewares = []
+         
+         logger.info("中间件管理器初始化成功")
+     
+     def add_request_middleware(self, middleware: RequestMiddleware):
+         """
+         添加请求中间件
+         
+         Args:
+             middleware: 请求中间件函数
+         """
+         self.request_middlewares.append(middleware)
+         logger.info(f"添加请求中间件: {middleware.__name__}")
+     
+     def add_response_middleware(self, middleware: ResponseMiddleware):
+         """
+         添加响应中间件
+         
+         Args:
+             middleware: 响应中间件函数
+         """
+         self.response_middlewares.append(middleware)
+         logger.info(f"添加响应中间件: {middleware.__name__}")
+     
+     def add_item_middleware(self, middleware: ItemMiddleware):
+         """
+         添加数据项中间件
+         
+         Args:
+             middleware: 数据项中间件函数
+         """
+         self.item_middlewares.append(middleware)
+         logger.info(f"添加数据项中间件: {middleware.__name__}")
+     
+     def process_request(self, request: Dict) -> Dict:
+         """
+         处理请求
+         
+         Args:
+             request: 请求参数
+             
+         Returns:
+             Dict: 处理后的请求参数
+         """
+         for middleware in self.request_middlewares:
+             try:
+                 request = middleware(request)
+             except Exception as e:
+                 logger.error(f"请求中间件 {middleware.__name__} 处理失败: {str(e)}")
+         
+         return request
+     
+     def process_response(self, response: Dict, request: Dict) -> Dict:
+         """
+         处理响应
+         
+         Args:
+             response: 响应结果
+             request: 原始请求参数
+             
+         Returns:
+             Dict: 处理后的响应结果
+         """
+         for middleware in self.response_middlewares:
+             try:
+                 response = middleware(response, request)
+             except Exception as e:
+                 logger.error(f"响应中间件 {middleware.__name__} 处理失败: {str(e)}")
+         
+         return response
+     
+     def process_item(self, item: Dict) -> Dict:
+         """
+         处理数据项
+         
+         Args:
+             item: 数据项
+             
+         Returns:
+             Dict: 处理后的数据项
+         """
+         for middleware in self.item_middlewares:
+             try:
+                 item = middleware(item)
+                 # 如果中间件返回None，表示数据项被过滤掉
+                 if item is None:
+                     return None
+             except Exception as e:
+                 logger.error(f"数据项中间件 {middleware.__name__} 处理失败: {str(e)}")
+         
+         return item
+ 
+ 
+ # 预定义的中间件函数
+ 
+ def ua_rotation_middleware(request: Dict) -> Dict:
+     """
+     User-Agent轮换中间件
+     
+     Args:
+         request: 请求参数
+         
+     Returns:
+         Dict: 添加随机User-Agent后的请求参数
+     """
+     from app.crawlers.components.requesters import SyncRequester
+     
+     # 创建临时请求器以使用其get_random_user_agent方法
+     temp_requester = SyncRequester()
+     
+     # 获取随机User-Agent
+     user_agent = temp_requester._get_random_user_agent()
+     
+     # 设置请求头
+     if 'headers' not in request:
+         request['headers'] = {}
+     
+     request['headers']['User-Agent'] = user_agent
+     
+     return request
+ 
+ 
+ def referer_middleware(request: Dict) -> Dict:
+     """
+     Referer中间件，添加Referer头
+     
+     Args:
+         request: 请求参数
+         
+     Returns:
+         Dict: 添加Referer后的请求参数
+     """
+     # 设置请求头
+     if 'headers' not in request:
+         request['headers'] = {}
+     
+     # 如果没有指定Referer，且有URL
+     if 'Referer' not in request['headers'] and 'url' in request:
+         url = request['url']
+         
+         # 生成合理的Referer
+         from urllib.parse import urlparse
+         parsed_url = urlparse(url)
+         referer = f"{parsed_url.scheme}://{parsed_url.netloc}/"
+         
+         request['headers']['Referer'] = referer
+     
+     return request
+ 
+ 
+ def rate_limiting_middleware(request: Dict) -> Dict:
+     """
+     速率限制中间件，避免请求过于频繁
+     
+     Args:
+         request: 请求参数
+         
+     Returns:
+         Dict: 处理后的请求参数
+     """
+     # 记录上次请求时间的字典
+     if not hasattr(rate_limiting_middleware, 'last_request_time'):
+         rate_limiting_middleware.last_request_time = {}
+     
+     # 获取域名
+     from urllib.parse import urlparse
+     
+     if 'url' in request:
+         parsed_url = urlparse(request['url'])
+         domain = parsed_url.netloc
+         
+         # 检查上次请求时间
+         current_time = time.time()
+         if domain in rate_limiting_middleware.last_request_time:
+             last_time = rate_limiting_middleware.last_request_time[domain]
+             elapsed = current_time - last_time
+             
+             # 如果间隔太短，需要等待
+             min_interval = request.get('min_interval', 1.0)  # 默认最小间隔1秒
+             if elapsed < min_interval:
+                 wait_time = min_interval - elapsed
+                 logger.debug(f"速率限制: 等待 {wait_time:.2f} 秒后请求 {domain}")
+                 time.sleep(wait_time)
+         
+         # 更新上次请求时间
+         rate_limiting_middleware.last_request_time[domain] = time.time()
+     
+     return request
+ 
+ 
+ def request_logging_middleware(request: Dict) -> Dict:
+     """
+     请求日志中间件
+     
+     Args:
+         request: 请求参数
+         
+     Returns:
+         Dict: 处理后的请求参数
+     """
+     if 'url' in request:
+         logger.info(f"发送请求: {request['url']}")
+     
+     return request
+ 
+ 
+ def response_logging_middleware(response: Dict, request: Dict) -> Dict:
+     """
+     响应日志中间件
+     
+     Args:
+         response: 响应结果
+         request: 原始请求参数
+         
+     Returns:
+         Dict: 处理后的响应结果
+     """
+     if 'url' in request and 'status_code' in response:
+         logger.info(f"收到响应: {request['url']}, 状态码: {response['status_code']}")
+     
+     return response
+ 
+ 
+ def error_retry_middleware(response: Dict, request: Dict) -> Dict:
+     """
+     错误重试中间件
+     
+     Args:
+         response: 响应结果
+         request: 原始请求参数
+         
+     Returns:
+         Dict: 处理后的响应结果
+     """
+     # 如果状态码表示错误
+     if 'status_code' in response and response['status_code'] >= 400:
+         # 获取已重试次数
+         retry_count = request.get('retry_count', 0)
+         max_retries = request.get('max_retries', 3)
+         
+         if retry_count < max_retries:
+             logger.warning(f"请求失败，状态码: {response['status_code']}，将进行第 {retry_count + 1} 次重试")
+             
+             # 更新请求参数
+             request['retry_count'] = retry_count + 1
+             
+             # 设置退避时间
+             backoff_time = 2 ** retry_count  # 指数退避
+             logger.debug(f"等待 {backoff_time} 秒后重试")
+             time.sleep(backoff_time)
+             
+             # 重新发送请求
+             from app.crawlers.components.requesters import SyncRequester
+             requester = SyncRequester()
+             
+             try:
+                 url = request.get('url')
+                 params = request.get('params')
+                 headers = request.get('headers')
+                 timeout = request.get('timeout')
+                 
+                 content = requester.fetch(url, params, headers, timeout, max_retries=1)
+                 
+                 if content:
+                     response['content'] = content
+                     response['status_code'] = 200
+                     logger.info(f"重试成功: {url}")
+             except Exception as e:
+                 logger.error(f"重试失败: {str(e)}")
+     
+     return response
+ 
+ 
+ def content_type_middleware(response: Dict, request: Dict) -> Dict:
+     """
+     内容类型中间件，根据Content-Type处理响应
+     
+     Args:
+         response: 响应结果
+         request: 原始请求参数
+         
+     Returns:
+         Dict: 处理后的响应结果
+     """
+     # 检查Content-Type
+     content_type = response.get('headers', {}).get('Content-Type', '')
+     
+     # 如果是JSON
+     if 'application/json' in content_type and 'content' in response:
+         try:
+             import json
+             response['json'] = json.loads(response['content'])
+         except json.JSONDecodeError as e:
+             logger.error(f"JSON解析失败: {str(e)}")
+     
+     # 如果是XML
+     elif ('application/xml' in content_type or 'text/xml' in content_type) and 'content' in response:
+         try:
+             from bs4 import BeautifulSoup
+             response['xml'] = BeautifulSoup(response['content'], 'xml')
+         except Exception as e:
+             logger.error(f"XML解析失败: {str(e)}")
+     
+     # 如果是HTML
+     elif 'text/html' in content_type and 'content' in response:
+         try:
+             from bs4 import BeautifulSoup
+             response['html'] = BeautifulSoup(response['content'], 'html.parser')
+         except Exception as e:
+             logger.error(f"HTML解析失败: {str(e)}")
+     
+     return response
+ 
+ 
+ def ads_filter_middleware(item: Dict) -> Optional[Dict]:
+     """
+     广告过滤中间件，过滤掉广告内容
+     
+     Args:
+         item: 数据项
+         
+     Returns:
+         Optional[Dict]: 处理后的数据项，如果是广告则返回None
+     """
+     # 广告关键词
+     ad_keywords = [
+         '广告', '推广', 'AD', '赞助', '特惠', '活动', '下载APP', '打开APP',
+         '立即下载', '立即注册', '点击查看', '点击下载', '点击注册'
+     ]
+     
+     # 检查标题
+     if 'title' in item:
+         title = item['title']
+         for keyword in ad_keywords:
+             if keyword in title:
+                 logger.info(f"过滤广告: {title}")
+                 return None
+     
+     # 检查内容
+     if 'content' in item:
+         content = item['content']
+         
+         # 检查内容长度是否过短
+         if len(content) < 100:
+             # 如果内容过短，进一步检查是否包含广告关键词
+             for keyword in ad_keywords:
+                 if keyword in content:
+                     logger.info(f"过滤短广告内容: {item.get('title', '')}")
+                     return None
+     
+     # 检查URL
+     if 'url' in item:
+         url = item['url'].lower()
+         
+         # 检查URL是否包含广告相关路径
+         ad_paths = ['ad', 'ads', 'advertisement', 'promotion', 'sponsored']
+         for path in ad_paths:
+             if f'/{path}/' in url or f'/{path}?' in url:
+                 logger.info(f"过滤广告URL: {url}")
+                 return None
+     
+     return item
+ 
+ 
+ def duplicate_filter_middleware(item: Dict) -> Optional[Dict]:
+     """
+     去重中间件，过滤掉重复内容
+     
+     Args:
+         item: 数据项
+         
+     Returns:
+         Optional[Dict]: 处理后的数据项，如果是重复项则返回None
+     """
+     # 记录已处理的项目ID
+     if not hasattr(duplicate_filter_middleware, 'processed_ids'):
+         duplicate_filter_middleware.processed_ids = set()
+     
+     # 生成ID
+     item_id = None
+     
+     # 如果有ID字段
+     if 'id' in item:
+         item_id = item['id']
+     # 如果有URL字段
+     elif 'url' in item:
+         import hashlib
+         item_id = hashlib.md5(item['url'].encode('utf-8')).hexdigest()
+     # 如果有标题字段
+     elif 'title' in item:
+         import hashlib
+         item_id = hashlib.md5(item['title'].encode('utf-8')).hexdigest()
+     
+     # 检查是否已处理
+     if item_id and item_id in duplicate_filter_middleware.processed_ids:
+         logger.debug(f"过滤重复项: {item.get('title', '')}")
+         return None
+     
+     # 记录ID
+     if item_id:
+         duplicate_filter_middleware.processed_ids.add(item_id)
+     
+     return item
+ 
+ 
+ def sentiment_analysis_middleware(item: Dict) -> Dict:
+     """
+     情感分析中间件，添加情感分析结果
+     
+     Args:
+         item: 数据项
+         
+     Returns:
+         Dict: 添加情感分析结果后的数据项
+     """
+     # 如果已有情感分析结果，跳过
+     if 'sentiment' in item:
+         return item
+     
+     # 检查是否有内容
+     if 'content' not in item or not item['content']:
+         return item
+     
+     try:
+         # 尝试导入情感分析器
+         from app.utils.sentiment_analyzer import SentimentAnalyzer
+         
+         # 创建情感分析器
+         analyzer = SentimentAnalyzer()
+         
+         # 分析情感
+         content = item['content']
+         sentiment = analyzer.analyze(content)
+         
+         # 添加情感分析结果
+         item['sentiment'] = sentiment
+         
+         logger.debug(f"完成情感分析: {item.get('title', '')}, 情感得分: {sentiment}")
+         
+     except ImportError:
+         logger.warning("情感分析模块不可用")
+     except Exception as e:
+         logger.error(f"情感分析失败: {str(e)}")
+     
+     return item
+ 
+ 
+ def keyword_extraction_middleware(item: Dict) -> Dict:
+     """
+     关键词提取中间件，提取内容关键词
+     
+     Args:
+         item: 数据项
+         
+     Returns:
+         Dict: 添加关键词后的数据项
+     """
+     # 如果已有关键词，跳过
+     if 'keywords' in item and item['keywords']:
+         return item
+     
+     # 检查是否有内容
+     if 'content' not in item or not item['content']:
+         return item
+     
+     try:
+         # 导入关键词提取函数
+         from app.utils.text_cleaner import extract_keywords
+         
+         # 提取关键词
+         content = item['content']
+         keywords = extract_keywords(content)
+         
+         # 添加关键词
+         item['keywords'] = keywords
+         
+         logger.debug(f"提取关键词: {item.get('title', '')}, 关键词: {keywords}")
+         
+     except ImportError:
+         logger.warning("关键词提取模块不可用")
+     except Exception as e:
+         logger.error(f"关键词提取失败: {str(e)}")
+     
+     return item
+ 
+ 
+ def summary_middleware(item: Dict) -> Dict:
+     """
+     摘要生成中间件，生成内容摘要
+     
+     Args:
+         item: 数据项
+         
+     Returns:
+         Dict: 添加摘要后的数据项
+     """
+     # 如果已有摘要，跳过
+     if 'summary' in item and item['summary']:
+         return item
+     
+     # 检查是否有内容
+     if 'content' not in item or not item['content']:
+         return item
+     
+     try:
+         # 尝试导入摘要生成器
+         from app.utils.summarizer import Summarizer
+         
+         # 创建摘要生成器
+         summarizer = Summarizer()
+         
+         # 生成摘要
+         content = item['content']
+         summary = summarizer.summarize(content)
+         
+         # 添加摘要
+         item['summary'] = summary
+         
+         logger.debug(f"生成摘要: {item.get('title', '')}")
+         
+     except ImportError:
+         # 如果摘要生成器不可用，使用简单方法生成摘要
+         content = item['content']
+         
+         # 简单摘要：取内容的前200个字符
+         simple_summary = content[:200].strip()
+         if len(content) > 200:
+             simple_summary += '...'
+         
+         item['summary'] = simple_summary
+         
+         logger.debug(f"生成简单摘要: {item.get('title', '')}")
+     except Exception as e:
+         logger.error(f"生成摘要失败: {str(e)}")
+     
+     return item
+ 
+ 
+ def create_middleware_manager() -> MiddlewareManager:
+     """
+     创建中间件管理器并添加常用中间件
+     
+     Returns:
+         MiddlewareManager: 初始化好的中间件管理器
+     """
+     manager = MiddlewareManager()
+     
+     # 添加请求中间件
+     manager.add_request_middleware(ua_rotation_middleware)
+     manager.add_request_middleware(referer_middleware)
+     manager.add_request_middleware(rate_limiting_middleware)
+     manager.add_request_middleware(request_logging_middleware)
+     
+     # 添加响应中间件
+     manager.add_response_middleware(response_logging_middleware)
+     manager.add_response_middleware(error_retry_middleware)
+     manager.add_response_middleware(content_type_middleware)
+     
+     # 添加数据项中间件
+     manager.add_item_middleware(ads_filter_middleware)
+     manager.add_item_middleware(duplicate_filter_middleware)
+     manager.add_item_middleware(sentiment_analysis_middleware)
+     manager.add_item_middleware(keyword_extraction_middleware)
+     manager.add_item_middleware(summary_middleware)
+     
+     return manager 
```

---

```diff
- 
+ #!/usr/bin/env python3
+ # -*- coding: utf-8 -*-
+ 
+ """
+ 财经新闻爬虫系统 - 配置组件
+ 实现配置与代码分离
+ """
+ 
+ import os
+ import yaml
+ import json
+ import logging
+ from typing import Dict, Any, Optional, List
+ 
+ from app.utils.logger import get_crawler_logger
+ 
+ # 使用专门的爬虫日志记录器
+ logger = get_crawler_logger('config')
+ 
+ class ConfigLoader:
+     """配置加载器，负责加载爬虫配置"""
+     
+     def __init__(self, config_dir: Optional[str] = None):
+         """
+         初始化配置加载器
+         
+         Args:
+             config_dir: 配置文件目录
+         """
+         if config_dir:
+             self.config_dir = config_dir
+         else:
+             # 默认配置目录
+             self.config_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'config')
+         
+         # 确保配置目录存在
+         os.makedirs(self.config_dir, exist_ok=True)
+         
+         # 缓存配置
+         self.config_cache = {}
+         
+         logger.info(f"配置加载器初始化成功: {self.config_dir}")
+     
+     def get_config(self, name: str, force_reload: bool = False) -> Dict[str, Any]:
+         """
+         获取指定名称的配置
+         
+         Args:
+             name: 配置名称
+             force_reload: 是否强制重新加载
+             
+         Returns:
+             Dict: 配置字典
+         """
+         # 如果缓存中有且不需要强制重新加载，直接返回
+         if name in self.config_cache and not force_reload:
+             return self.config_cache[name]
+         
+         # 查找配置文件
+         config_path = self._find_config_file(name)
+         
+         if not config_path:
+             logger.warning(f"找不到配置文件: {name}")
+             return {}
+         
+         # 加载配置
+         config = self._load_config_file(config_path)
+         
+         # 缓存配置
+         self.config_cache[name] = config
+         
+         return config
+     
+     def get_crawler_config(self, crawler_name: str) -> Dict[str, Any]:
+         """
+         获取爬虫配置，先查找专用配置，再查找通用配置
+         
+         Args:
+             crawler_name: 爬虫名称
+             
+         Returns:
+             Dict: 爬虫配置
+         """
+         # 加载通用配置
+         common_config = self.get_config('common')
+         
+         # 加载专用配置
+         crawler_config = self.get_config(crawler_name)
+         
+         # 合并配置
+         merged_config = {}
+         merged_config.update(common_config)
+         merged_config.update(crawler_config)
+         
+         return merged_config
+     
+     def save_config(self, name: str, config: Dict[str, Any], format: str = 'yaml') -> bool:
+         """
+         保存配置
+         
+         Args:
+             name: 配置名称
+             config: 配置字典
+             format: 配置文件格式，'yaml'或'json'
+             
+         Returns:
+             bool: 保存是否成功
+         """
+         # 确定文件路径
+         if format.lower() == 'json':
+             file_path = os.path.join(self.config_dir, f"{name}.json")
+         else:
+             file_path = os.path.join(self.config_dir, f"{name}.yaml")
+         
+         try:
+             # 保存配置
+             if format.lower() == 'json':
+                 with open(file_path, 'w', encoding='utf-8') as f:
+                     json.dump(config, f, ensure_ascii=False, indent=2)
+             else:
+                 with open(file_path, 'w', encoding='utf-8') as f:
+                     yaml.safe_dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
+             
+             # 更新缓存
+             self.config_cache[name] = config
+             
+             logger.info(f"配置已保存: {file_path}")
+             return True
+             
+         except Exception as e:
+             logger.error(f"保存配置失败: {str(e)}")
+             return False
+     
+     def _find_config_file(self, name: str) -> Optional[str]:
+         """
+         查找配置文件
+         
+         Args:
+             name: 配置名称
+             
+         Returns:
+             Optional[str]: 配置文件路径或None
+         """
+         # 优先查找YAML文件
+         yaml_path = os.path.join(self.config_dir, f"{name}.yaml")
+         if os.path.exists(yaml_path):
+             return yaml_path
+         
+         # 查找YML文件
+         yml_path = os.path.join(self.config_dir, f"{name}.yml")
+         if os.path.exists(yml_path):
+             return yml_path
+         
+         # 查找JSON文件
+         json_path = os.path.join(self.config_dir, f"{name}.json")
+         if os.path.exists(json_path):
+             return json_path
+         
+         return None
+     
+     def _load_config_file(self, file_path: str) -> Dict[str, Any]:
+         """
+         加载配置文件
+         
+         Args:
+             file_path: 配置文件路径
+             
+         Returns:
+             Dict: 配置字典
+         """
+         try:
+             # 根据文件扩展名选择加载方式
+             ext = os.path.splitext(file_path)[1].lower()
+             
+             if ext in ['.yaml', '.yml']:
+                 with open(file_path, 'r', encoding='utf-8') as f:
+                     config = yaml.safe_load(f)
+             elif ext == '.json':
+                 with open(file_path, 'r', encoding='utf-8') as f:
+                     config = json.load(f)
+             else:
+                 logger.error(f"不支持的配置文件格式: {ext}")
+                 return {}
+             
+             # 如果配置是None，返回空字典
+             if config is None:
+                 return {}
+                 
+             logger.info(f"配置已加载: {file_path}")
+             return config
+             
+         except Exception as e:
+             logger.error(f"加载配置文件失败: {str(e)}")
+             return {}
+     
+     def list_available_configs(self) -> List[str]:
+         """
+         列出所有可用的配置
+         
+         Returns:
+             List[str]: 配置名称列表
+         """
+         configs = []
+         
+         # 遍历配置目录中的文件
+         for file_name in os.listdir(self.config_dir):
+             if file_name.endswith(('.yaml', '.yml', '.json')):
+                 # 去除扩展名
+                 config_name = os.path.splitext(file_name)[0]
+                 configs.append(config_name)
+         
+         return configs
+     
+     def create_crawler_config_template(self, crawler_name: str) -> bool:
+         """
+         创建爬虫配置模板
+         
+         Args:
+             crawler_name: 爬虫名称
+             
+         Returns:
+             bool: 创建是否成功
+         """
+         # 如果配置已存在，不覆盖
+         if self._find_config_file(crawler_name):
+             logger.warning(f"配置已存在: {crawler_name}")
+             return False
+         
+         # 创建配置模板
+         template = {
+             "crawler": {
+                 "name": crawler_name,
+                 "description": f"{crawler_name}爬虫配置",
+                 "version": "1.0.0",
+                 "enabled": True
+             },
+             "request": {
+                 "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
+                 "timeout": 30,
+                 "max_retries": 3,
+                 "retry_delay": 2,
+                 "use_proxy": False,
+                 "proxy": "",
+                 "rate_limit": 1.0,  # 每秒请求数
+             },
+             "parser": {
+                 "selectors": {
+                     "list": {
+                         "container": "div.news-list, ul.news-list",
+                         "item": "div.news-item, li.news-item",
+                         "title": "h2, h3, a.title",
+                         "link": "a",
+                         "date": "span.date, time.date",
+                         "summary": "div.summary, p.summary"
+                     },
+                     "detail": {
+                         "title": "h1.title, h1.article-title",
+                         "content": "div.article-content, div.content",
+                         "date": "span.date, time.date",
+                         "author": "span.author, div.author",
+                         "source": "span.source, div.source",
+                         "category": "span.category, div.category"
+                     }
+                 },
+                 "date_format": "%Y-%m-%d %H:%M:%S"
+             },
+             "storage": {
+                 "type": "sqlite",
+                 "db_path": f"data/db/{crawler_name}.db",
+                 "table_name": "news"
+             },
+             "urls": {
+                 "entry": f"https://example.com/{crawler_name}",
+                 "category": {
+                     "财经": "https://example.com/finance",
+                     "股票": "https://example.com/stock",
+                     "基金": "https://example.com/fund"
+                 },
+                 "api": {
+                     "list": "https://example.com/api/list",
+                     "detail": "https://example.com/api/detail"
+                 }
+             },
+             "middlewares": {
+                 "request": ["ua_rotation", "referer", "rate_limiting", "request_logging"],
+                 "response": ["response_logging", "error_retry", "content_type"],
+                 "item": ["ads_filter", "duplicate_filter", "sentiment_analysis", "keyword_extraction", "summary"]
+             },
+             "monitor": {
+                 "enabled": True,
+                 "performance": {
+                     "enabled": True,
+                     "interval": 60
+                 },
+                 "status": {
+                     "enabled": True
+                 },
+                 "resource": {
+                     "enabled": True,
+                     "interval": 60,
+                     "alert_threshold": {
+                         "cpu": 90,
+                         "memory": 85,
+                         "disk": 90
+                     }
+                 }
+             }
+         }
+         
+         # 保存配置
+         return self.save_config(crawler_name, template)
+ 
+ 
+ # 创建通用配置模板
+ def create_common_config(config_dir: Optional[str] = None) -> bool:
+     """
+     创建通用配置模板
+     
+     Args:
+         config_dir: 配置目录
+         
+     Returns:
+         bool: 创建是否成功
+     """
+     loader = ConfigLoader(config_dir)
+     
+     # 创建通用配置
+     common_config = {
+         "common": {
+             "timeout": 30,
+             "max_retries": 3,
+             "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
+             "headers": {
+                 "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
+                 "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
+                 "Accept-Encoding": "gzip, deflate, br",
+                 "Connection": "keep-alive",
+                 "Upgrade-Insecure-Requests": "1"
+             },
+             "date_format": "%Y-%m-%d %H:%M:%S",
+             "storage": {
+                 "type": "sqlite",
+                 "db_dir": "data/db",
+                 "table_name": "news"
+             },
+             "middlewares": {
+                 "enabled": True,
+                 "default_request_middlewares": ["ua_rotation", "referer", "rate_limiting", "request_logging"],
+                 "default_response_middlewares": ["response_logging", "error_retry", "content_type"],
+                 "default_item_middlewares": ["ads_filter", "duplicate_filter", "sentiment_analysis", "keyword_extraction", "summary"]
+             },
+             "monitor": {
+                 "enabled": True,
+                 "log_dir": "logs/performance",
+                 "performance_interval": 60,
+                 "resource_interval": 60
+             }
+         }
+     }
+     
+     # 保存通用配置
+     return loader.save_config("common", common_config)
+ 
+ 
+ # 创建网易财经配置示例
+ def create_netease_config(config_dir: Optional[str] = None) -> bool:
+     """
+     创建网易财经配置示例
+     
+     Args:
+         config_dir: 配置目录
+         
+     Returns:
+         bool: 创建是否成功
+     """
+     loader = ConfigLoader(config_dir)
+     
+     # 创建网易财经配置
+     netease_config = {
+         "crawler": {
+             "name": "网易财经",
+             "description": "网易财经爬虫配置",
+             "version": "1.0.0",
+             "enabled": True
+         },
+         "request": {
+             "timeout": 30,
+             "max_retries": 3,
+             "retry_delay": 2,
+             "use_proxy": False,
+             "rate_limit": 1.0  # 每秒请求数
+         },
+         "parser": {
+             "selectors": {
+                 "list": {
+                     "container": "div.news-list, ul.news-list",
+                     "item": "div.news-item, li.news-item, div.data-item",
+                     "title": "h2, h3, a.title, div.news_title",
+                     "link": "a",
+                     "date": "span.date, time.date, span.time",
+                     "summary": "div.summary, p.summary, div.news_short"
+                 },
+                 "detail": {
+                     "title": "h1.title, h1.post_title, div.post_content h1",
+                     "content": "div.post_body, div#endText, div.post_text, div.article-content",
+                     "date": "div.post_time_source, div.post_info, span.time",
+                     "author": "span.author, span.ep-editor, span.ep-source",
+                     "source": "div.post_time_source, a.source",
+                     "category": "div.post_crumb, div.breadcrumb"
+                 }
+             },
+             "date_format": "%Y-%m-%d %H:%M:%S"
+         },
+         "storage": {
+             "type": "sqlite",
+             "db_path": "data/db/网易财经.db",
+             "table_name": "news"
+         },
+         "urls": {
+             "entry": "https://money.163.com/",
+             "category": {
+                 "财经": "https://money.163.com/",
+                 "股票": "https://money.163.com/stock/",
+                 "理财": "https://money.163.com/licai/",
+                 "基金": "https://money.163.com/fund/",
+                 "热点": "https://money.163.com/special/00252G50/macro.html",
+                 "商业": "https://biz.163.com/"
+             },
+             "api": {
+                 "股票": "https://money.163.com/special/00259BVP/news_json.js",
+                 "理财": "https://money.163.com/special/00259BVP/licai_json.js",
+                 "基金": "https://money.163.com/special/00259BVP/fund_json.js"
+             }
+         },
+         "middlewares": {
+             "request": ["ua_rotation", "referer", "rate_limiting", "request_logging"],
+             "response": ["response_logging", "error_retry", "content_type"],
+             "item": ["ads_filter", "duplicate_filter", "sentiment_analysis", "keyword_extraction", "summary"]
+         },
+         "monitor": {
+             "enabled": True,
+             "performance": {
+                 "enabled": True,
+                 "interval": 60
+             },
+             "status": {
+                 "enabled": True
+             },
+             "resource": {
+                 "enabled": True,
+                 "interval": 60
+             }
+         }
+     }
+     
+     # 保存网易财经配置
+     return loader.save_config("网易财经", netease_config)
+ 
+ 
+ # 创建配置单例
+ _config_loader = None
+ 
+ def get_config_loader(config_dir: Optional[str] = None) -> ConfigLoader:
+     """
+     获取配置加载器单例
+     
+     Args:
+         config_dir: 配置目录
+         
+     Returns:
+         ConfigLoader: 配置加载器实例
+     """
+     global _config_loader
+     
+     if _config_loader is None:
+         _config_loader = ConfigLoader(config_dir)
+     
+     return _config_loader
+ 
+ 
+ def init_default_configs(config_dir: Optional[str] = None) -> None:
+     """
+     初始化默认配置
+     
+     Args:
+         config_dir: 配置目录
+     """
+     loader = get_config_loader(config_dir)
+     
+     # 如果没有通用配置，创建
+     if not loader._find_config_file("common"):
+         create_common_config(config_dir)
+         logger.info("已创建通用配置")
+     
+     # 如果没有网易财经配置，创建
+     if not loader._find_config_file("网易财经"):
+         create_netease_config(config_dir)
+         logger.info("已创建网易财经配置示例")
+     
+     logger.info("默认配置初始化完成") 
```

---

```bash
pip install pyyaml
```

```

PS D:\Git\Github\NewsLook> pip install pyyaml
Collecting pyyaml
  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)
Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)
Installing collected packages: pyyaml
Successfully installed pyyaml-6.0.2
PS D:\Git\Github\NewsLook>
```

---

已成功安装PyYAML模块。现在config.py文件中的"import yaml"语句应该不会再出现导入错误。

---

Read file: app/crawlers/components/config.py

---

Read file: app/crawlers/components/config.py

---

Could not resolve URI: Error: 无法解析不存在的文件 'd:\Git\Github\NewsLook\app\config'

---

<details>
            <summary>Searched codebase "main.py or app.py file that uses the crawler configuration" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `_archived_today3_20250324200011/run.py` | L1-45 |
| `scripts/temp/run.py.bak.20250324200011` | L1-43 |
| `run.py` | L333-374 |
| `main.py` | L1-42 |
| `scripts/run_crawler.py` | L294-297 |
| `app/config.py` | L193-238 |
| `app/crawlers/eastmoney.py` | L806-811 |
| `app/utils/config.py` | L1-44 |
| `app/crawlers/components/config.py` | L1-42 |
| `newslook/crawlers/manager.py` | L29-74 |
| `app/crawlers/manager.py` | L1-64 |
| `run.py` | L1-69 |
| `run.py` | L374-416 |
| `_archived_today3_20250324200011/main.py` | L1-39 |
| `scripts/run_eastmoney.py` | L135-176 |
| `app/config.py` | L141-193 |
| `_archived_today3_20250324200011/app/config.py` | L139-184 |
| `run.py` | L288-333 |
| `scripts/run_crawler.py` | L1-47 |
| `app/crawlers/eastmoney.py` | L791-806 |
| `_archived_today3_20250324200011/app/web/__init__.py` | L1-63 |
| `tests/crawlers/test_eastmoney.py` | L1-32 |
| `app/web/__init__.py` | L1-40 |
| `run.py` | L512-562 |
| `_archived_today3_20250324200011/run.py` | L59-97 |

</details>

---

Read file: app/crawlers/components/config.py

---

Could not resolve URI: Error: 无法解析不存在的文件 'd:\Git\Github\NewsLook\app\config'

---

<details>
            <summary>Listed directory app • **15** results</summary>
        
| Name |
|-------|
| 📁 `crawlers` |
| 📁 `utils` |
| 📁 `data` |
| 📁 `__pycache__` |
| 📄 `config.py` |
| 📄 `app.py` |
| 📄 `__init__.py` |
| 📁 `routes` |
| 📁 `api` |
| 📁 `db` |
| 📁 `models` |
| 📁 `templates` |
| 📁 `web` |
| 📁 `tasks` |
| 📄 `scheduler.py` |

</details>

---

Read file: app/app.py

---

Read file: app/config.py

---

<details>
            <summary>Listed directory app/crawlers • **19** results</summary>
        
| Name |
|-------|
| 📁 `components` |
| 📁 `__pycache__` |
| 📄 `manager.py` |
| 📄 `ifeng.py` |
| 📄 `base.py` |
| 📄 `__init__.py` |
| 📁 `image_cache` |
| 📁 `data` |
| 📁 `db` |
| 📄 `netease.py` |
| 📄 `async_crawler.py` |
| 📄 `async_ifeng.py` |
| 📄 `async_eastmoney.py` |
| 📄 `optimized_sina.py` |
| 📄 `optimized_netease.py` |
| 📄 `eastmoney.py` |
| 📄 `sina.py` |
| 📄 `optimized_crawler.py` |
| 📄 `eastmoney_simple.py` |

</details>

---

Read file: app/app.py

---

Read file: app/crawlers/base.py

---

Read file: app/crawlers/manager.py

---

<details>
            <summary>Listed directory app/utils • **23** results</summary>
        
| Name |
|-------|
| 📁 `__pycache__` |
| 📄 `sync_databases.py` |
| 📄 `database.py` |
| 📄 `text_cleaner.py` |
| 📄 `logger.py` |
| 📄 `performance_monitor.py` |
| 📄 `db_transaction.py` |
| 📄 `image_detector.py` |
| 📄 `ad_filter.py` |
| 📄 `log_cleaner.py` |
| 📄 `proxy_manager.py` |
| 📄 `url_utils.py` |
| 📄 `text_utils.py` |
| 📄 `config.py` |
| 📄 `crawler_optimizer.py` |
| 📄 `logger.py.20250324194411` |
| 📄 `fix_tencent.py` |
| 📄 `logger.py.20250324193847.bak` |
| 📄 `fix_crawler_init.py` |
| 📄 `fix_crawlers.py` |
| 📄 `fix_unknown_sources.py` |
| 📄 `sentiment_analyzer.py` |
| 📄 `__init__.py` |

</details>

---

Read file: app/crawlers/ifeng.py

---

Read file: app/utils/database.py

---

<details>
            <summary>Listed directory app/db • **4** results</summary>
        
| Name |
|-------|
| 📁 `__pycache__` |
| 📄 `sqlite_manager.py` |
| 📄 `user_preferences_db.py` |
| 📄 `__init__.py` |

</details>

---

Read file: app/db/sqlite_manager.py

---

<details>
            <summary>Searched codebase "数据库相关的接口和功能" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `docs/database/数据库连接说明.md` | L102-166 |
| `docs/数据库连接说明.md` | L102-166 |
| `utils/db_manager.py` | L207-265 |
| `utils/db_manager.py` | L43-67 |
| `docs/database/数据库连接说明.md` | L166-227 |
| `docs/数据库连接说明.md` | L166-227 |
| `docs/database/数据库连接说明.md` | L57-102 |
| `docs/database/数据库连接说明.md` | L1-57 |
| `docs/数据库连接说明.md` | L57-102 |
| `newslook/utils/database.py` | L1014-1060 |
| `newslook/utils/database.py` | L1442-1502 |
| `docs/数据库连接说明.md` | L1-57 |
| `app/utils/database.py` | L1546-1606 |
| `app/utils/database.py` | L1354-1412 |
| `newslook/utils/database.py` | L1250-1308 |
| `_archived_today3_20250324200011/utils/db_manager.py` | L207-265 |
| `app/utils/database.py` | L1606-1655 |
| `newslook/utils/database.py` | L1502-1551 |
| `utils/db_manager.py` | L67-115 |
| `docs/database/数据库连接说明.md` | L227-286 |
| `_archived_today3_20250324200011/utils/db_manager.py` | L43-67 |
| `docs/数据库连接说明.md` | L227-286 |
| `app/utils/database.py` | L1412-1437 |
| `newslook/utils/database.py` | L1308-1333 |
| `app/utils/database.py` | L1135-1203 |

</details>

---

Read file: app/db/user_preferences_db.py

---

<details>
            <summary>Listed directory app/models • **4** results</summary>
        
| Name |
|-------|
| 📄 `user_preferences.py` |
| 📄 `news.py` |
| 📄 `__init__.py` |
| 📄 `news.py.20250324194412.bak` |

</details>

---



---

Read file: app/models/news.py

---

```bash
mkdir -p app/config
```

```

    Directory: D:\Git\Github\NewsLook\app

Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
PS D:\Git\Github\NewsLook>

```

---

```diff
- 
+ # 网易财经爬虫配置
+ 
+ crawler:
+   name: netease
+   display_name: 网易财经
+   enabled: true
+   
+ request:
+   base_url: https://money.163.com/
+   list_urls:
+     - https://money.163.com/special/00251LR5/news_json_01.js
+     - https://money.163.com/special/00251LR5/news_json_02.js
+   headers:
+     User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36
+   timeout: 10
+   retry_times: 3
+   retry_interval: 5
+ 
+ parser:
+   list:
+     title_selector: .title
+     url_selector: .link
+     date_selector: .time
+   detail:
+     title_selector: h1.post_title
+     content_selector: .post_body
+     date_selector: .post_time_source
+     author_selector: .post_author
+     source_selector: .post_source
+ 
+ storage:
+   database_path: app/data/netease.db
+   save_to_json: true
+   json_path: app/data/netease_news.json
+   save_full_content: true
+   save_images: true
+   
+ monitoring:
+   log_level: INFO
+   notification: true
+   error_alert: true
+   performance_tracking: false
+ 
+ scheduler:
+   interval: 1800  # 单位：秒
+   start_time: "06:00"
+   end_time: "23:00"
+   crawl_latest_first: true
+   max_pages_per_run: 5 
```

---

```diff
- 
+ # 新浪财经爬虫配置
+ 
+ crawler:
+   name: sina
+   display_name: 新浪财经
+   enabled: true
+   
+ request:
+   base_url: https://finance.sina.com.cn/
+   list_urls:
+     - https://finance.sina.com.cn/roll/index.d.html?cid=56589&page=1
+     - https://finance.sina.com.cn/roll/index.d.html?cid=56589&page=2
+   headers:
+     User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36
+   timeout: 10
+   retry_times: 3
+   retry_interval: 5
+ 
+ parser:
+   list:
+     title_selector: .list_009 li a
+     url_selector: .list_009 li a
+     date_selector: .time
+   detail:
+     title_selector: .main-title
+     content_selector: .article-content
+     date_selector: .date-source span
+     author_selector: .author
+     source_selector: .source
+ 
+ storage:
+   database_path: app/data/sina.db
+   save_to_json: true
+   json_path: app/data/sina_news.json
+   save_full_content: true
+   save_images: true
+   
+ monitoring:
+   log_level: INFO
+   notification: true
+   error_alert: true
+   performance_tracking: false
+ 
+ scheduler:
+   interval: 1800  # 单位：秒
+   start_time: "06:00"
+   end_time: "23:00"
+   crawl_latest_first: true
+   max_pages_per_run: 5 
```

---

已成功创建新浪财经的爬虫配置文件。配置包含了以下主要部分：

1. 爬虫基本信息：名称、显示名称和启用状态
2. 请求设置：基础URL、列表页URL、请求头、超时和重试策略
3. 解析规则：列表页和详情页的选择器
4. 存储设置：数据库路径、JSON存储路径和内容保存选项
5. 监控配置：日志级别、通知和错误提醒
6. 调度设置：抓取间隔、时间范围和每次运行的最大页数

这个配置与之前创建的网易财经配置保持了相同的结构，同时针对新浪财经的特点进行了适当调整，特别是在选择器和URL方面。

---

_**User**_

异步IO统一接口
为所有爬虫实现统一的异步接口
支持同步/异步双模式运行
任务调度器分离
将调度逻辑与爬虫逻辑分离
实现高级调度功能，如优先级、定时等



---

_**Assistant**_

Cancelled

---



---



---

