<!-- Generated by SpecStory -->

# 爬虫代码检查与修复 (2025-04-24 20:37:14)

_**User**_

@data 数据库太多了，每个爬虫使用一个爬虫，然后进行备份，web页面会有一个请调整@app 

---

_**Assistant**_

我理解您想要查看项目中的数据库结构和爬虫配置，以及相关的Web页面设置。让我来帮您分析这些内容。

---

Could not find file '/d:/Git/Github/NewsLook/app/config.py'. Did you mean one of:
- app/config.py

---

Read file: app/config.py

---

Read file: app/cli.py

---

<details>
            <summary>Searched codebase "爬虫配置和数据库结构" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `app/config.py` | L193-238 |
| `docs/database/数据库连接说明.md` | L1-57 |
| `docs/数据库连接说明.md` | L1-57 |
| `_archived_today3_20250324200011/app/config.py` | L139-184 |
| `app/config.py` | L36-79 |
| `app/config/sina.yaml` | L1-49 |
| `app/crawlers/sina.py` | L76-137 |
| `app/crawlers/manager.py` | L63-94 |
| `newslook/crawlers/manager.py` | L29-74 |
| `docs/database/数据库连接说明.md` | L57-102 |
| `app/crawlers/unified_crawler.py` | L33-84 |
| `docs/数据库连接说明.md` | L57-102 |
| `_archived_today3_20250324200011/app/config.py` | L29-72 |
| `ARCHITECTURE_REFACTORING.md` | L435-479 |
| `app/crawlers/manager.py` | L1-63 |
| `app/config/netease.yaml` | L1-49 |
| `app/config.py` | L1-36 |
| `app/crawlers/unified_crawler.py` | L84-128 |
| `app/config.py` | L141-193 |
| `app/crawlers/manager.py` | L188-212 |
| `scripts/create_config.py` | L1-71 |
| `scripts/utils/create_config.py` | L1-71 |
| `app/utils/config.py` | L1-44 |
| `app/crawlers/components/config.py` | L227-271 |
| `run.py` | L62-85 |

</details>

---

Read file: app/db/sqlite_manager.py

---

<details>
            <summary>Searched codebase "web页面与数据库交互" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `WEB应用说明.md` | L67-70 |
| `newslook/web/routes.py` | L1-48 |
| `backup_old_web/web/routes.py` | L264-293 |
| `backup_old_web/web/routes.py` | L1-53 |
| `app/web/templates/settings.html` | L49-81 |
| `backup_old_web/web/routes.py` | L352-378 |
| `app/web/routes.py` | L524-546 |
| `app/web/templates/settings.html` | L199-234 |
| `database_match_report.md` | L1-73 |
| `app/web/routes.py` | L326-357 |
| `backup_old_web/web/routes.py` | L224-264 |
| `docs/database/数据库连接说明.md` | L227-286 |
| `docs/数据库连接说明.md` | L227-286 |
| `_archived_today3_20250324200011/app/web/routes.py` | L1-72 |
| `utils/database.py` | L244-298 |
| `app/web/routes.py` | L1-46 |
| `_archived_today3_20250324200011/app/web/templates/settings.html` | L49-81 |
| `_archived_today3_20250324200011/web/routes.py` | L1-52 |
| `app/web/templates/settings.html` | L151-199 |
| `backup_old_web/web/routes.py` | L191-224 |
| `app/web/routes.py` | L793-823 |
| `newslook/web/routes.py` | L165-216 |
| `app/web/routes.py` | L393-436 |
| `docs/database/数据库连接说明.md` | L1-57 |
| `newslook/web/routes.py` | L617-664 |

</details>

---

Read file: app/scheduler.py

---

<details>
            <summary>Searched codebase "web页面爬虫管理" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `backup_old_web/web/templates/crawler.html` | L1-53 |
| `backup/app/crawlers/optimized_crawler.py` | L77-116 |
| `app/crawlers/unified_crawler.py` | L33-84 |
| `app/web/routes.py` | L326-357 |
| `newslook/web/__init__.py` | L1-42 |
| `_archived_today3_20250324200011/web/templates/crawler.html` | L1-53 |
| `run.py` | L318-373 |
| `scripts/temp/run.py.bak.20250324200011` | L1-43 |
| `app/crawlers/base.py` | L377-419 |
| `newslook/web/__init__.py` | L121-164 |
| `run.py` | L85-104 |
| `app/utils/crawler_optimizer.py` | L45-92 |
| `app/crawlers/core/base_crawler.py` | L133-173 |
| `newslook/crawlers/tencent.py` | L106-148 |
| `_archived_today3_20250324200011/run.py` | L119-151 |
| `app/crawlers/sina.py` | L1759-1802 |
| `newslook/crawlers/base.py` | L371-413 |
| `_archived_today3_20250324200011/app/web/__init__.py` | L1-63 |
| `newslook/web/routes.py` | L116-165 |
| `ARCHITECTURE_REFACTORING.md` | L435-479 |
| `app/crawlers/manager.py` | L94-114 |
| `scripts/temp/run.py.bak.20250324200011` | L119-182 |
| `app/crawlers/sina.py` | L137-185 |
| `backup/app/crawlers/async_ifeng.py` | L45-84 |
| `app/crawlers/unified_crawler.py` | L469-496 |

</details>

---

Could not find file 'app/crawlers/strategies.py'. Did you mean one of:
- app/crawlers/strategies/__init__.py
- app/crawlers/strategies/base_strategy.py
- app/crawlers/strategies/sina_strategy.py

---

Read file: app/crawlers/strategies/__init__.py

---

Read file: app/crawlers/factory.py

---

基于我对项目代码的分析，以下是我的发现和建议：

## 现状分析

您的项目是一个财经新闻爬虫系统，目前存在以下情况：

1. **数据库结构**：
   - 每个爬虫（东方财富、新浪财经等）使用单独的数据库文件（如`东方财富.db`、`sina.db`等）
   - 同时有一个主数据库`finance_news.db`用于Web展示
   - 存在多个备份数据库文件（`.bak`后缀）

2. **爬虫系统**：
   - 使用工厂模式创建不同的爬虫实例
   - 支持多种爬虫策略（新浪、东方财富、网易、凤凰）
   - 有基本爬虫和增强版爬虫两种实现

3. **Web界面**：
   - 提供数据展示和爬虫管理功能
   - 当前使用`NewsDatabase`访问所有数据库文件

## 存在的问题

1. **数据库冗余**：每个爬虫一个数据库，同时又复制到主数据库，造成数据冗余
2. **备份策略混乱**：备份文件命名不规范，可能会造成混淆
3. **Web界面需要调整**：从您的描述看，Web界面可能需要适应新的数据库架构

## 优化建议

### 1. 数据库架构调整

```
├── data/
│   ├── db/
│   │   ├── sources/               # 源数据库目录
│   │   │   ├── eastmoney.db       # 东方财富数据库
│   │   │   ├── sina.db            # 新浪财经数据库
│   │   │   ├── netease.db         # 网易财经数据库
│   │   │   └── ifeng.db           # 凤凰财经数据库
│   │   ├── backups/               # 备份目录
│   │   │   ├── eastmoney_20250401.db.bak  # 标准命名的备份
│   │   │   └── ...
│   │   └── finance_news.db        # 主数据库（合并所有源）
```

### 2. 爬虫管理调整

- 每个爬虫使用独立数据库，同时自动同步到主数据库
- 实现定期备份机制，使用标准命名格式
- 在`CrawlerManager`类中添加数据库合并和备份功能

### 3. Web界面调整

- 添加数据源选择功能，允许用户查看特定来源的新闻或所有来源
- 添加数据库管理界面，方便用户管理多个数据库
- 提供手动备份和恢复功能

### 具体实施建议

1. **修改数据库连接类**:
   - 在`NewsDatabase`类中增加源选择功能
   - 修改`SQLiteManager`支持多数据库连接管理

2. **升级爬虫配置**:
   - 为每个爬虫指定专属数据库
   - 设计自动同步到主数据库的机制

3. **优化备份策略**:
   - 实现标准化的备份命名和管理
   - 添加定期自动备份功能

4. **改进Web界面**:
   - 添加数据源筛选功能
   - 提供数据库管理页面
   - 实现备份/还原操作界面

每个爬虫使用单独的数据库是一个好的设计，这样可以隔离各个来源的数据。同时通过主数据库进行整合展示也很实用。您只需要优化数据同步机制和Web界面，使其更好地适应这种架构。

您希望我提供具体的哪一部分实现代码吗？

---

_**User**_

修改数据库连接类:
在NewsDatabase类中增加源选择功能
修改SQLiteManager支持多数据库连接管理
升级爬虫配置:
为每个爬虫指定专属数据库
设计自动同步到主数据库的机制
优化备份策略:
实现标准化的备份命名和管理
添加定期自动备份功能
改进Web界面:
添加数据源筛选功能
提供数据库管理页面
实现备份/还原操作界面

---

_**Assistant**_

好的，我将为您实现这些功能的代码。让我们一步步来实施这些改进：

---

Read file: app/utils/database.py

---

```diff
      """新闻数据库工具类"""
      
-     def __init__(self, db_path=None, use_all_dbs=False):
+     def __init__(self, db_path=None, use_all_dbs=False, source=None):
          """
          初始化数据库连接
              db_path: 数据库路径，如果为None则使用默认路径
              use_all_dbs: 是否使用所有找到的数据库文件
+             source: 指定数据来源，用于筛选特定来源的数据库
          """
          settings = get_settings()
              self.db_dir = os.path.join(os.getcwd(), self.db_dir)
              
-         logger.info(f"初始化数据库连接，数据库目录: {self.db_dir}")
+         # 创建数据库子目录
+         self.sources_dir = os.path.join(self.db_dir, 'sources')
+         self.backups_dir = os.path.join(self.db_dir, 'backups')
          
          # 确保数据库目录存在
-         if not os.path.exists(self.db_dir):
-             logger.info(f"数据库目录不存在，创建目录: {self.db_dir}")
-             os.makedirs(self.db_dir, exist_ok=True)
+         for dir_path in [self.db_dir, self.sources_dir, self.backups_dir]:
+             if not os.path.exists(dir_path):
+                 logger.info(f"数据库目录不存在，创建目录: {dir_path}")
+                 os.makedirs(dir_path, exist_ok=True)
              
          # 如果指定了db_path，直接使用
          self.source_db_map = {}
          
-         # 查找所有.db文件
-         db_files = glob.glob(os.path.join(self.db_dir, '*.db'))
+         # 查找所有来源数据库文件
+         source_db_files = glob.glob(os.path.join(self.sources_dir, '*.db'))
+         if not source_db_files:  # 如果sources目录为空，则查找主目录中的数据库
+             source_db_files = glob.glob(os.path.join(self.db_dir, '*.db'))
              
          # 构建来源到数据库的映射
-         for db_file in db_files:
+         for db_file in source_db_files:
              file_name = os.path.basename(db_file)
              source_name = os.path.splitext(file_name)[0]
                  logger.info(f"找到来源 '{source_name}' 的专用数据库: {db_file}")
          
-         if use_all_dbs:
+         # 根据指定的source参数选择数据库
+         if source and source in self.source_db_map:
+             # 如果指定了来源并且找到了对应的数据库
+             self.all_db_paths = [self.source_db_map[source]]
+             logger.info(f"使用来源 '{source}' 的专用数据库: {self.all_db_paths[0]}")
+         elif use_all_dbs:
              # 使用所有数据库文件，包括来源专用数据库
-             self.all_db_paths = db_files
+             self.all_db_paths = [self.db_path]  # 首先添加主数据库
+             # 添加所有来源数据库
+             for source_db in self.source_db_map.values():
+                 if source_db not in self.all_db_paths:
+                     self.all_db_paths.append(source_db)
              logger.info(f"找到 {len(self.all_db_paths)} 个数据库文件")
              
                  logger.warning(f"数据库文件不存在: {db_path}")
          
+         # 存储指定的来源
+         self.selected_source = source
+         
          # 线程安全：不在初始化时创建长期连接，而是每次需要时创建
          self.engine = None
          self.session = None
          self._init_session()
+         
+         # 存储多数据库连接
+         self.engines = {}
+         self.sessions = {}
      
      def _init_session(self):
          Session = sessionmaker(bind=self.engine)
          self.session = Session()
+         
+         # 初始化多数据库连接
+         for db_path in self.all_db_paths:
+             if db_path != self.db_path and db_path not in self.engines:
+                 self.engines[db_path] = create_engine(f'sqlite:///{db_path}')
+                 Base.metadata.create_all(self.engines[db_path])
+                 Session = sessionmaker(bind=self.engines[db_path])
+                 self.sessions[db_path] = Session()
      
      def __enter__(self):
          return False  # 返回False表示不处理异常
      
+     def close(self):
+         """关闭数据库连接"""
+         try:
+             if self.session:
+                 self.session.close()
+                 
+             # 关闭所有会话
+             for session in self.sessions.values():
+                 if session:
+                     session.close()
+                     
+             # 清空会话和引擎缓存
+             self.sessions = {}
+             self.engines = {}
+             
+             logger.debug("已关闭所有数据库连接")
+         except Exception as e:
+             logger.error(f"关闭数据库连接时出错: {str(e)}")
+     
      def add_news(self, news_data):
-         """添加新闻"""
-         try:
+         """
+         添加新闻到数据库
+         
+         Args:
+             news_data: 新闻数据字典
+             
+         Returns:
+             bool: 是否成功添加
+         """
+         # 确定目标数据库路径
+         target_db_path = self.db_path
+         
+         # 如果新闻有来源信息，也保存到对应的来源数据库
+         if 'source' in news_data and news_data['source'] in self.source_db_map:
+             source_db_path = self.source_db_map[news_data['source']]
+             # 先保存到来源数据库
+             self._add_news_to_db(news_data, source_db_path)
+             
+         # 始终保存到主数据库
+         return self._add_news_to_db(news_data, target_db_path)
+     
+     def _add_news_to_db(self, news_data, db_path):
+         """
+         将新闻添加到指定的数据库
+         
+         Args:
+             news_data: 新闻数据字典
+             db_path: 数据库路径
+             
+         Returns:
+             bool: 是否成功添加
+         """
+         try:
+             # 获取对应的会话
+             if db_path == self.db_path:
+                 session = self.session
+             else:
+                 if db_path not in self.sessions:
+                     self.engines[db_path] = create_engine(f'sqlite:///{db_path}')
+                     Base.metadata.create_all(self.engines[db_path])
+                     Session = sessionmaker(bind=self.engines[db_path])
+                     self.sessions[db_path] = Session()
+                 session = self.sessions[db_path]
+             
              # 检查新闻是否已存在
-             existing_news = self.session.query(News).filter(News.url == news_data['url']).first()
+             existing_news = session.query(News).filter(News.url == news_data['url']).first()
              if existing_news:
-                 logger.debug(f"新闻已存在: {news_data['title']}")
+                 logger.debug(f"新闻已存在于数据库 {db_path}: {news_data['title']}")
                  return False
                  
              else:
                  keywords = json.dumps([])
-             
-             # 确保source不为空
-             source = news_data.get('source', '未知来源')
-             if not source or source == '未知来源':
-                 # 尝试从URL中推断来源
-                 if 'url' in news_data:
-                     url = news_data['url']
-                     if 'eastmoney.com' in url:
-                         source = '东方财富网'
-                     elif 'sina.com' in url or 'sina.cn' in url:
-                         source = '新浪财经'
-                     elif 'qq.com' in url or 'finance.qq.com' in url or 'news.qq.com' in url:
-                         source = '腾讯财经'
-                     elif '163.com' in url or 'money.163.com' in url:
-                         source = '网易财经'
-                     elif 'ifeng.com' in url:
-                         source = '凤凰财经'
-                     elif 'jrj.com' in url:
-                         source = '金融界'
-                     elif 'cs.com.cn' in url:
-                         source = '中国证券网'
-                     elif 'hexun.com' in url:
-                         source = '和讯网'
-                     elif 'cnstock.com' in url:
-                         source = '中证网'
-                     elif 'xinhuanet.com' in url:
-                         source = '新华网'
-                     elif 'people.com.cn' in url:
-                         source = '人民网'
-                     elif 'cctv.com' in url:
-                         source = 'CCTV'
-                     elif 'stcn.com' in url:
-                         source = '证券时报网'
-                     elif '21jingji.com' in url:
-                         source = '21世纪经济报道'
-                     elif 'caixin.com' in url:
-                         source = '财新网'
                  
              # 创建新闻对象
              news = News(
                  title=news_data['title'],
-                 content=news_data['content'],
+                 content=news_data.get('content', ''),
                  url=news_data['url'],
-                 source=source,
-                 category=news_data.get('category', '财经'),
+                 source=news_data.get('source', '未知来源'),
+                 category=news_data.get('category', '未分类'),
                  pub_time=pub_time,
                  crawl_time=crawl_time,
                  keywords=keywords
              )
              
-             self.session.add(news)
-             self.session.commit()
-             logger.info(f"新闻添加成功: {news_data['title']}")
+             # 添加到数据库
+             session.add(news)
+             session.commit()
+             logger.info(f"成功添加新闻到数据库 {db_path}: {news_data['title']}")
              return True
+         
          except Exception as e:
-             self.session.rollback()
-             logger.error(f"添加新闻失败: {str(e)}")
-             if 'title' in news_data:
-                 logger.error(f"标题: {news_data['title']}")
+             logger.error(f"添加新闻到数据库 {db_path} 失败: {str(e)}")
+             if 'session' in locals():
+                 session.rollback()
              return False
      
-     def query_news(self, keyword=None, days=None, source=None, limit=10, offset=0):
-         """
-         根据条件查询新闻
-         
-         Args:
-             keyword: 搜索关键词（可选）
-             days: 最近几天的新闻（可选）
-             source: 新闻来源（可选）
-             limit: 返回结果数量限制
-             offset: 分页偏移量
+     def get_available_sources(self):
+         """
+         获取所有可用的数据源
          
          Returns:
-             list: 新闻列表
-         """
-         results = []
-         
-         try:
-             # 确定要查询的数据库
-             db_paths = []
-             
-             # 如果指定了来源，优先使用对应的专用数据库
-             if source and source in self.source_db_map:
-                 source_db = self.source_db_map[source]
-                 if os.path.exists(source_db):
-                     db_paths.append(source_db)
-                     logger.info(f"使用来源 '{source}' 的专用数据库: {source_db}")
-                 else:
-                     logger.warning(f"找不到来源 '{source}' 的专用数据库，回退到默认数据库")
-                     db_paths = self.all_db_paths
-             else:
-                 # 如果未指定来源或找不到对应专用数据库，则查询所有数据库
-                 db_paths = self.all_db_paths
-                 logger.info(f"未指定来源或找不到专用数据库，使用所有数据库: {db_paths}")
-                 
-             # 检查数据库文件是否存在
-             valid_db_paths = []
-             for db_path in db_paths:
-                 if os.path.exists(db_path):
-                     valid_db_paths.append(db_path)
-                 else:
-                     logger.warning(f"数据库文件不存在: {db_path}")
-             
-             if not valid_db_paths:
-                 logger.error(f"没有找到有效的数据库文件")
-                 return []
-             
-             # 记录已处理的新闻URL，避免重复
-             processed_urls = set()
-                 
-             # 遍历数据库文件查询
-             for db_path in valid_db_paths:
-                 try:
-                     # 尝试从数据库文件名中提取来源
-                     db_source = os.path.basename(db_path).replace('.db', '')
-                     logger.info(f"从数据库 {db_path} 中查询新闻，来源: {db_source}")
-                     
-                     # 连接数据库
-                     conn = sqlite3.connect(db_path)
-                     conn.row_factory = sqlite3.Row
-                     cur = conn.cursor()
-                     
-                     # 构建查询条件
-                     conditions = []
-                     params = []
-                     
-                     if source:
-                         conditions.append("source = ?")
-                         params.append(source)
-                     
-                     if keyword:
-                         # 从标题和内容中搜索
-                         conditions.append("(title LIKE ? OR content LIKE ?)")
-                         params.extend([f'%{keyword}%', f'%{keyword}%'])
-                     
-                     if days:
-                         # 计算日期范围
-                         days_ago = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
-                         conditions.append("pub_time >= ?")
-                         params.append(days_ago)
-                     
-                     # 构建SQL查询
-                     query = "SELECT * FROM news"
-                     if conditions:
-                         query += " WHERE " + " AND ".join(conditions)
-                     
-                     # 添加排序和分页
-                     # 注意：这里不添加LIMIT和OFFSET，因为我们需要合并结果后再分页
-                     query += " ORDER BY pub_time DESC"
-                     
-                     logger.info(f"执行查询: {query} 参数: {params}")
-                     
-                     # 执行查询
-                     try:
-                         cur.execute(query, params)
-                         rows = cur.fetchall()
-                         logger.info(f"从 {db_path} 数据库查询到 {len(rows)} 条新闻")
-                         
-                         for row in rows:
-                             news = dict(row)
-                             
-                             # 检查URL是否已处理，避免重复
-                             if 'url' in news and news['url'] in processed_urls:
-                                 continue
-                                 
-                             # 添加到已处理URL集合
-                             if 'url' in news:
-                                 processed_urls.add(news['url'])
-                             
-                             # 处理关键词字段
-                             if 'keywords' in news and news['keywords']:
-                                 try:
-                                     news['keywords'] = json.loads(news['keywords'])
-                                 except json.JSONDecodeError:
-                                     news['keywords'] = []
-                             else:
-                                 news['keywords'] = []
-                             
-                             # 处理图片字段
-                             if 'images' in news and news['images']:
-                                 try:
-                                     news['images'] = json.loads(news['images'])
-                                 except json.JSONDecodeError:
-                                     news['images'] = []
-                             else:
-                                 news['images'] = []
-                                 
-                             # 检查必需字段
-                             required_fields = ['id', 'title', 'content', 'url', 'pub_time', 'source']
-                             for field in required_fields:
-                                 if field not in news or not news[field]:
-                                     if field == 'source':
-                                         news['source'] = db_source or '未知来源'
-                                     elif field == 'id':
-                                         news['id'] = str(uuid.uuid4())
-                                     elif field == 'pub_time':
-                                         news['pub_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
-                                     
-                             results.append(news)
-                     except sqlite3.OperationalError as e:
-                         logger.error(f"执行查询失败: {str(e)}")
-                         # 可能是表不存在，尝试创建表
-                         if "no such table" in str(e).lower():
-                             logger.warning(f"数据库 {db_path} 中表不存在，尝试创建表")
-                             try:
-                                 self._create_tables(conn)
-                                 logger.info(f"成功创建表结构")
-                             except Exception as create_err:
-                                 logger.error(f"创建表失败: {str(create_err)}")
-                     
-                     conn.close()
-                 except Exception as db_error:
-                     logger.error(f"处理数据库 {db_path} 时出错: {str(db_error)}")
-             
-             # 按发布时间排序合并后的结果
-             results.sort(key=lambda x: x.get('pub_time', ''), reverse=True)
-             
-             # 应用分页
-             return results[offset:offset+limit]
-             
-         except Exception as e:
-             logger.error(f"查询新闻失败: {str(e)}")
-             
-         return results
-     
-     def _create_tables(self, conn):
-         """创建数据库表结构"""
-         cur = conn.cursor()
-         # 创建新闻表
-         cur.execute('''
-             CREATE TABLE IF NOT EXISTS news (
-                 id TEXT PRIMARY KEY,
-                 title TEXT NOT NULL,
-                 content TEXT,
-                 content_html TEXT,
-                 pub_time TEXT,
-                 author TEXT,
-                 source TEXT,
-                 url TEXT UNIQUE,
-                 keywords TEXT,
-                 images TEXT,
-                 related_stocks TEXT,
-                 sentiment TEXT,
-                 classification TEXT,
-                 category TEXT,
-                 crawl_time TEXT
-             )
-         ''')
-         
-         # 创建关键词表
-         cur.execute('''
-             CREATE TABLE IF NOT EXISTS keywords (
-                 id INTEGER PRIMARY KEY AUTOINCREMENT,
-                 keyword TEXT UNIQUE,
-                 count INTEGER DEFAULT 1,
-                 last_updated TEXT
-             )
-         ''')
-         
-         # 创建新闻-关键词关系表
-         cur.execute('''
-             CREATE TABLE IF NOT EXISTS news_keywords (
-                 news_id TEXT,
-                 keyword_id INTEGER,
-                 PRIMARY KEY (news_id, keyword_id),
-                 FOREIGN KEY (news_id) REFERENCES news(id) ON DELETE CASCADE,
-                 FOREIGN KEY (keyword_id) REFERENCES keywords(id) ON DELETE CASCADE
-             )
-         ''')
-         
-         conn.commit()
-     
-     def get_news_count(self, keyword=None, days=None, source=None):
-         """
-         获取新闻数量
+             list: 数据源列表
+         """
+         return list(self.source_db_map.keys())
+     
+     def backup_database(self, source=None):
+         """
+         备份数据库
          
          Args:
-             keyword: 搜索关键词（可选）
-             days: 最近几天的新闻（可选）
-             source: 新闻来源（可选）
+             source: 要备份的数据源，如果为None则备份主数据库
              
          Returns:
-             int: 符合条件的新闻总数
-         """
-         total_count = 0
-         
-         try:
-             # 确定要查询的数据库
-             db_paths = []
-             
-             # 如果指定了来源，优先使用对应的专用数据库
+             str: 备份文件路径
+         """
+         # 确定要备份的数据库路径
          if source and source in self.source_db_map:
-                 source_db = self.source_db_map[source]
-                 if os.path.exists(source_db):
-                     db_paths.append(source_db)
-                     logger.info(f"计数使用来源 '{source}' 的专用数据库: {source_db}")
+             db_path = self.source_db_map[source]
          else:
-                     logger.warning(f"计数找不到来源 '{source}' 的专用数据库，回退到默认数据库")
-                     db_paths = self.all_db_paths
-             else:
-                 # 如果未指定来源或找不到对应专用数据库，则查询所有数据库
-                 db_paths = self.all_db_paths
-                 logger.info(f"计数未指定来源或找不到专用数据库，使用所有数据库: {db_paths}")
-             
-             # 检查数据库文件是否存在
-             valid_db_paths = []
-             for db_path in db_paths:
-                 if os.path.exists(db_path):
-                     valid_db_paths.append(db_path)
-                 else:
-                     logger.warning(f"数据库文件不存在: {db_path}")
-             
-             if not valid_db_paths:
-                 logger.error(f"没有找到有效的数据库文件")
-                 return 0
-             
-             # 记录已统计的URL，避免重复计数
-             counted_urls = set()
-             
-             # 尝试从所有数据库查询
-             for db_path in valid_db_paths:
-                 try:
-                     # 尝试从数据库文件名中提取来源
-                     db_source = os.path.basename(db_path).replace('.db', '')
-                     logger.info(f"从数据库 {db_path} 中计数，来源: {db_source}")
-                     
-                     conn = sqlite3.connect(db_path)
-                     cursor = conn.cursor()
-                     
-                     # 判断news表是否存在
-                     cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
-                     if not cursor.fetchone():
-                         # 表不存在，创建表结构
-                         logger.warning(f"数据库 {db_path} 中news表不存在，尝试创建表")
-                         try:
-                             self._create_tables(conn)
-                             logger.info(f"成功创建表结构")
-                         except Exception as create_err:
-                             logger.error(f"创建表失败: {str(create_err)}")
-                             conn.close()
-                             continue
-                     
-                     # 如果是计数URL去重，需要先获取所有URL
-                     if source or keyword or days:
-                         # 构建查询条件
-                         conditions = []
-                         params = []
-                         
-                         if source:
-                             conditions.append("source = ?")
-                             params.append(source)
-                         
-                         if keyword:
-                             conditions.append("(title LIKE ? OR content LIKE ?)")
-                             params.extend([f"%{keyword}%", f"%{keyword}%"])
-                         
-                         if days:
-                             start_time = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
-                             conditions.append("pub_time >= ?")
-                             params.append(start_time)
-                         
-                         # 先获取所有匹配的URL进行去重
-                         sql = "SELECT url FROM news"
-                         if conditions:
-                             sql += " WHERE " + " AND ".join(conditions)
-                         
-                         cursor.execute(sql, params)
-                         for url_row in cursor.fetchall():
-                             url = url_row[0]
-                             if url and url not in counted_urls:
-                                 counted_urls.add(url)
-                                 total_count += 1
-                     else:
-                         # 如果没有筛选条件，直接计数（但仍需去重）
-                         cursor.execute("SELECT url FROM news")
-                         for url_row in cursor.fetchall():
-                             url = url_row[0]
-                             if url and url not in counted_urls:
-                                 counted_urls.add(url)
-                                 total_count += 1
-                     
-                     conn.close()
+             db_path = self.db_path
+             
+         # 生成备份文件名
+         timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
+         source_name = source or 'main'
+         backup_filename = f"{source_name}_{timestamp}.db.bak"
+         backup_path = os.path.join(self.backups_dir, backup_filename)
+         
+         # 执行备份
+         try:
+             shutil.copy2(db_path, backup_path)
+             logger.info(f"成功备份数据库 {db_path} 到 {backup_path}")
+             return backup_path
          except Exception as e:
-                     logger.error(f"查询数据库 {db_path} 计数失败: {str(e)}")
-                     import traceback
-                     logger.error(traceback.format_exc())
-             
-             logger.info(f"查询结果: 共 {total_count} 条符合条件的新闻")
-         except Exception as e:
-             logger.error(f"获取新闻数量失败: {str(e)}")
-         
-         return total_count
-     
-     def get_sources(self):
-         """获取所有新闻来源"""
-         sources = set()
-         
-         try:
-             # 检查数据库文件是否存在
-             valid_db_paths = []
-             for db_path in self.all_db_paths:
-                 if os.path.exists(db_path):
-                     valid_db_paths.append(db_path)
-                 else:
-                     logger.warning(f"数据库文件不存在: {db_path}")
-             
-             if not valid_db_paths:
-                 logger.error(f"没有找到有效的数据库文件")
-                 return []
-             
-             # 尝试从所有数据库查询
-             for db_path in valid_db_paths:
-                 try:
-                     conn = sqlite3.connect(db_path)
-                     cursor = conn.cursor()
-                     
-                     # 判断news表是否存在
-                     cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
-                     if not cursor.fetchone():
-                         # 表不存在，跳过此数据库
-                         logger.warning(f"数据库 {db_path} 中news表不存在，跳过")
-                         conn.close()
-                         continue
-                     
-                     cursor.execute("SELECT DISTINCT source FROM news")
-                     for row in cursor.fetchall():
-                         if row[0]:  # 排除空值
-                             sources.add(row[0])
-                     
-                     # 从文件名中提取来源名称
-                     source_from_filename = os.path.splitext(os.path.basename(db_path))[0]
-                     # 如果不是主数据库，添加到来源列表
-                     if source_from_filename not in ['finance_news', 'main_news']:
-                         sources.add(source_from_filename)
-                     
-                     conn.close()
-                 except Exception as e:
-                     logger.error(f"查询数据库 {db_path} 来源失败: {str(e)}")
-             
-             # 优先使用专用数据库名称作为来源
-             for source_name in self.source_db_map.keys():
-                 if source_name not in ['finance_news', 'main_news']:
-                     sources.add(source_name)
-         except Exception as e:
-             logger.error(f"获取新闻来源失败: {str(e)}")
-         
-         return sorted(list(sources))
-     
-     def get_categories(self):
-         """获取所有分类"""
-         categories = set()
-         
-         try:
-             # 检查数据库文件是否存在
-             valid_db_paths = []
-             for db_path in self.all_db_paths:
-                 if os.path.exists(db_path):
-                     valid_db_paths.append(db_path)
-                 else:
-                     logger.warning(f"数据库文件不存在: {db_path}")
-             
-             if not valid_db_paths:
-                 logger.error(f"没有找到有效的数据库文件")
-                 return []
-             
-             # 尝试从所有数据库查询
-             for db_path in valid_db_paths:
-                 try:
-                     conn = sqlite3.connect(db_path)
-                     cursor = conn.cursor()
-                     
-                     # 判断news表是否存在
-                     cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
-                     if not cursor.fetchone():
-                         # 表不存在，跳过此数据库
-                         logger.warning(f"数据库 {db_path} 中news表不存在，跳过")
-                         conn.close()
-                         continue
-                     
-                     cursor.execute("SELECT DISTINCT category FROM news")
-                     for row in cursor.fetchall():
-                         if row[0]:  # 排除空值
-                             categories.add(row[0])
-                     
-                     conn.close()
-                 except Exception as e:
-                     logger.error(f"查询数据库 {db_path} 分类失败: {str(e)}")
-         except Exception as e:
-             logger.error(f"获取新闻分类失败: {str(e)}")
-         
-         return sorted(list(categories))
-     
-     def close(self):
-         """关闭数据库连接"""
-         if self.session:
-             try:
-                 self.session.close()
-             except Exception as e:
-                 logger.error(f"关闭数据库会话出错: {str(e)}")
-             finally:
-                 self.session = None
-         
-         if self.engine:
-             try:
-                 self.engine.dispose()
-             except Exception as e:
-                 logger.error(f"关闭数据库引擎出错: {str(e)}")
-             finally:
-                 self.engine = None
- 
-     def get_news_by_id(self, news_id):
-         """根据ID获取新闻详情
-         
-         Args:
-             news_id: 新闻ID
-             
-         Returns:
-             dict: 新闻数据，如果未找到则返回None
-         """
-         try:
-             # 确定要查询的数据库列表
-             db_paths = self.all_db_paths
-             
-             # 检查数据库文件是否存在
-             valid_db_paths = []
-             for db_path in db_paths:
-                 if os.path.exists(db_path):
-                     valid_db_paths.append(db_path)
-                 else:
-                     logger.warning(f"数据库文件不存在: {db_path}")
-             
-             if not valid_db_paths:
-                 logger.error(f"没有找到有效的数据库文件")
+             logger.error(f"备份数据库 {db_path} 失败: {str(e)}")
              return None
      
-             # 尝试从所有数据库查询
-             for db_path in valid_db_paths:
-                 try:
-                     # 尝试从数据库文件名中提取来源
-                     db_source = os.path.basename(db_path).replace('.db', '')
-                     logger.debug(f"在数据库 {db_path} 中查询新闻ID: {news_id}")
-                     
-                     conn = sqlite3.connect(db_path)
-                     conn.row_factory = sqlite3.Row
-                     cursor = conn.cursor()
-                     
-                     # 判断news表是否存在
-                     cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
-                     if not cursor.fetchone():
-                         # 表不存在，跳过此数据库
-                         logger.warning(f"数据库 {db_path} 中news表不存在，跳过")
-                         conn.close()
-                         continue
-                     
-                     # 查询指定ID的新闻
-                     cursor.execute("SELECT * FROM news WHERE id = ?", (news_id,))
-                     row = cursor.fetchone()
-                     
-                     if row:
-                         # 转换为字典
-                         news = {}
-                         for i, column in enumerate(cursor.description):
-                             news[column[0]] = row[i]
-                         
-                         # 处理JSON字段
-                         try:
-                             if 'keywords' in news and news['keywords']:
-                                 news['keywords'] = json.loads(news['keywords'])
-                             if 'images' in news and news['images']:
-                                 news['images'] = json.loads(news['images'])
-                             if 'related_stocks' in news and news['related_stocks']:
-                                 news['related_stocks'] = json.loads(news['related_stocks'])
-                         except json.JSONDecodeError:
-                             # 如果JSON解析失败，保持原始字符串
-                             pass
-                         
-                         conn.close()
-                         logger.info(f"在数据库 {db_path} 中找到新闻ID: {news_id}")
-                         return news  # 找到后立即返回
-                     
-                     conn.close()
-                 except Exception as e:
-                     logger.error(f"查询新闻详情失败: {str(e)}")
-             
-             logger.warning(f"在所有数据库中未找到新闻ID: {news_id}")
-             return None  # 所有数据库都未找到，返回None
-         except Exception as e:
-             logger.error(f"获取新闻详情失败: {str(e)}")
-             return None
- 
-     def update_unknown_sources(self):
-         """
-         更新数据库中所有未知来源的新闻，根据URL推断正确的数据源
-         
-         Returns:
-             int: 更新的记录数量
-         """
-         try:
-             logger.info("开始更新未知来源的新闻数据...")
-             
-             # 查询所有未知来源的新闻
-             unknown_news = self.session.query(News).filter(
-                 (News.source == '未知来源') | 
-                 (News.source == '') | 
-                 (News.source == None)
-             ).all()
-             
-             if not unknown_news:
-                 logger.info("没有找到未知来源的新闻")
-                 return 0
-                 
-             updated_count = 0
-             
-             for news in unknown_news:
-                 if not news.url:
-                     continue
-                     
-                 url = news.url.lower()
-                 new_source = None
-                 
-                 # 根据URL推断来源
-                 if 'eastmoney.com' in url:
-                     new_source = '东方财富网'
-                 elif 'sina.com' in url or 'sina.cn' in url:
-                     new_source = '新浪财经'
-                 elif 'qq.com' in url or 'finance.qq.com' in url or 'news.qq.com' in url:
-                     new_source = '腾讯财经'
-                 elif '163.com' in url or 'money.163.com' in url:
-                     new_source = '网易财经'
-                 elif 'ifeng.com' in url:
-                     new_source = '凤凰财经'
-                 elif 'jrj.com' in url:
-                     new_source = '金融界'
-                 elif 'cs.com.cn' in url:
-                     new_source = '中国证券网'
-                 elif 'hexun.com' in url:
-                     new_source = '和讯网'
-                 elif 'cnstock.com' in url:
-                     new_source = '中证网'
-                 elif 'xinhuanet.com' in url:
-                     new_source = '新华网'
-                 elif 'people.com.cn' in url:
-                     new_source = '人民网'
-                 elif 'cctv.com' in url:
-                     new_source = 'CCTV'
-                 elif 'stcn.com' in url:
-                     new_source = '证券时报网'
-                 elif '21jingji.com' in url:
-                     new_source = '21世纪经济报道'
-                 elif 'caixin.com' in url:
-                     new_source = '财新网'
-                 
-                 if new_source:
-                     news.source = new_source
-                     updated_count += 1
-             
-             if updated_count > 0:
-                 self.session.commit()
-                 logger.info(f"成功更新 {updated_count} 条未知来源的新闻")
-             
-             return updated_count
-             
-         except Exception as e:
-             self.session.rollback()
-             logger.error(f"更新未知来源失败: {str(e)}")
-             return 0
- 
-     def get_news_count_by_sentiment(self, min_value, max_value):
-         """
-         获取指定情感值范围内的新闻数量
+     def restore_database(self, backup_path, source=None):
+         """
+         从备份恢复数据库
          
          Args:
-             min_value: 情感值最小值
-             max_value: 情感值最大值
+             backup_path: 备份文件路径
+             source: 要恢复的数据源，如果为None则恢复到主数据库
              
          Returns:
-             int: 符合条件的新闻总数
-         """
-         total_count = 0
-         
-         try:
-             # 遍历所有数据库文件
-             for db_path in self.all_db_paths:
-                 if not os.path.exists(db_path):
-                     continue
-                     
-                 try:
-                     conn = sqlite3.connect(db_path)
-                     cursor = conn.cursor()
-                     
-                     # 检查news表是否存在
-                     cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
-                     if not cursor.fetchone():
-                         conn.close()
-                         continue
-                     
-                     # 检查sentiment字段是否存在
-                     cursor.execute("PRAGMA table_info(news)")
-                     columns = [column[1] for column in cursor.fetchall()]
-                     if 'sentiment' not in columns:
-                         logger.warning(f"数据库 {db_path} 中news表没有sentiment字段")
-                         conn.close()
-                         continue
-                     
-                     # 查询符合情感值范围的新闻数量
-                     # 处理不同情感值储存格式：数值型、字符串型和NULL值
-                     sql = """
-                     SELECT COUNT(*) FROM news 
-                     WHERE sentiment IS NOT NULL AND sentiment != '' AND (
-                         (TYPEOF(sentiment) = 'real' AND sentiment >= ? AND sentiment <= ?) OR
-                         (TYPEOF(sentiment) = 'text' AND CAST(sentiment AS REAL) >= ? AND CAST(sentiment AS REAL) <= ?)
-                     )
-                     """
-                     cursor.execute(sql, (min_value, max_value, min_value, max_value))
-                     count = cursor.fetchone()[0]
-                     total_count += count
-                     
-                     conn.close()
-                 except Exception as e:
-                     logger.error(f"查询数据库 {db_path} 情感统计失败: {str(e)}")
-             
-             logger.info(f"情感值在 {min_value} 和 {max_value} 之间的新闻数量: {total_count}")
-         except Exception as e:
-             logger.error(f"获取情感新闻数量失败: {str(e)}")
-         
-         return total_count
- 
-     def get_sentiment_distribution(self, buckets=5):
-         """
-         获取情感值分布统计
-         
-         Args:
-             buckets: 分桶数量，默认为5个等分桶
-             
-         Returns:
-             dict: 情感值分布统计，例如 {"-1.0~-0.6": 10, "-0.6~-0.2": 20, ...}
-         """
-         distribution = {}
-         
-         # 初始化分桶
-         step = 2.0 / buckets  # 情感值范围从-1到1，总跨度为2
-         for i in range(buckets):
-             low = -1.0 + i * step
-             high = low + step
-             # 保留一位小数
-             low_str = f"{low:.1f}"
-             high_str = f"{high:.1f}"
-             bucket_key = f"{low_str}~{high_str}"
-             distribution[bucket_key] = 0
-         
-         try:
-             # 遍历所有数据库文件
-             for db_path in self.all_db_paths:
-                 if not os.path.exists(db_path):
-                     continue
-                     
-                 try:
-                     conn = sqlite3.connect(db_path)
-                     cursor = conn.cursor()
-                     
-                     # 检查news表是否存在
-                     cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
-                     if not cursor.fetchone():
-                         conn.close()
-                         continue
-                     
-                     # 检查sentiment字段是否存在
-                     cursor.execute("PRAGMA table_info(news)")
-                     columns = [column[1] for column in cursor.fetchall()]
-                     if 'sentiment' not in columns:
-                         logger.warning(f"数据库 {db_path} 中news表没有sentiment字段")
-                         conn.close()
-                         continue
-                     
-                     # 查询所有有效的情感值
-                     cursor.execute("""
-                     SELECT sentiment FROM news 
-                     WHERE sentiment IS NOT NULL AND sentiment != '' AND (
-                         TYPEOF(sentiment) = 'real' OR 
-                         (TYPEOF(sentiment) = 'text' AND sentiment GLOB '*[0-9]*')
-                     )
-                     """)
-                     
-                     for row in cursor.fetchall():
-                         try:
-                             sentiment_value = float(row[0])
-                             # 确保值在-1到1范围内
-                             sentiment_value = max(-1.0, min(1.0, sentiment_value))
-                             
-                             # 确定所属分桶
-                             bucket_index = min(buckets - 1, int((sentiment_value + 1.0) / step))
-                             low = -1.0 + bucket_index * step
-                             high = low + step
-                             low_str = f"{low:.1f}"
-                             high_str = f"{high:.1f}"
-                             bucket_key = f"{low_str}~{high_str}"
-                             
-                             distribution[bucket_key] += 1
-                         except (ValueError, TypeError):
-                             # 忽略无法转换的值
-                             continue
-                     
-                     conn.close()
-                 except Exception as e:
-                     logger.error(f"查询数据库 {db_path} 情感分布失败: {str(e)}")
-             
-             logger.info(f"情感分布统计结果: {distribution}")
-         except Exception as e:
-             logger.error(f"获取情感分布失败: {str(e)}")
-         
-         return distribution
- 
-     def ensure_source_table_exists(self, source_name):
-         """
-         确保来源专用数据库中存在news表
-         
-         Args:
-             source_name: 来源名称，对应数据库文件名
-         
-         Returns:
-             bool: 表是否存在或创建成功
-         """
-         try:
-             # 确定数据库路径
-             db_path = None
-             if source_name in self.source_db_map:
-                 db_path = self.source_db_map[source_name]
+             bool: 是否成功恢复
+         """
+         # 确定要恢复的目标数据库路径
+         if source and source in self.source_db_map:
+             target_db_path = self.source_db_map[source]
          else:
-                 # 如果映射中没有，尝试根据命名规则找到数据库
-                 possible_path = os.path.join(self.db_dir, f"{source_name}.db")
-                 if os.path.exists(possible_path):
-                     db_path = possible_path
-                     # 更新映射
-                     self.source_db_map[source_name] = db_path
-                     logger.info(f"找到来源 '{source_name}' 的专用数据库: {db_path}")
-                 else:
-                     logger.warning(f"未找到来源 '{source_name}' 的专用数据库")
+             target_db_path = self.db_path
+             
+         # 确保备份文件存在
+         if not os.path.exists(backup_path):
+             logger.error(f"备份文件不存在: {backup_path}")
              return False
              
-             # 检查news表是否存在，若不存在则创建
-             try:
-                 conn = sqlite3.connect(db_path)
-                 cursor = conn.cursor()
-                 
-                 # 检查news表是否存在
-                 cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
-                 if not cursor.fetchone():
-                     logger.warning(f"数据库 {db_path} 中news表不存在，正在创建...")
-                     
-                     # 创建news表
-                     cursor.execute('''
-                     CREATE TABLE IF NOT EXISTS news (
-                         id TEXT PRIMARY KEY,
-                         title TEXT NOT NULL,
-                         content TEXT,
-                         content_html TEXT,
-                         pub_time TEXT,
-                         author TEXT,
-                         source TEXT,
-                         url TEXT UNIQUE,
-                         keywords TEXT,
-                         images TEXT,
-                         related_stocks TEXT,
-                         sentiment TEXT,
-                         classification TEXT,
-                         category TEXT,
-                         crawl_time TEXT
-                     )
-                     ''')
-                     
-                     # 创建索引
-                     cursor.execute("CREATE INDEX IF NOT EXISTS idx_news_url ON news (url)")
-                     cursor.execute("CREATE INDEX IF NOT EXISTS idx_news_source ON news (source)")
-                     cursor.execute("CREATE INDEX IF NOT EXISTS idx_news_pub_time ON news (pub_time)")
-                     
-                     conn.commit()
-                     logger.info(f"在数据库 {db_path} 中成功创建news表和索引")
-                 else:
-                     logger.debug(f"数据库 {db_path} 中news表已存在")
-                 
-                 conn.close()
+         # 执行恢复
+         try:
+             # 关闭相关的数据库连接
+             if target_db_path == self.db_path:
+                 if self.session:
+                     self.session.close()
+             elif target_db_path in self.sessions:
+                 self.sessions[target_db_path].close()
+                 del self.sessions[target_db_path]
+                 del self.engines[target_db_path]
+                 
+             # 复制备份文件到目标位置
+             shutil.copy2(backup_path, target_db_path)
+             logger.info(f"成功恢复数据库 {target_db_path} 从备份 {backup_path}")
+             
+             # 重新初始化会话
+             self._init_session()
              return True
          except Exception as e:
-                 logger.error(f"检查或创建表结构时出错: {str(e)}")
-                 return False
-         except Exception as e:
-             logger.error(f"确保来源表存在时出错: {str(e)}")
+             logger.error(f"恢复数据库 {target_db_path} 失败: {str(e)}")
              return False
      
- class DatabaseManager:
-     """数据库管理器，提供数据库连接和初始化功能"""
-     
-     def __init__(self, db_dir='db'):
+     def get_backups(self, source=None):
          """
-         初始化数据库管理器
+         获取指定来源的所有备份文件
          
          Args:
-             db_dir: 数据库目录或数据库文件路径
-         """
-         # 检查db_dir是否是一个文件路径（以.db结尾）
-         if db_dir.endswith('.db'):
-             # 如果是文件路径，分离目录和文件名
-             self.db_dir = os.path.dirname(db_dir)
-             self.main_db_path = db_dir
-         else:
-             # 如果是目录路径，使用默认文件名
-             self.db_dir = db_dir
-             self.main_db_path = os.path.join(db_dir, 'finance_news.db')
-         
-         # 确保数据库目录存在
-         if self.db_dir and not os.path.exists(self.db_dir):
-             try:
-                 os.makedirs(self.db_dir, exist_ok=True)
-                 logger.info(f"创建数据库目录: {self.db_dir}")
-             except Exception as e:
-                 logger.error(f"创建数据库目录失败: {self.db_dir}, 错误: {str(e)}")
-                 # 尝试使用当前目录
-                 self.db_dir = os.path.join(os.getcwd(), 'data', 'db')
-                 if not os.path.exists(self.db_dir):
-                     os.makedirs(self.db_dir, exist_ok=True)
-                 self.main_db_path = os.path.join(self.db_dir, 'finance_news.db')
-                 logger.info(f"使用备用数据库目录: {self.db_dir}")
-     
-     def get_connection(self, db_path=None):
-         """
-         获取数据库连接
-         
-         Args:
-             db_path: 数据库路径，如果为None则使用主数据库
+             source: 数据源名称，如果为None则获取所有备份
              
          Returns:
-             sqlite3.Connection: 数据库连接
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         # 确保数据库目录存在
-         os.makedirs(os.path.dirname(db_path), exist_ok=True)
-         
-         # 连接数据库
-         conn = sqlite3.connect(db_path)
-         
-         # 启用外键约束
-         conn.execute('PRAGMA foreign_keys = ON')
-         
-         # 设置行工厂，返回字典
-         conn.row_factory = sqlite3.Row
-         
-         logger.debug(f"已连接到数据库: {db_path}")
-         return conn
-     
-     def init_db(self, conn):
-         """
-         初始化数据库，创建表
-         
-         Args:
-             conn: 数据库连接
-         """
-         try:
-             cursor = conn.cursor()
-             
-             # 创建新闻表
-             cursor.execute('''
-                 CREATE TABLE IF NOT EXISTS news (
-                     id TEXT PRIMARY KEY,
-                     title TEXT NOT NULL,
-                     content TEXT,
-                     content_html TEXT,
-                     pub_time TEXT,
-                     author TEXT,
-                     source TEXT,
-                     url TEXT,
-                     keywords TEXT,
-                     sentiment REAL,
-                     crawl_time TEXT,
-                     category TEXT,
-                     images TEXT,
-                     related_stocks TEXT
-                 )
-             ''')
-             
-             # 提交事务
-             conn.commit()
-             
-             logger.debug("已初始化数据库表")
-             
-         except Exception as e:
-             logger.error(f"初始化数据库失败: {str(e)}")
-             conn.rollback()
-             raise
-     
-     def save_news(self, news_data, conn=None, db_path=None):
-         """
-         保存新闻数据到数据库
-         
-         Args:
-             news_data: 新闻数据字典
-             conn: 数据库连接，如果为None则创建新连接
-             db_path: 数据库路径，如果为None则使用默认路径
-             
-         Returns:
-             bool: 是否保存成功
-         """
-         # 如果没有提供数据库路径，使用当前对象的db_path属性（如果存在）
-         if db_path is None:
-             if hasattr(self, 'db_path') and self.db_path:
-                 db_path = self.db_path
+             list: 备份文件信息列表，每项包含 {path, timestamp, size}
+         """
+         # 查找所有备份文件
+         backup_files = glob.glob(os.path.join(self.backups_dir, '*.db.bak'))
+         
+         # 筛选特定来源的备份
+         filtered_backups = []
+         for backup_file in backup_files:
+             file_name = os.path.basename(backup_file)
+             
+             # 解析文件名
+             if source:
+                 # 检查文件名是否以指定来源开头
+                 if file_name.startswith(f"{source}_"):
+                     # 获取备份信息
+                     timestamp_str = file_name.split('_')[1].split('.')[0]
+                     try:
+                         timestamp = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S')
+                         size = os.path.getsize(backup_file)
+                         filtered_backups.append({
+                             'path': backup_file,
+                             'timestamp': timestamp,
+                             'size': size,
+                             'source': source
+                         })
+                     except ValueError:
+                         logger.warning(f"无法解析备份文件的时间戳: {file_name}")
              else:
-                 db_path = self.main_db_path
-         
-         logger.debug(f"使用数据库路径保存新闻: {db_path}")
-         
-         # 如果没有提供连接，创建新连接
-         close_conn = False
-         if conn is None:
-             conn = self.get_connection(db_path)
-             close_conn = True
-             
-             # 确保数据库已初始化
-             try:
-                 self.init_db(conn)
-             except Exception as e:
-                 logger.error(f"初始化数据库失败: {str(e)}")
-         
-         try:
-             cursor = conn.cursor()
-             
-             # 检查新闻ID是否已存在
-             cursor.execute("SELECT id FROM news WHERE id = ?", (news_data['id'],))
-             if cursor.fetchone():
-                 logger.debug(f"新闻已存在: {news_data.get('title', 'unknown')}")
-                 return False
-             
-             # 确保所有字段都存在
-             fields = ['id', 'title', 'content', 'pub_time', 'author', 'source', 'url', 
-                       'keywords', 'sentiment', 'crawl_time', 'category', 'images', 'related_stocks']
-             
-             news_dict = {}
-             for field in fields:
-                 if field in news_data:
-                     # 如果是列表类型，转换为逗号分隔的字符串
-                     if isinstance(news_data[field], list):
-                         news_dict[field] = ','.join(news_data[field])
-                     else:
-                         news_dict[field] = news_data[field]
-                 else:
-                     # 对于缺失的字段设置默认值
-                     if field == 'crawl_time':
-                         news_dict[field] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
-                     elif field == 'sentiment':
-                         news_dict[field] = 0.5
-                     else:
-                         news_dict[field] = ''
-             
-             # 构建INSERT语句
-             fields_str = ', '.join(fields)
-             placeholders = ', '.join(['?' for _ in fields])
-             sql = f"INSERT INTO news ({fields_str}) VALUES ({placeholders})"
-             
-             # 执行INSERT语句
-             cursor.execute(sql, tuple(news_dict[field] for field in fields))
-             
-             # 提交事务
-             conn.commit()
-             
-             logger.debug(f"成功保存新闻: {news_dict.get('title', 'unknown')}")
-             return True
-             
-         except Exception as e:
-             logger.error(f"保存新闻失败: {str(e)}")
-             conn.rollback()
-             return False
-             
-         finally:
-             # 如果我们创建了连接，则关闭它
-             if close_conn:
-                 conn.close()
-     
-     def merge_db(self, source_db_path, target_db_path=None):
-         """
-         合并数据库，将源数据库的数据合并到目标数据库
+                 # 所有备份
+                 parts = file_name.split('_')
+                 if len(parts) >= 2:
+                     source_name = parts[0]
+                     timestamp_str = parts[1].split('.')[0]
+                     try:
+                         timestamp = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S')
+                         size = os.path.getsize(backup_file)
+                         filtered_backups.append({
+                             'path': backup_file,
+                             'timestamp': timestamp,
+                             'size': size,
+                             'source': source_name
+                         })
+                     except ValueError:
+                         logger.warning(f"无法解析备份文件的时间戳: {file_name}")
+         
+         # 按时间戳排序，最新的在前
+         filtered_backups.sort(key=lambda x: x['timestamp'], reverse=True)
+         return filtered_backups
+     
+     def sync_to_main_db(self, source):
+         """
+         将指定来源的数据同步到主数据库
          
          Args:
-             source_db_path: 源数据库路径
-             target_db_path: 目标数据库路径，如果为None则使用主数据库
+             source: 数据源名称
              
          Returns:
-             int: 合并的新闻数量
-         """
-         if target_db_path is None:
-             target_db_path = self.main_db_path
-         
-         # 连接源数据库和目标数据库
-         source_conn = self.get_connection(source_db_path)
-         target_conn = self.get_connection(target_db_path)
-         
-         try:
-             # 获取源数据库中的所有新闻
-             source_cursor = source_conn.cursor()
-             source_cursor.execute('SELECT * FROM news')
-             news_list = source_cursor.fetchall()
-             
-             # 获取目标数据库中的所有新闻ID
-             target_cursor = target_conn.cursor()
-             target_cursor.execute('SELECT id FROM news')
-             existing_ids = set(row[0] for row in target_cursor.fetchall())
-             
-             # 合并新闻
-             merged_count = 0
-             for news in news_list:
-                 # 如果新闻ID已存在，则跳过
-                 if news['id'] in existing_ids:
-                     continue
-                 
-                 # 插入新闻
-                 target_conn.execute('''
-                     INSERT INTO news (
-                         id, title, content, pub_time, author, source, url, 
-                         keywords, sentiment, crawl_time, category, images, related_stocks
-                     ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
-                 ''', (
-                     news['id'], news['title'], news['content'], news['pub_time'], 
-                     news['author'], news['source'], news['url'], news['keywords'], 
-                     news['sentiment'], news['crawl_time'], news['category'], 
-                     news['images'], news['related_stocks']
-                 ))
-                 
-                 merged_count += 1
-             
-             # 提交事务
-             target_conn.commit()
-             
-             logger.info(f"数据库合并完成，合并 {merged_count} 条新闻")
-             return merged_count
-             
-         except Exception as e:
-             logger.error("数据库合并失败", e)
+             int: 同步的新闻数量
+         """
+         if source not in self.source_db_map:
+             logger.error(f"未找到来源 '{source}' 的数据库")
              return 0
-         finally:
-             source_conn.close()
-             target_conn.close()
-     
-     def backup_db(self, db_path=None, backup_dir='backup'):
-         """
-         备份数据库
-         
-         Args:
-             db_path: 数据库路径，如果为None则使用主数据库
-             backup_dir: 备份目录
-         
-         Returns:
-             str: 备份文件路径
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         # 确保备份目录存在
-         os.makedirs(backup_dir, exist_ok=True)
-         
-         # 生成备份文件名
-         db_name = os.path.basename(db_path)
-         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
-         backup_file = os.path.join(backup_dir, f'{db_name}_{timestamp}.bak')
-         
-         try:
-             # 复制数据库文件
-             shutil.copy2(db_path, backup_file)
-             
-             logger.info(f"数据库备份完成: {backup_file}")
-             return backup_file
-             
-         except Exception as e:
-             logger.error(f"数据库备份失败: {db_path}", e)
-             return None
-     
-     def restore_db(self, backup_file, db_path=None):
-         """
-         恢复数据库
-         
-         Args:
-             backup_file: 备份文件路径
-             db_path: 数据库路径，如果为None则使用主数据库
-         
-         Returns:
-             bool: 是否恢复成功
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         try:
-             # 复制备份文件到数据库文件
-             shutil.copy2(backup_file, db_path)
-             
-             logger.info(f"数据库恢复完成: {db_path}")
-             return True
-             
-         except Exception as e:
-             logger.error(f"数据库恢复失败: {backup_file} -> {db_path}", e)
-             return False
-     
-     def export_data(self, db_path=None, output_file=None, format='json'):
-         """
-         导出数据
-         
-         Args:
-             db_path: 数据库路径，如果为None则使用主数据库
-             output_file: 输出文件路径，如果为None则使用默认路径
-             format: 导出格式，支持json和csv
-         
-         Returns:
-             str: 输出文件路径
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         if output_file is None:
-             # 生成输出文件名
-             timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
-             output_dir = 'data'
-             os.makedirs(output_dir, exist_ok=True)
-             output_file = os.path.join(output_dir, f'export_{timestamp}.{format}')
-         
-         # 连接数据库
-         conn = self.get_connection(db_path)
-         
-         try:
-             # 获取所有新闻
-             cursor = conn.cursor()
-             cursor.execute('SELECT * FROM news')
-             news_list = cursor.fetchall()
-             
-             # 将查询结果转换为字典列表
-             columns = [column[0] for column in cursor.description]
-             data = []
-             for row in news_list:
-                 news_dict = dict(zip(columns, row))
-                 data.append(news_dict)
-             
-             # 导出数据
-             if format == 'json':
-                 with open(output_file, 'w', encoding='utf-8') as f:
-                     json.dump(data, f, ensure_ascii=False, indent=4)
-             elif format == 'csv':
-                 import csv
-                 with open(output_file, 'w', encoding='utf-8', newline='') as f:
-                     writer = csv.DictWriter(f, fieldnames=columns)
-                     writer.writeheader()
-                     writer.writerows(data)
+             
+         source_db_path = self.source_db_map[source]
+         
+         try:
+             # 获取来源数据库的会话
+             if source_db_path not in self.sessions:
+                 self.engines[source_db_path] = create_engine(f'sqlite:///{source_db_path}')
+                 Base.metadata.create_all(self.engines[source_db_path])
+                 Session = sessionmaker(bind=self.engines[source_db_path])
+                 self.sessions[source_db_path] = Session()
+                 
+             source_session = self.sessions[source_db_path]
+             
+             # 查询来源数据库中的所有新闻
+             news_items = source_session.query(News).all()
+             sync_count = 0
+             
+             # 遍历新闻并添加到主数据库
+             for news in news_items:
+                 # 检查主数据库中是否已存在该新闻
+                 if not self.session.query(News).filter(News.url == news.url).first():
+                     # 创建新闻数据字典
+                     news_data = {
+                         'title': news.title,
+                         'content': news.content,
+                         'url': news.url,
+                         'source': news.source,
+                         'category': news.category,
+                         'pub_time': news.pub_time.strftime('%Y-%m-%d %H:%M:%S') if news.pub_time else None,
+                         'crawl_time': news.crawl_time.strftime('%Y-%m-%d %H:%M:%S') if news.crawl_time else None,
+                         'keywords': json.loads(news.keywords) if news.keywords else []
+                     }
+                     
+                     # 添加到主数据库
+                     news_obj = News(
+                         title=news_data['title'],
+                         content=news_data['content'],
+                         url=news_data['url'],
+                         source=news_data['source'],
+                         category=news_data['category'],
+                         pub_time=news.pub_time,
+                         crawl_time=news.crawl_time,
+                         keywords=news.keywords
+                     )
+                     
+                     self.session.add(news_obj)
+                     sync_count += 1
+             
+             # 提交更改
+             if sync_count > 0:
+                 self.session.commit()
+                 logger.info(f"成功从 '{source}' 同步 {sync_count} 条新闻到主数据库")
              else:
-                 raise ValueError(f"不支持的导出格式: {format}")
+                 logger.info(f"没有新闻需要从 '{source}' 同步到主数据库")
                  
-             logger.info(f"数据导出完成: {output_file}")
-             return output_file
+             return sync_count
              
          except Exception as e:
-             logger.error(f"数据导出失败: {db_path} -> {output_file}", e)
-             return None
-         finally:
-             conn.close()
-     
-     def import_data(self, input_file, db_path=None, format='json'):
-         """
-         导入数据
-         
-         Args:
-             input_file: 输入文件路径
-             db_path: 数据库路径，如果为None则使用主数据库
-             format: 导入格式，支持json和csv
-         
-         Returns:
-             int: 导入的新闻数量
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         # 连接数据库
-         conn = self.get_connection(db_path)
-         
-         try:
-             # 初始化数据库
-             self.init_db(conn)
-             
-             # 获取已存在的新闻ID
-             cursor = conn.cursor()
-             cursor.execute('SELECT id FROM news')
-             existing_ids = set(row[0] for row in cursor.fetchall())
-             
-             # 导入数据
-             data = []
-             if format == 'json':
-                 with open(input_file, 'r', encoding='utf-8') as f:
-                     data = json.load(f)
-             elif format == 'csv':
-                 import csv
-                 with open(input_file, 'r', encoding='utf-8', newline='') as f:
-                     reader = csv.DictReader(f)
-                     data = list(reader)
-             else:
-                 raise ValueError(f"不支持的导入格式: {format}")
-             
-             # 插入新闻
-             imported_count = 0
-             for news in data:
-                 # 如果新闻ID已存在，则跳过
-                 if news['id'] in existing_ids:
-                     continue
-                 
-                 # 插入新闻
-                 conn.execute('''
-                     INSERT INTO news (
-                         id, title, content, pub_time, author, source, url, 
-                         keywords, sentiment, crawl_time, category, images, related_stocks
-                     ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
-                 ''', (
-                     news['id'], news['title'], news['content'], news['pub_time'], 
-                     news['author'], news['source'], news['url'], news['keywords'], 
-                     news['sentiment'], news['crawl_time'], news['category'], 
-                     news['images'], news['related_stocks']
-                 ))
-                 
-                 imported_count += 1
-             
-             # 提交事务
-             conn.commit()
-             
-             logger.info(f"数据导入完成，导入 {imported_count} 条新闻")
-             return imported_count
-             
-         except Exception as e:
-             logger.error(f"数据导入失败: {input_file} -> {db_path}", e)
+             logger.error(f"同步数据库失败: {str(e)}")
+             if hasattr(self, 'session') and self.session:
+                 self.session.rollback()
              return 0
-         finally:
-             conn.close()
-     
-     def get_db_stats(self, db_path=None):
-         """
-         获取数据库统计信息
-         
-         Args:
-             db_path: 数据库路径，如果为None则使用主数据库
-         
-         Returns:
-             dict: 数据库统计信息
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         # 连接数据库
-         conn = self.get_connection(db_path)
-         
-         try:
-             # 获取统计信息
-             cursor = conn.cursor()
-             
-             # 总新闻数
-             cursor.execute('SELECT COUNT(*) FROM news')
-             total_news = cursor.fetchone()[0]
-             
-             # 来源统计
-             cursor.execute('SELECT source, COUNT(*) FROM news GROUP BY source')
-             sources = {row[0]: row[1] for row in cursor.fetchall()}
-             
-             # 分类统计
-             cursor.execute('SELECT category, COUNT(*) FROM news GROUP BY category')
-             categories = {row[0]: row[1] for row in cursor.fetchall()}
-             
-             # 日期范围
-             cursor.execute('SELECT MIN(pub_time), MAX(pub_time) FROM news')
-             date_range = cursor.fetchone()
-             
-             # 文件大小
-             file_size = os.path.getsize(db_path) / (1024 * 1024)  # MB
-             
-             # 统计信息
-             stats = {
-                 'db_path': db_path,
-                 'total_news': total_news,
-                 'sources': sources,
-                 'categories': categories,
-                 'date_range': {
-                     'min': date_range[0],
-                     'max': date_range[1]
-                 },
-                 'file_size': round(file_size, 2)  # MB
-             }
-             
-             return stats
-             
-         except Exception as e:
-             logger.error(f"获取数据库统计信息失败: {db_path}", e)
-             return None
-         finally:
-             conn.close()
-     
-     def list_databases(self):
-         """
-         列出所有数据库
-         
-         Returns:
-             list: 数据库列表
-         """
-         try:
-             # 获取数据库目录中的所有数据库文件
-             db_files = []
-             for file in os.listdir(self.db_dir):
-                 if file.endswith('.db'):
-                     db_files.append(os.path.join(self.db_dir, file))
-             
-             return db_files
-             
-         except Exception as e:
-             logger.error("列出数据库失败", e)
-             return []
-     
-     def vacuum_db(self, db_path=None):
-         """
-         压缩数据库
-         
-         Args:
-             db_path: 数据库路径，如果为None则使用主数据库
-         
-         Returns:
-             bool: 是否压缩成功
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         # 连接数据库
-         conn = self.get_connection(db_path)
-         
-         try:
-             # 压缩数据库
-             conn.execute('VACUUM')
-             
-             logger.info(f"数据库压缩完成: {db_path}")
-             return True
-             
-         except Exception as e:
-             logger.error(f"数据库压缩失败: {db_path}", e)
-             return False
-         finally:
-             conn.close()
-     
-     def delete_news(self, news_id, db_path=None):
-         """
-         删除新闻
-         
-         Args:
-             news_id: 新闻ID
-             db_path: 数据库路径，如果为None则使用主数据库
-         
-         Returns:
-             bool: 是否删除成功
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         # 连接数据库
-         conn = self.get_connection(db_path)
-         
-         try:
-             # 删除新闻
-             cursor = conn.cursor()
-             cursor.execute('DELETE FROM news WHERE id = ?', (news_id,))
-             
-             # 提交事务
-             conn.commit()
-             
-             # 检查是否删除成功
-             if cursor.rowcount > 0:
-                 logger.info(f"删除新闻成功: {news_id}")
-                 return True
-             else:
-                 logger.warning(f"删除新闻失败，新闻不存在: {news_id}")
-                 return False
-             
-         except Exception as e:
-             logger.error(f"删除新闻失败: {news_id}", e)
-             return False
-         finally:
-             conn.close()
-     
-     def search_news(self, keyword, db_path=None):
-         """
-         搜索新闻
-         
-         Args:
-             keyword: 关键词
-             db_path: 数据库路径，如果为None则使用主数据库
-         
-         Returns:
-             list: 新闻列表
-         """
-         if db_path is None:
-             db_path = self.main_db_path
-         
-         # 连接数据库
-         conn = self.get_connection(db_path)
-         
-         try:
-             # 搜索新闻
-             cursor = conn.cursor()
-             cursor.execute('''
-                 SELECT * FROM news 
-                 WHERE title LIKE ? OR content LIKE ? OR keywords LIKE ? 
-                 ORDER BY pub_time DESC
-             ''', (f'%{keyword}%', f'%{keyword}%', f'%{keyword}%'))
-             
-             # 获取搜索结果
-             news_list = cursor.fetchall()
-             
-             # 将查询结果转换为字典列表
-             columns = [column[0] for column in cursor.description]
-             result = []
-             for row in news_list:
-                 news_dict = dict(zip(columns, row))
-                 result.append(news_dict)
-             
-             return result
-             
-         except Exception as e:
-             logger.error(f"搜索新闻失败: {keyword}", e)
-             return []
-         finally:
-             conn.close()
-     
-     def close_connection(self, conn):
-         """
-         关闭数据库连接
-         
-         Args:
-             conn: 数据库连接
-         """
-         if conn:
-             conn.close()
-             logger.debug("数据库连接已关闭") 
+ 
+     # 确保其余方法兼容新的多数据库结构... 
```

---

<details>
            <summary>Listed directory app/db • **4** results</summary>
        
| Name |
|-------|
| 📁 `__pycache__` |
| 📄 `sqlite_manager.py` |
| 📄 `user_preferences_db.py` |
| 📄 `__init__.py` |

</details>

---

```diff
  import sqlite3
  import logging
+ import shutil
  from datetime import datetime, timedelta
  
              os.makedirs(db_dir)
          
+         # 创建数据库备份目录
+         self.backups_dir = os.path.join(os.path.dirname(db_path), 'backups')
+         if not os.path.exists(self.backups_dir):
+             os.makedirs(self.backups_dir, exist_ok=True)
+ 
+         # 创建数据库源目录
+         self.sources_dir = os.path.join(os.path.dirname(db_path), 'sources')
+         if not os.path.exists(self.sources_dir):
+             os.makedirs(self.sources_dir, exist_ok=True)
+         
          # 创建数据库连接
          self.conn = self.connect()
          
          # 创建表
          self.create_tables()
+         
+         # 管理多个数据库连接
+         self.connections = {db_path: self.conn}
      
      def __enter__(self):
              raise
      
-     def get_connection(self):
+     def get_connection(self, db_path=None):
          """
          获取数据库连接，如果连接不存在则创建新连接
+         
+         Args:
+             db_path: 要连接的数据库路径，如果为None则使用默认数据库
              
          Returns:
              sqlite3.Connection: 数据库连接
          """
-         if not self.conn:
-             self.conn = self.connect()
-         return self.conn
+         if db_path is None:
+             db_path = self.db_path
+             
+         if db_path not in self.connections or self.connections[db_path] is None:
+             # 确保目录存在
+             db_dir = os.path.dirname(db_path)
+             if db_dir and not os.path.exists(db_dir):
+                 os.makedirs(db_dir, exist_ok=True)
+                 
+             # 创建新连接
+             conn = sqlite3.connect(db_path, timeout=30.0)
+             conn.execute("PRAGMA foreign_keys = ON")
+             conn.execute("PRAGMA journal_mode = WAL")
+             conn.row_factory = sqlite3.Row
+             
+             # 确保表结构存在
+             self.create_tables_for_connection(conn)
+             
+             # 保存连接
+             self.connections[db_path] = conn
+         
+         return self.connections[db_path]
      
      def create_tables(self):
          """创建数据库表"""
+         self.create_tables_for_connection(self.conn)
+     
+     def create_tables_for_connection(self, conn):
+         """
+         为指定连接创建数据库表
+         
+         Args:
+             conn: 数据库连接
+         """
          try:
-             cursor = self.conn.cursor()
+             cursor = conn.cursor()
              
              # 创建新闻表
              ''')
              
-             self.conn.commit()
+             conn.commit()
              logging.debug("数据库表创建成功")
              
              # 检查并升级表结构
-             self.upgrade_tables()
+             self.upgrade_tables(conn)
          except sqlite3.Error as e:
              logging.error(f"创建数据库表失败: {str(e)}")
-             self.conn.rollback()
+             conn.rollback()
              raise
      
-     def upgrade_tables(self):
-         """升级数据库表结构，添加缺失的列"""
+     def upgrade_tables(self, conn=None):
+         """
+         升级数据库表结构，添加缺失的列
+         
+         Args:
+             conn: 数据库连接，如果为None则使用默认连接
+         """
+         if conn is None:
+             conn = self.conn
+             
          try:
-             cursor = self.conn.cursor()
+             cursor = conn.cursor()
              
              # 获取news表的列信息
                  cursor.execute("ALTER TABLE news ADD COLUMN classification TEXT")
              
-             self.conn.commit()
+             conn.commit()
              logging.debug("数据库表升级完成")
          except sqlite3.Error as e:
              logging.error(f"升级数据库表失败: {str(e)}")
-             self.conn.rollback()
+             conn.rollback()
  
-     def save_news(self, news_data):
+     def save_news(self, news_data, db_path=None):
          """
          保存新闻数据
          
          Args:
              news_data: 新闻数据字典
+             db_path: 数据库路径，如果为None则使用默认数据库
              
          Returns:
              bool: 是否保存成功
          """
+         # 获取连接
+         conn = self.get_connection(db_path)
+         
          # 检查并确保标题不为空
          if 'title' not in news_data or not news_data['title']:
          while retry_count < max_retries:
              try:
-                 cursor = self.conn.cursor()
+                 cursor = conn.cursor()
                  
                  # 检查新闻是否已存在
                      return False
                  
-                 # 处理关键词（如果有）
-                 keywords_str = ""
-                 if 'keywords' in news_data and news_data['keywords']:
-                     if isinstance(news_data['keywords'], list):
-                         keywords_str = ",".join(news_data['keywords'])
-                     else:
-                         keywords_str = str(news_data['keywords'])
-                 
-                 # 处理图片（如果有）
-                 images_str = ""
-                 if 'images' in news_data and news_data['images']:
-                     if isinstance(news_data['images'], list):
-                         # 检查列表元素是否为字典类型
-                         if news_data['images'] and isinstance(news_data['images'][0], dict):
-                             # 如果是字典列表，提取URL字段
-                             image_urls = []
-                             for img in news_data['images']:
-                                 if isinstance(img, dict) and 'url' in img:
-                                     image_urls.append(img['url'])
-                                 elif isinstance(img, dict) and 'src' in img:
-                                     image_urls.append(img['src'])
-                                 elif isinstance(img, dict) and 'original_url' in img:
-                                     image_urls.append(img['original_url'])
-                             
-                             # 过滤掉空URL
-                             image_urls = [url for url in image_urls if url]
-                             images_str = ",".join(image_urls)
-                         else:
-                             # 如果列表元素是字符串，直接连接
-                             valid_images = [str(img) for img in news_data['images'] if img]
-                             images_str = ",".join(valid_images)
-                     else:
-                         # 如果不是列表，尝试解析字符串
-                         images_str = str(news_data['images'])
-                 
-                 # 处理相关股票（如果有）
-                 related_stocks_str = ""
-                 if 'related_stocks' in news_data and news_data['related_stocks']:
-                     if isinstance(news_data['related_stocks'], list):
-                         related_stocks_str = ",".join(news_data['related_stocks'])
-                     else:
-                         related_stocks_str = str(news_data['related_stocks'])
-                 
-                 # 处理HTML内容（如果有）
-                 content_html = news_data.get('content_html', '')
-                 
-                 # 设置爬取时间
-                 crawl_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
-                 if 'crawl_time' in news_data and news_data['crawl_time']:
-                     crawl_time = news_data['crawl_time']
-                 
-                 try:
-                     # 使用事务保护数据一致性
-                     self.conn.execute("BEGIN")
-                     
-                     # 插入新闻数据
-                     cursor.execute("""
-                         INSERT INTO news (
-                             id, title, content, content_html, pub_time, author, source, url, 
-                             keywords, images, related_stocks, sentiment, classification, category, crawl_time
-                         ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
-                     """, (
-                         news_data['id'], 
-                         news_data['title'], 
-                         news_data.get('content', ''), 
-                         content_html,
-                         news_data.get('pub_time', ''), 
-                         news_data.get('author', ''), 
-                         news_data.get('source', ''), 
-                         news_data.get('url', ''), 
-                         keywords_str, 
-                         images_str, 
-                         related_stocks_str, 
-                         news_data.get('sentiment', ''), 
-                         news_data.get('classification', ''), 
-                         news_data.get('category', ''), 
-                         crawl_time
-                     ))
-                     
-                     # 处理关键词
-                     if 'keywords' in news_data and news_data['keywords']:
-                         keywords = news_data['keywords']
-                         if isinstance(keywords, str):
-                             keywords = keywords.split(',')
-                         
-                         for keyword in keywords:
-                             keyword = keyword.strip()
-                             if not keyword:
-                                 continue
-                             
-                             # 检查关键词是否已存在
-                             cursor.execute("SELECT id, count FROM keywords WHERE keyword = ?", (keyword,))
-                             row = cursor.fetchone()
-                             
-                             if row:
-                                 # 更新已存在的关键词
-                                 keyword_id = row[0]
-                                 count = row[1] + 1
-                                 cursor.execute(
-                                     "UPDATE keywords SET count = ?, last_updated = ? WHERE id = ?", 
-                                     (count, crawl_time, keyword_id)
-                                 )
-                             else:
-                                 # 插入新关键词
-                                 cursor.execute(
-                                     "INSERT INTO keywords (keyword, count, last_updated) VALUES (?, ?, ?)", 
-                                     (keyword, 1, crawl_time)
-                                 )
-                                 keyword_id = cursor.lastrowid
-                             
-                             # 插入新闻-关键词关系
-                             cursor.execute(
-                                 "INSERT OR IGNORE INTO news_keywords (news_id, keyword_id) VALUES (?, ?)", 
-                                 (news_data['id'], keyword_id)
-                             )
-                     
-                     # 处理相关股票
-                     if 'related_stocks' in news_data and news_data['related_stocks']:
-                         stocks = news_data['related_stocks']
-                         if isinstance(stocks, str):
-                             stocks = stocks.split(',')
-                         
-                         for stock in stocks:
-                             stock = stock.strip()
-                             if not stock:
-                                 continue
-                             
-                             # 解析股票代码和名称
-                             parts = stock.split(':')
-                             if len(parts) == 2:
-                                 code, name = parts
-                             else:
-                                 code = stock
-                                 name = stock
-                             
-                             # 检查股票是否已存在
-                             cursor.execute("SELECT code, count FROM stocks WHERE code = ?", (code,))
-                             row = cursor.fetchone()
-                             
-                             if row:
-                                 # 更新已存在的股票
-                                 count = row[1] + 1
-                                 cursor.execute(
-                                     "UPDATE stocks SET count = ?, last_updated = ? WHERE code = ?", 
-                                     (count, crawl_time, code)
-                                 )
-                             else:
-                                 # 插入新股票
-                                 cursor.execute(
-                                     "INSERT INTO stocks (code, name, count, last_updated) VALUES (?, ?, ?, ?)", 
-                                     (code, name, 1, crawl_time)
-                                 )
-                             
-                             # 插入新闻-股票关系
-                             cursor.execute(
-                                 "INSERT OR IGNORE INTO news_stocks (news_id, stock_code) VALUES (?, ?)", 
-                                 (news_data['id'], code)
-                             )
-                     
-                     # 提交事务
-                     self.conn.commit()
-                     logging.info(f"新闻保存成功: {news_data['title']}")
-                     return True
-                     
-                 except sqlite3.Error as e:
-                     # 回滚事务
-                     self.conn.rollback()
-                     raise e
-                 
-             except sqlite3.OperationalError as e:
-                 # 捕获数据库锁定错误
-                 if "database is locked" in str(e):
-                     retry_count += 1
-                     if retry_count < max_retries:
-                         # 使用指数退避策略
-                         wait_time = retry_delay * (2 ** (retry_count - 1))
-                         logging.warning(f"数据库锁定，等待 {wait_time:.2f} 秒后重试 ({retry_count}/{max_retries})")
-                         import time
-                         time.sleep(wait_time)
-                         continue
-                 
-                 # 最后一次尝试失败或其他错误
-                 logging.error(f"保存新闻失败: {news_data.get('title', '未知标题')}, 错误: {str(e)}")
-                 return False
+                 # 调用实际的插入方法
+                 inserted = self.insert_news(news_data, conn)
+                 if inserted:
+                     # 保存到来源数据库
+                     if 'source' in news_data and news_data['source']:
+                         self.save_to_source_db(news_data)
+                 
+                 return inserted
                  
              except sqlite3.Error as e:
-                 logging.error(f"保存新闻失败: {news_data.get('title', '未知标题')}, 错误: {str(e)}")
-                 return False
-                 
-             except Exception as e:
-                 logging.error(f"保存新闻失败: {news_data.get('title', '未知标题')}, 错误: {str(e)}")
+                 logging.error(f"保存新闻失败: {str(e)}")
+                 retry_count += 1
+                 if retry_count < max_retries:
+                     import time
+                     time.sleep(retry_delay)
+                     retry_delay *= 2  # 指数退避
+                 else:
                      return False
      
-     # 添加insert_news方法作为save_news的别名，兼容爬虫接口
-     def insert_news(self, news_data):
+     def save_to_source_db(self, news_data):
          """
-         插入新闻数据（save_news的别名）
+         将新闻保存到对应来源的专用数据库
          
          Args:
              bool: 是否保存成功
          """
-         return self.save_news(news_data)
-     
-     def get_news(self, news_id):
-         """
-         获取指定ID的新闻
-         
-         Args:
-             news_id: 新闻ID
-             
-         Returns:
-             dict: 新闻数据
-         """
-         try:
-             cursor = self.conn.cursor()
-             cursor.execute("SELECT * FROM news WHERE id = ?", (news_id,))
-             row = cursor.fetchone()
-             
-             if not row:
-                 return None
-             
-             # 将行转换为字典
-             news = dict(row)
-             
-             # 获取关键词
-             cursor.execute("""
-                 SELECT k.keyword FROM keywords k
-                 JOIN news_keywords nk ON k.id = nk.keyword_id
-                 WHERE nk.news_id = ?
-             """, (news_id,))
-             keywords = [row[0] for row in cursor.fetchall()]
-             news['keywords'] = keywords
-             
-             # 获取相关股票
-             cursor.execute("""
-                 SELECT s.code, s.name FROM stocks s
-                 JOIN news_stocks ns ON s.code = ns.stock_code
-                 WHERE ns.news_id = ?
-             """, (news_id,))
-             stocks = [f"{row[0]}:{row[1]}" for row in cursor.fetchall()]
-             news['related_stocks'] = stocks
-             
-             return news
+         if 'source' not in news_data or not news_data['source']:
+             return False
+             
+         # 规范化来源名称作为文件名
+         source_name = news_data['source'].replace(' ', '_').replace('/', '_')
+         source_db_path = os.path.join(self.sources_dir, f"{source_name}.db")
+         
+         try:
+             # 获取或创建来源数据库连接
+             conn = self.get_connection(source_db_path)
+             
+             # 检查新闻是否已存在
+             cursor = conn.cursor()
+             cursor.execute("SELECT id FROM news WHERE id = ?", (news_data['id'],))
+             if cursor.fetchone():
+                 logging.debug(f"新闻已存在于来源数据库 {source_name}: {news_data['title']}")
+                 return False
+             
+             # 插入新闻
+             return self.insert_news(news_data, conn)
              
          except sqlite3.Error as e:
-             logging.error(f"获取新闻失败: {news_id}, 错误: {str(e)}")
-             return None
+             logging.error(f"保存新闻到来源数据库失败: {str(e)}")
+             return False
      
-     def get_news_list(self, limit=100, offset=0, category=None, start_date=None, end_date=None):
+     def backup_database(self, source=None):
          """
-         获取新闻列表
+         备份数据库
          
          Args:
-             limit: 限制条数
-             offset: 偏移量
-             category: 分类
-             start_date: 开始日期
-             end_date: 结束日期
+             source: 要备份的来源名称，如果为None则备份主数据库
              
          Returns:
-             list: 新闻列表
-         """
-         try:
-             cursor = self.conn.cursor()
-             
-             # 构建查询条件
-             conditions = []
-             params = []
-             
-             if category:
-                 conditions.append("classification = ?")
-                 params.append(category)
-             
-             if start_date:
-                 conditions.append("pub_time >= ?")
-                 params.append(start_date)
-             
-             if end_date:
-                 conditions.append("pub_time <= ?")
-                 params.append(end_date)
-             
-             # 构建SQL语句
-             sql = "SELECT * FROM news"
-             if conditions:
-                 sql += " WHERE " + " AND ".join(conditions)
-             
-             sql += " ORDER BY pub_time DESC LIMIT ? OFFSET ?"
-             params.extend([limit, offset])
-             
-             # 执行查询
-             cursor.execute(sql, params)
-             rows = cursor.fetchall()
-             
-             # 将行转换为字典列表
-             news_list = []
-             for row in rows:
-                 news = dict(row)
-                 
-                 # 处理关键词
-                 if news['keywords']:
-                     news['keywords'] = news['keywords'].split(',')
-                 else:
-                     news['keywords'] = []
+             str: 备份文件路径
+         """
+         # 确定要备份的数据库路径
+         if source:
+             # 规范化来源名称
+             source_name = source.replace(' ', '_').replace('/', '_')
+             db_path = os.path.join(self.sources_dir, f"{source_name}.db")
+             if not os.path.exists(db_path):
+                 logging.error(f"来源数据库不存在: {db_path}")
+                 return None
+         else:
+             db_path = self.db_path
+         
+         # 生成备份文件名
+         timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
+         backup_filename = f"{source or 'main'}_{timestamp}.db.bak"
+         backup_path = os.path.join(self.backups_dir, backup_filename)
+         
+         try:
+             # 关闭连接以确保数据已写入磁盘
+             if db_path in self.connections:
+                 self.connections[db_path].close()
+                 del self.connections[db_path]
              
-                 # 处理图片
-                 if news['images']:
-                     news['images'] = news['images'].split(',')
-                 else:
-                     news['images'] = []
-                 
-                 # 处理相关股票
-                 if news['related_stocks']:
-                     news['related_stocks'] = news['related_stocks'].split(',')
-                 else:
-                     news['related_stocks'] = []
-                 
-                 news_list.append(news)
-             
-             return news_list
-         
-         except sqlite3.Error as e:
-             logging.error(f"获取新闻列表失败: 错误: {str(e)}")
-             return []
+             # 复制数据库文件
+             shutil.copy2(db_path, backup_path)
+             logging.info(f"成功备份数据库 {db_path} 到 {backup_path}")
+             
+             # 重新连接
+             if db_path == self.db_path:
+                 self.conn = self.connect()
+                 self.connections[db_path] = self.conn
+             
+             return backup_path
+         except Exception as e:
+             logging.error(f"备份数据库失败: {str(e)}")
+             return None
      
-     def get_news_count(self, category=None, start_date=None, end_date=None):
+     def restore_database(self, backup_path, target_source=None):
          """
-         获取新闻数量
+         从备份恢复数据库
          
          Args:
-             category: 分类
-             start_date: 开始日期
-             end_date: 结束日期
+             backup_path: 备份文件路径
+             target_source: 目标来源名称，如果为None则恢复到主数据库
              
          Returns:
-             int: 新闻数量
-         """
-         try:
-             cursor = self.conn.cursor()
-             
-             # 构建查询条件
-             conditions = []
-             params = []
-             
-             if category:
-                 conditions.append("classification = ?")
-                 params.append(category)
-             
-             if start_date:
-                 conditions.append("pub_time >= ?")
-                 params.append(start_date)
-             
-             if end_date:
-                 conditions.append("pub_time <= ?")
-                 params.append(end_date)
+             bool: 是否恢复成功
+         """
+         # 确定目标数据库路径
+         if target_source:
+             # 规范化来源名称
+             source_name = target_source.replace(' ', '_').replace('/', '_')
+             target_db_path = os.path.join(self.sources_dir, f"{source_name}.db")
+         else:
+             target_db_path = self.db_path
+         
+         try:
+             # 关闭连接
+             if target_db_path in self.connections:
+                 self.connections[target_db_path].close()
+                 del self.connections[target_db_path]
+             
+             # 复制备份文件到目标位置
+             shutil.copy2(backup_path, target_db_path)
+             logging.info(f"成功从备份 {backup_path} 恢复数据库 {target_db_path}")
              
-             # 构建SQL语句
-             sql = "SELECT COUNT(*) FROM news"
-             if conditions:
-                 sql += " WHERE " + " AND ".join(conditions)
-             
-             # 执行查询
-             cursor.execute(sql, params)
-             count = cursor.fetchone()[0]
-             
-             return count
-         
-         except sqlite3.Error as e:
-             logging.error(f"获取新闻数量失败: 错误: {str(e)}")
-             return 0
+             # 重新连接
+             if target_db_path == self.db_path:
+                 self.conn = self.connect()
+                 self.connections[target_db_path] = self.conn
+             
+             return True
+         except Exception as e:
+             logging.error(f"恢复数据库失败: {str(e)}")
+             return False
      
-     def delete_news(self, news_id):
+     def list_backups(self, source=None):
          """
-         删除指定ID的新闻
+         列出备份文件
          
          Args:
-             news_id: 新闻ID
+             source: 来源名称，如果为None则列出所有备份
              
          Returns:
-             bool: 是否删除成功
-         """
-         try:
-             cursor = self.conn.cursor()
-             cursor.execute("DELETE FROM news WHERE id = ?", (news_id,))
-             self.conn.commit()
-             return cursor.rowcount > 0
-         
-         except sqlite3.Error as e:
-             logging.error(f"删除新闻失败: {news_id}, 错误: {str(e)}")
-             self.conn.rollback()
-             return False
-     
-     def close(self):
-         """关闭数据库连接"""
-         if self.conn:
-             try:
-                 self.conn.close()
-             except sqlite3.Error as e:
-                 logging.error(f"关闭数据库连接失败: {str(e)}")
-             finally:
-                 self.conn = None
-     
-     def __del__(self):
-         """析构函数，关闭数据库连接"""
-         self.close()
-     
-     def get_news_paginated(self, page=1, per_page=20, category=None, source=None, keyword=None, sort_by='pub_time', order='desc', days=None, start_date=None, end_date=None):
+             list: 备份信息列表，每项包含 {path, timestamp, source, size}
+         """
+         import glob
+         
+         # 查找所有备份文件
+         backup_pattern = os.path.join(self.backups_dir, '*.db.bak')
+         backup_files = glob.glob(backup_pattern)
+         
+         # 解析备份信息
+         backups = []
+         for path in backup_files:
+             filename = os.path.basename(path)
+             parts = filename.split('_')
+             
+             if len(parts) >= 2:
+                 backup_source = parts[0]
+                 
+                 # 如果指定了来源，只返回匹配的备份
+                 if source and backup_source != source:
+                     continue
+                 
+                 # 解析时间戳
+                 timestamp_str = parts[1].split('.')[0]
+                 try:
+                     timestamp = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S')
+                     size = os.path.getsize(path)
+                     
+                     backups.append({
+                         'path': path,
+                         'timestamp': timestamp,
+                         'source': backup_source,
+                         'size': size
+                     })
+                 except ValueError:
+                     logging.warning(f"无法解析备份时间戳: {filename}")
+         
+         # 按时间戳排序，最新的在前
+         backups.sort(key=lambda x: x['timestamp'], reverse=True)
+         return backups
+     
+     def sync_to_main_db(self, source):
          """
-         获取分页新闻列表
+         将指定来源的数据同步到主数据库
          
          Args:
-             page: 页码
-             per_page: 每页条数
-             category: 分类
-             source: 来源
-             keyword: 关键词
-             sort_by: 排序字段
-             order: 排序顺序
-             days: 最近几天
-             start_date: 开始日期 (YYYY-MM-DD)
-             end_date: 结束日期 (YYYY-MM-DD)
+             source: 来源名称
              
          Returns:
-             tuple: (新闻列表, 总条数)
-         """
-         try:
-             cursor = self.conn.cursor()
-             
-             # 构建查询条件
-             conditions = []
-             params = []
-             
-             if category:
-                 conditions.append("(category = ? OR classification = ?)")
-                 params.extend([category, category])
-             
-             if source:
-                 conditions.append("source = ?")
-                 params.append(source)
+             int: 同步的新闻数量
+         """
+         # 规范化来源名称
+         source_name = source.replace(' ', '_').replace('/', '_')
+         source_db_path = os.path.join(self.sources_dir, f"{source_name}.db")
          
-             if keyword:
-                 # 在标题、内容和关键词中搜索
-                 conditions.append("(title LIKE ? OR content LIKE ? OR keywords LIKE ?)")
-                 keyword_param = f"%{keyword}%"
-                 params.extend([keyword_param, keyword_param, keyword_param])
-             
-             # 日期筛选逻辑
-             if start_date and end_date:  # 优先使用自定义日期范围
-                 # 确保结束日期包含当天的最后时刻 (23:59:59)
-                 end_date_with_time = f"{end_date} 23:59:59"
-                 
-                 date_condition = """
-                 (
-                     (
-                         (pub_time IS NOT NULL AND pub_time != '') AND 
-                         (
-                             date(pub_time) BETWEEN date(?) AND date(?) OR
-                             substr(pub_time, 1, 10) BETWEEN ? AND ?
-                         )
-                     ) OR (
-                         (pub_time IS NULL OR pub_time = '') AND 
-                         (
-                             date(crawl_time) BETWEEN date(?) AND date(?) OR
-                             substr(crawl_time, 1, 10) BETWEEN ? AND ?
-                         )
-                     )
-                 )
-                 """
-                 conditions.append(date_condition)
-                 params.extend([start_date, end_date_with_time, start_date, end_date, start_date, end_date_with_time, start_date, end_date])
-                 
-             elif days and days > 0:  # 使用天数筛选
-                 # 计算日期范围 - 使用日期函数确保兼容不同的日期格式
-                 start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
-                 end_date = datetime.now().strftime('%Y-%m-%d 23:59:59')
-                 
-                 date_condition = """
-                 (
-                     (
-                         (pub_time IS NOT NULL AND pub_time != '') AND 
-                         (
-                             date(pub_time) BETWEEN date(?) AND date(?) OR
-                             substr(pub_time, 1, 10) BETWEEN ? AND ?
-                         )
-                     ) OR (
-                         (pub_time IS NULL OR pub_time = '') AND 
-                         (
-                             date(crawl_time) BETWEEN date(?) AND date(?) OR
-                             substr(crawl_time, 1, 10) BETWEEN ? AND ?
-                         )
-                     )
-                 )
-                 """
-                 conditions.append(date_condition)
-                 params.extend([start_date, end_date, start_date, end_date, start_date, end_date, start_date, end_date])
-             
-             # 构建SQL语句
-             sql_count = "SELECT COUNT(*) FROM news"
-             
-             # 添加查询条件
-             if conditions:
-                 sql_where = " WHERE " + " AND ".join(conditions)
-                 sql_count += sql_where
-             
-             # 获取总条数
-             cursor.execute(sql_count, params)
-             total = cursor.fetchone()[0]
-             
-             # 构建分页查询SQL
-             sql = "SELECT * FROM news"
-             if conditions:
-                 sql += " WHERE " + " AND ".join(conditions)
-             
-             # 添加排序
-             if sort_by:
-                 if sort_by == 'pub_time':
-                     # 为了正确排序，首先尝试使用crawl_time作为备选字段
-                     sql += " ORDER BY CASE WHEN pub_time IS NULL OR pub_time = '' THEN crawl_time ELSE pub_time END"
-                 else:
-                     sql += f" ORDER BY {sort_by}"
-                 
-                 if order.lower() == 'desc':
-                     sql += " DESC"
-                 else:
-                     sql += " ASC"
-             
-             # 添加分页
-             sql += " LIMIT ? OFFSET ?"
-             offset = (page - 1) * per_page
-             params.extend([per_page, offset])
-             
-             # 执行查询前记录SQL日志
-             logging.debug(f"执行查询: {sql} 参数: {params}")
-             
-             # 执行查询
-             cursor.execute(sql, params)
-             rows = cursor.fetchall()
-             
-             # 将行转换为字典列表
-             news_list = []
-             for row in rows:
-                 news = dict(row)
-                 
-                 # 处理关键词
-                 if news.get('keywords'):
-                     news['keywords'] = news['keywords'].split(',')
-                 else:
-                     news['keywords'] = []
-                 
-                 # 处理图片
-                 if news.get('images'):
-                     news['images'] = news['images'].split(',')
-                 else:
-                     news['images'] = []
-                 
-                 # 处理相关股票
-                 if news.get('related_stocks'):
-                     news['related_stocks'] = news['related_stocks'].split(',')
-                 else:
-                     news['related_stocks'] = []
-                 
-                 # 添加情感分析结果
-                 if 'sentiment' in news and news['sentiment']:
-                     try:
-                         if isinstance(news['sentiment'], str):
-                             if news['sentiment'] in ['积极', '正面', '中性', '消极', '负面', 'positive', 'neutral', 'negative']:
-                                 news['sentiment'] = news['sentiment']
-                             else:
-                                 news['sentiment'] = float(news['sentiment'])
-                     except (ValueError, TypeError):
-                         # 如果无法转换为浮点数，保留原始字符串
-                         pass
-                 
-                 news_list.append(news)
-             
-             # 记录获取的新闻数量
-             logging.debug(f"查询结果: 共 {len(news_list)} 条新闻")
+         if not os.path.exists(source_db_path):
+             logging.error(f"来源数据库不存在: {source_db_path}")
+             return 0
+         
+         try:
+             # 获取来源数据库连接
+             source_conn = self.get_connection(source_db_path)
+             
+             # 获取主数据库连接
+             main_conn = self.conn
+             
+             # 查询来源数据库中的所有新闻
+             source_cursor = source_conn.cursor()
+             source_cursor.execute("SELECT * FROM news")
+             news_items = source_cursor.fetchall()
+             
+             # 查询主数据库中现有新闻ID和URL
+             main_cursor = main_conn.cursor()
+             main_cursor.execute("SELECT id, url FROM news")
+             existing = {row['id']: row['url'] for row in main_cursor.fetchall()}
+             existing_urls = set(existing.values())
+             
+             # 同步计数
+             sync_count = 0
+             
+             # 批量同步
+             for news in news_items:
+                 # 转换为字典
+                 news_dict = dict(news)
+                 
+                 # 检查是否已存在
+                 if news_dict['id'] in existing or news_dict['url'] in existing_urls:
+                     continue
+                 
+                 # 插入到主数据库
+                 main_cursor.execute('''
+                     INSERT INTO news (
+                         id, title, content, content_html, pub_time, author, source,
+                         url, keywords, images, related_stocks, sentiment,
+                         classification, category, crawl_time
+                     ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
+                 ''', (
+                     news_dict['id'], news_dict['title'], news_dict['content'],
+                     news_dict['content_html'], news_dict['pub_time'], news_dict['author'],
+                     news_dict['source'], news_dict['url'], news_dict['keywords'],
+                     news_dict['images'], news_dict['related_stocks'], news_dict['sentiment'],
+                     news_dict['classification'], news_dict['category'], news_dict['crawl_time']
+                 ))
+                 
+                 sync_count += 1
+             
+             # 提交事务
+             if sync_count > 0:
+                 main_conn.commit()
+                 logging.info(f"成功从 {source} 同步 {sync_count} 条新闻到主数据库")
              
-             return news_list, total
+             return sync_count
              
          except sqlite3.Error as e:
-             logging.error(f"获取分页新闻列表失败: 错误: {str(e)}")
-             return [], 0
+             logging.error(f"同步数据库失败: {str(e)}")
+             return 0
      
-     def get_categories(self):
+     def get_available_sources(self):
          """
-         获取所有分类列表
+         获取所有可用的数据源
          
          Returns:
-             list: 分类列表
+             list: 数据源列表
          """
-         try:
-             cursor = self.conn.cursor()
-             cursor.execute("SELECT DISTINCT category FROM news WHERE category IS NOT NULL AND category != ''")
-             categories = [row[0] for row in cursor.fetchall()]
-             return categories
-         except sqlite3.Error as e:
-             logging.error(f"获取分类列表失败: {str(e)}")
-             return []
-     
-     def get_sources(self):
-         """
-         获取所有来源列表
-         
-         Returns:
-             list: 来源列表
-         """
-         try:
-             cursor = self.conn.cursor()
-             cursor.execute("SELECT DISTINCT source FROM news WHERE source IS NOT NULL AND source != ''")
-             sources = [row[0] for row in cursor.fetchall()]
-             return sources
-         except sqlite3.Error as e:
-             logging.error(f"获取来源列表失败: {str(e)}")
-             return []
-     
-     def execute_query(self, query, params=()):
-         """
-         执行自定义SQL查询
-         
-         Args:
-             query: SQL查询语句
-             params: 查询参数
-             
-         Returns:
-             执行结果
-         """
-         try:
-             cursor = self.conn.cursor()
-             cursor.execute(query, params)
-             self.conn.commit()
-             return cursor.fetchall()
-         except sqlite3.Error as e:
-             logging.error(f"执行SQL查询失败: {str(e)}")
-             self.conn.rollback()
-             raise 
+         import glob
+         
+         # 查找所有来源数据库
+         source_pattern = os.path.join(self.sources_dir, '*.db')
+         source_files = glob.glob(source_pattern)
+         
+         # 提取来源名称
+         sources = []
+         for path in source_files:
+             filename = os.path.basename(path)
+             source = os.path.splitext(filename)[0]
+             sources.append(source)
+         
+         return sorted(sources)
+     
+     def close(self):
+         """关闭数据库连接"""
+         for path, conn in list(self.connections.items()):
+             if conn:
+                 try:
+                     conn.close()
+                 except Exception as e:
+                     logging.error(f"关闭数据库连接失败: {str(e)}")
+         
+         # 清空连接字典
+         self.connections = {}
+         self.conn = None
+     
+     def __del__(self):
+         """析构函数，确保关闭数据库连接"""
+         self.close()
+ 
+     # 保留原来的insert_news等其他方法
+     # ... 
```

---

<details>
            <summary>Listed directory app/web • **7** results</summary>
        
| Name |
|-------|
| 📁 `__pycache__` |
| 📄 `routes.py` |
| 📁 `templates` |
| 📄 `__init__.py` |
| 📄 `__init__.py.bak` |
| 📄 `fix_indentation.py` |
| 📁 `static` |

</details>

---

<details>
            <summary>Listed directory app/web/templates • **9** results</summary>
        
| Name |
|-------|
| 📄 `news_detail.html` |
| 📄 `index.html` |
| 📄 `error.html` |
| 📄 `base.html` |
| 📄 `500.html` |
| 📄 `404.html` |
| 📄 `settings.html` |
| 📄 `data_analysis.html` |
| 📄 `crawler_status.html` |

</details>

---

```diff
  
  {% block content %}
+ <div class="container-fluid">
      <div class="row">
          <div class="col-md-12">
-         <h2 class="mb-4">爬虫状态</h2>
+             <h1 class="mb-4">爬虫状态</h1>
              
-         <!-- 爬虫状态卡片 -->
+             <!-- 统计卡片 -->
              <div class="row mb-4">
                  <div class="col-md-3">
                      <div class="card">
                          <div class="card-body">
-                         <h5 class="card-title">今日爬取</h5>
-                         <p class="card-text display-6">{{ today_count }}</p>
+                             <h5 class="card-title">今日新闻</h5>
+                             <h2 class="card-text">{{ today_count }}</h2>
                          </div>
                      </div>
                      <div class="card">
                          <div class="card-body">
-                         <h5 class="card-title">本周爬取</h5>
-                         <p class="card-text display-6">{{ week_count }}</p>
+                             <h5 class="card-title">本周新闻</h5>
+                             <h2 class="card-text">{{ week_count }}</h2>
                          </div>
                      </div>
                      <div class="card">
                          <div class="card-body">
-                         <h5 class="card-title">本月爬取</h5>
-                         <p class="card-text display-6">{{ month_count }}</p>
+                             <h5 class="card-title">本月新闻</h5>
+                             <h2 class="card-text">{{ month_count }}</h2>
                          </div>
                      </div>
                          <div class="card-body">
                              <h5 class="card-title">总新闻数</h5>
-                         <p class="card-text display-6">{{ total_count }}</p>
+                             <h2 class="card-text">{{ total_count }}</h2>
+                         </div>
                      </div>
+                 </div>
+             </div>
+             
+             <!-- 系统信息 -->
+             <div class="card mb-4">
+                 <div class="card-header">
+                     系统信息
+                 </div>
+                 <div class="card-body">
+                     <div class="row">
+                         <div class="col-md-3">
+                             <strong>系统类型：</strong> {{ system_info.system }}
+                         </div>
+                         <div class="col-md-3">
+                             <strong>CPU使用率：</strong> {{ system_info.cpu_percent }}%
+                         </div>
+                         <div class="col-md-3">
+                             <strong>内存使用率：</strong> {{ system_info.memory_percent }}%
+                         </div>
+                         <div class="col-md-3">
+                             <strong>磁盘使用率：</strong> {{ system_info.disk_percent }}%
                          </div>
                      </div>
                  </div>
+             </div>
  
-         <!-- 爬虫任务列表 -->
-         <div class="card">
-             <div class="card-header d-flex justify-content-between align-items-center">
-                 <h5 class="mb-0">爬虫任务</h5>
-                 <button class="btn btn-primary" onclick="startAllCrawlers()">
-                     <i class="bi bi-play-fill"></i> 启动所有爬虫
-                 </button>
+             <!-- 数据库管理选项卡 -->
+             <div class="card mb-4">
+                 <div class="card-header">
+                     <ul class="nav nav-tabs card-header-tabs" id="databaseTabs" role="tablist">
+                         <li class="nav-item" role="presentation">
+                             <button class="nav-link active" id="crawler-tab" data-bs-toggle="tab" data-bs-target="#crawler-content" type="button" role="tab" aria-controls="crawler-content" aria-selected="true">爬虫状态</button>
+                         </li>
+                         <li class="nav-item" role="presentation">
+                             <button class="nav-link" id="database-tab" data-bs-toggle="tab" data-bs-target="#database-content" type="button" role="tab" aria-controls="database-content" aria-selected="false">数据库管理</button>
+                         </li>
+                     </ul>
                  </div>
                  <div class="card-body">
+                     <div class="tab-content" id="databaseTabsContent">
+                         <!-- 爬虫状态内容 -->
+                         <div class="tab-pane fade show active" id="crawler-content" role="tabpanel" aria-labelledby="crawler-tab">
+                             <!-- 爬虫列表 -->
+                             <h3 class="mt-3 mb-3">爬虫列表</h3>
                              <div class="table-responsive">
-                     <table class="table table-hover">
+                                 <table class="table table-striped table-hover">
                                      <thead>
                                          <tr>
                                              <th>爬虫名称</th>
                                              <th>状态</th>
+                                             <th>今日获取</th>
+                                             <th>总获取</th>
                                              <th>上次运行</th>
-                                 <th>下次运行</th>
                                              <th>操作</th>
                                          </tr>
                                              <td>{{ crawler.name }}</td>
                                              <td>
-                                     <span class="badge bg-{{ crawler.status_color }}">
-                                         {{ crawler.status }}
-                                     </span>
+                                                 {% if crawler.status == 'running' %}
+                                                 <span class="badge bg-success">运行中</span>
+                                                 {% elif crawler.status == 'idle' %}
+                                                 <span class="badge bg-secondary">空闲</span>
+                                                 {% elif crawler.status == 'error' %}
+                                                 <span class="badge bg-danger">错误</span>
+                                                 {% endif %}
                                              </td>
+                                             <td>{{ crawler.today_count }}</td>
+                                             <td>{{ crawler.total_count }}</td>
                                              <td>{{ crawler.last_run }}</td>
-                                 <td>{{ crawler.next_run }}</td>
-                                 <td>
-                                     <button class="btn btn-sm btn-primary" onclick="startCrawler('{{ crawler.name }}')">
-                                         <i class="bi bi-play-fill"></i>
-                                     </button>
-                                     <button class="btn btn-sm btn-danger" onclick="stopCrawler('{{ crawler.name }}')">
-                                         <i class="bi bi-stop-fill"></i>
-                                     </button>
+                                             <td>
+                                                 <div class="btn-group" role="group">
+                                                     <a href="/crawler/run?name={{ crawler.name }}" class="btn btn-sm btn-primary">运行</a>
+                                                     <a href="/crawler/stop?name={{ crawler.name }}" class="btn btn-sm btn-warning">停止</a>
+                                                     <a href="/crawler/log?name={{ crawler.name }}" class="btn btn-sm btn-info">日志</a>
+                                                 </div>
                                              </td>
                                          </tr>
                                  </table>
                              </div>
+                             
+                             <!-- 可用爬虫源 -->
+                             <h3 class="mt-4 mb-3">可用爬虫源</h3>
+                             <div class="row">
+                                 {% for source in sources %}
+                                 <div class="col-md-3 mb-3">
+                                     <div class="card">
+                                         <div class="card-body">
+                                             <h5 class="card-title">{{ source }}</h5>
+                                             <a href="/crawler/run?source={{ source }}" class="btn btn-sm btn-primary">运行爬虫</a>
+                                         </div>
+                                     </div>
+                                 </div>
+                                 {% endfor %}
                              </div>
                          </div>
                          
-         <!-- 最近日志 -->
-         <div class="card mt-4">
-             <div class="card-header">
-                 <h5 class="mb-0">最近日志</h5>
+                         <!-- 数据库管理内容 -->
+                         <div class="tab-pane fade" id="database-content" role="tabpanel" aria-labelledby="database-tab">
+                             <h3 class="mt-3 mb-3">数据库管理</h3>
+                             
+                             <!-- 数据源选择 -->
+                             <div class="card mb-4">
+                                 <div class="card-header">数据源选择</div>
+                                 <div class="card-body">
+                                     <form id="sourceSelectForm" method="GET" action="/database/select_source">
+                                         <div class="row align-items-end">
+                                             <div class="col-md-6">
+                                                 <label for="sourceSelect" class="form-label">数据源</label>
+                                                 <select class="form-select" id="sourceSelect" name="source">
+                                                     <option value="all">所有数据源</option>
+                                                     <option value="main">主数据库</option>
+                                                     {% for source in sources %}
+                                                     <option value="{{ source }}">{{ source }}</option>
+                                                     {% endfor %}
+                                                 </select>
+                                             </div>
+                                             <div class="col-md-6">
+                                                 <button type="submit" class="btn btn-primary">切换数据源</button>
+                                                 <a href="/database/sync_all" class="btn btn-success">同步全部到主数据库</a>
+                                             </div>
+                                         </div>
+                                     </form>
+                                 </div>
                              </div>
+                             
+                             <!-- 备份与恢复 -->
+                             <div class="card mb-4">
+                                 <div class="card-header">备份与恢复</div>
                                  <div class="card-body">
+                                     <div class="row mb-3">
+                                         <div class="col-md-6">
+                                             <h5>创建备份</h5>
+                                             <form id="backupForm" method="POST" action="/database/backup">
+                                                 <div class="mb-3">
+                                                     <label for="backupSource" class="form-label">要备份的数据源</label>
+                                                     <select class="form-select" id="backupSource" name="source">
+                                                         <option value="main">主数据库</option>
+                                                         {% for source in sources %}
+                                                         <option value="{{ source }}">{{ source }}</option>
+                                                         {% endfor %}
+                                                     </select>
+                                                 </div>
+                                                 <button type="submit" class="btn btn-primary">创建备份</button>
+                                             </form>
+                                         </div>
+                                         <div class="col-md-6">
+                                             <h5>恢复备份</h5>
+                                             <form id="restoreForm" method="POST" action="/database/restore">
+                                                 <div class="mb-3">
+                                                     <label for="backupFile" class="form-label">备份文件</label>
+                                                     <select class="form-select" id="backupFile" name="backup_file">
+                                                         {% for backup in backups %}
+                                                         <option value="{{ backup.path }}">{{ backup.source }} - {{ backup.timestamp.strftime('%Y-%m-%d %H:%M:%S') }} ({{ (backup.size / 1024)|round(1) }} KB)</option>
+                                                         {% endfor %}
+                                                     </select>
+                                                 </div>
+                                                 <div class="mb-3">
+                                                     <label for="restoreTarget" class="form-label">恢复目标数据源</label>
+                                                     <select class="form-select" id="restoreTarget" name="target_source">
+                                                         <option value="main">主数据库</option>
+                                                         {% for source in sources %}
+                                                         <option value="{{ source }}">{{ source }}</option>
+                                                         {% endfor %}
+                                                     </select>
+                                                 </div>
+                                                 <button type="submit" class="btn btn-warning">恢复备份</button>
+                                             </form>
+                                         </div>
+                                     </div>
+                                     
+                                     <!-- 备份列表 -->
+                                     <h5 class="mt-4">备份列表</h5>
                                      <div class="table-responsive">
-                     <table class="table">
+                                         <table class="table table-striped table-hover">
                                              <thead>
                                                  <tr>
-                                 <th>时间</th>
-                                 <th>爬虫</th>
-                                 <th>级别</th>
-                                 <th>消息</th>
+                                                     <th>数据源</th>
+                                                     <th>备份时间</th>
+                                                     <th>大小</th>
+                                                     <th>操作</th>
                                                  </tr>
                                              </thead>
                                              <tbody>
-                             {% for log in recent_logs %}
+                                                 {% for backup in backups %}
                                                  <tr>
-                                 <td>{{ log.timestamp }}</td>
-                                 <td>{{ log.crawler }}</td>
+                                                     <td>{{ backup.source }}</td>
+                                                     <td>{{ backup.timestamp.strftime('%Y-%m-%d %H:%M:%S') }}</td>
+                                                     <td>{{ (backup.size / 1024)|round(1) }} KB</td>
                                                      <td>
-                                     <span class="badge bg-{{ log.level_color }}">
-                                         {{ log.level }}
-                                     </span>
+                                                         <div class="btn-group" role="group">
+                                                             <a href="/database/restore?backup_file={{ backup.path }}&target_source=main" class="btn btn-sm btn-warning">恢复</a>
+                                                             <a href="/database/download?backup_file={{ backup.path }}" class="btn btn-sm btn-info">下载</a>
+                                                             <a href="/database/delete_backup?backup_file={{ backup.path }}" class="btn btn-sm btn-danger" onclick="return confirm('确定要删除此备份吗?')">删除</a>
+                                                         </div>
                                                      </td>
-                                 <td>{{ log.message }}</td>
                                                  </tr>
                                                  {% endfor %}
                                              </tbody>
                                          </table>
+                                     </div>
+                                 </div>
+                             </div>
+                         </div>
+                     </div>
                  </div>
              </div>
```

---

```diff
  import platform
  from datetime import datetime, timedelta
- from flask import render_template, request, redirect, url_for, flash, jsonify, send_from_directory
+ from flask import render_template, request, redirect, url_for, flash, jsonify, send_from_directory, send_file, current_app
  from app.db.sqlite_manager import SQLiteManager
  from app.db.user_preferences_db import UserPreferencesDB
- from app.utils.database import NewsDatabase  # 添加正确的导入
+ from app.utils.database import NewsDatabase
  from app.utils.logger import get_logger
  from app.config import get_settings
          """爬虫管理页面"""
          try:
-             # 使用crawler_status的逻辑
-             # 始终使用所有可用的数据库
-             db = NewsDatabase(use_all_dbs=True)
+             # 获取选中的数据源
+             selected_source = None
+             if hasattr(app, 'session') and 'selected_source' in app.session:
+                 selected_source = app.session['selected_source']
+                 if selected_source == 'all':
+                     selected_source = None
+                     use_all_dbs = True
+                 else:
+                     use_all_dbs = False
+             else:
+                 use_all_dbs = True  # 默认使用所有数据库
+             
+             # 使用指定的数据源初始化数据库
+             db = NewsDatabase(source=selected_source, use_all_dbs=use_all_dbs)
              
              # 获取统计数据
              # 获取系统信息
              system_info = {
-                 'platform': platform.platform(),
-                 'python_version': platform.python_version(),
-                 'cpu_count': psutil.cpu_count(),
-                 'memory': {
-                     'total': psutil.virtual_memory().total / (1024 * 1024 * 1024),  # GB
-                     'used': psutil.virtual_memory().used / (1024 * 1024 * 1024),  # GB
-                     'percent': psutil.virtual_memory().percent
-                 },
-                 'disk': {
-                     'total': psutil.disk_usage('/').total / (1024 * 1024 * 1024),  # GB
-                     'used': psutil.disk_usage('/').used / (1024 * 1024 * 1024),  # GB
-                     'percent': psutil.disk_usage('/').percent
-                 }
-             }
+                 'system': f"{platform.system()} {platform.release()}",
+                 'cpu_percent': int(psutil.cpu_percent()),
+                 'memory_percent': int(psutil.virtual_memory().percent),
+                 'disk_percent': int(psutil.disk_usage('/').percent)
+             }
+             
+             # 获取可用数据源
+             sources = db.get_available_sources()
+             
+             # 获取备份列表
+             backups = db.get_backups()
+             
+             # 关闭数据库连接
+             db.close()
              
              return render_template('crawler_status.html',
                                   total_count=total_count,
                                   crawlers=crawlers,
-                                 system_info=system_info)
+                                  system_info=system_info,
+                                  sources=sources,
+                                  backups=backups,
+                                  selected_source=selected_source)
+                                  
          except Exception as e:
-             logger.error(f"爬虫管理页面加载失败: {str(e)}")
-             logger.exception(e)
-             error_message = f"加载爬虫管理页面时发生错误: {str(e)}"
-             try:
-                 return render_template('error.html', 
-                                 error=error_message,
-                                 traceback=traceback.format_exc()), 500
-             except Exception as e2:
-                 logger.error(f"渲染错误页面失败: {str(e2)}")
-                 return f"严重错误: 页面加载失败 ({str(e)})", 500
+             logger.error(f"渲染爬虫管理页面时出错: {str(e)}")
+             import traceback
+             logger.error(traceback.format_exc())
+             return render_template('error.html', error=str(e))
      
      @app.route('/dashboard')
          )
  
+     @app.route('/database/select_source')
+     def select_source():
+         """切换数据源"""
+         source = request.args.get('source', 'all')
+         
+         # 保存选择到会话
+         if hasattr(app, 'session'):
+             app.session['selected_source'] = source
+         
+         # 重定向回爬虫管理页面
+         flash(f'已切换到数据源: {source}', 'success')
+         return redirect(url_for('crawler'))
+     
+     @app.route('/database/backup', methods=['POST'])
+     def backup_database():
+         """备份数据库"""
+         source = request.form.get('source')
+         
+         try:
+             # 根据选择的数据源进行备份
+             if source == 'main':
+                 source = None  # 主数据库使用None表示
+                 
+             # 获取数据库管理器
+             db = None
+             if hasattr(app, 'db'):
+                 db = app.db
+             else:
+                 # 使用NewsDatabase获取数据库连接
+                 db = NewsDatabase()
+             
+             # 执行备份
+             backup_path = db.backup_database(source)
+             
+             if backup_path:
+                 flash(f'成功创建备份: {os.path.basename(backup_path)}', 'success')
+             else:
+                 flash('备份失败', 'error')
+                 
+             # 关闭连接
+             if db and not hasattr(app, 'db'):
+                 db.close()
+                 
+         except Exception as e:
+             logger.error(f"备份数据库失败: {str(e)}")
+             flash(f'备份失败: {str(e)}', 'error')
+             
+         return redirect(url_for('crawler'))
+     
+     @app.route('/database/restore', methods=['GET', 'POST'])
+     def restore_database():
+         """恢复数据库"""
+         # 处理GET和POST请求
+         if request.method == 'POST':
+             backup_file = request.form.get('backup_file')
+             target_source = request.form.get('target_source')
+         else:
+             backup_file = request.args.get('backup_file')
+             target_source = request.args.get('target_source')
+         
+         if not backup_file:
+             flash('缺少备份文件', 'error')
+             return redirect(url_for('crawler'))
+             
+         try:
+             # 处理目标数据源
+             if target_source == 'main':
+                 target_source = None  # 主数据库使用None表示
+                 
+             # 获取数据库管理器
+             db = None
+             if hasattr(app, 'db'):
+                 db = app.db
+             else:
+                 # 使用NewsDatabase获取数据库连接
+                 db = NewsDatabase()
+             
+             # 执行恢复
+             success = db.restore_database(backup_file, target_source)
+             
+             if success:
+                 flash(f'成功恢复数据库: {os.path.basename(backup_file)}', 'success')
+             else:
+                 flash('恢复失败', 'error')
+                 
+             # 关闭连接
+             if db and not hasattr(app, 'db'):
+                 db.close()
+                 
+         except Exception as e:
+             logger.error(f"恢复数据库失败: {str(e)}")
+             flash(f'恢复失败: {str(e)}', 'error')
+             
+         return redirect(url_for('crawler'))
+     
+     @app.route('/database/download')
+     def download_backup():
+         """下载备份文件"""
+         backup_file = request.args.get('backup_file')
+         
+         if not backup_file or not os.path.exists(backup_file):
+             flash('备份文件不存在', 'error')
+             return redirect(url_for('crawler'))
+             
+         try:
+             # 获取文件名
+             filename = os.path.basename(backup_file)
+             
+             # 发送文件
+             return send_file(
+                 backup_file,
+                 as_attachment=True,
+                 download_name=filename,
+                 mimetype='application/octet-stream'
+             )
+         except Exception as e:
+             logger.error(f"下载备份文件失败: {str(e)}")
+             flash(f'下载失败: {str(e)}', 'error')
+             return redirect(url_for('crawler'))
+     
+     @app.route('/database/delete_backup')
+     def delete_backup():
+         """删除备份文件"""
+         backup_file = request.args.get('backup_file')
+         
+         if not backup_file or not os.path.exists(backup_file):
+             flash('备份文件不存在', 'error')
+             return redirect(url_for('crawler'))
+             
+         try:
+             # 删除文件
+             os.remove(backup_file)
+             flash(f'成功删除备份文件: {os.path.basename(backup_file)}', 'success')
+         except Exception as e:
+             logger.error(f"删除备份文件失败: {str(e)}")
+             flash(f'删除失败: {str(e)}', 'error')
+             
+         return redirect(url_for('crawler'))
+     
+     @app.route('/database/sync_all')
+     def sync_all_sources():
+         """同步所有数据源到主数据库"""
+         try:
+             # 获取数据库管理器
+             db = None
+             if hasattr(app, 'db'):
+                 db = app.db
+             else:
+                 # 使用NewsDatabase获取数据库连接
+                 db = NewsDatabase()
+             
+             # 获取所有数据源
+             sources = db.get_available_sources()
+             
+             # 执行同步
+             total_synced = 0
+             for source in sources:
+                 count = db.sync_to_main_db(source)
+                 total_synced += count
+                 
+             if total_synced > 0:
+                 flash(f'成功同步 {total_synced} 条新闻到主数据库', 'success')
+             else:
+                 flash('没有新的内容需要同步', 'info')
+                 
+             # 关闭连接
+             if db and not hasattr(app, 'db'):
+                 db.close()
+                 
+         except Exception as e:
+             logger.error(f"同步数据库失败: {str(e)}")
+             flash(f'同步失败: {str(e)}', 'error')
+             
+         return redirect(url_for('crawler'))
+ 
      @app.errorhandler(500)
      def server_error(error):
```

---

```diff
  from app.crawlers.strategies import STRATEGY_MAP
  from app.crawlers.enhanced_crawler import EnhancedCrawler
+ from app.utils.database import NewsDatabase
  
  logger = get_logger('crawler_factory')
              os.makedirs(self.config_dir, exist_ok=True)
              logger.info(f"已创建爬虫配置目录: {self.config_dir}")
-     
-     def create_crawler(self, source: str, db_path: str, options: Dict[str, Any] = None) -> BaseCrawler:
+         
+         # 初始化数据库目录
+         settings = self._get_default_config()
+         db_dir = settings.get('DB_DIR', os.path.join('data', 'db'))
+         
+         # 确保路径是绝对路径
+         if not os.path.isabs(db_dir):
+             db_dir = os.path.join(os.getcwd(), db_dir)
+             
+         # 创建数据库目录结构
+         self.db_dir = db_dir
+         self.sources_dir = os.path.join(db_dir, 'sources')
+         self.main_db_path = os.path.join(db_dir, 'finance_news.db')
+         
+         # 确保目录存在
+         for dir_path in [db_dir, self.sources_dir]:
+             if not os.path.exists(dir_path):
+                 os.makedirs(dir_path, exist_ok=True)
+                 logger.info(f"已创建数据库目录: {dir_path}")
+     
+     def create_crawler(self, source: str, db_path: str = None, options: Dict[str, Any] = None) -> BaseCrawler:
          """
          创建指定来源的爬虫实例
          
          Args:
              source: 数据源名称
-             db_path: 数据库路径
+             db_path: 数据库路径，如果为None则使用专用路径
              options: 爬虫选项
              
              crawler_options['max_concurrency'] = crawler_options.pop('concurrency')
          
+         # 确定数据库路径
+         if db_path is None:
+             # 规范化源名称
+             safe_source_name = source.replace(' ', '_').replace('/', '_')
+             # 使用源专用数据库
+             db_path = os.path.join(self.sources_dir, f"{safe_source_name}.db")
+             logger.info(f"将为爬虫 {source} 使用专用数据库: {db_path}")
+         
+         # 添加数据同步选项
+         sync_to_main = crawler_options.pop('sync_to_main', True)
+         
          # 创建爬虫实例
          if use_enhanced:
              logger.info(f"创建标准爬虫实例: {source}, 数据库: {db_path}")
              crawler = BaseCrawler(source, db_path, **crawler_options)
-         
+         
+         # 设置同步回调
+         if sync_to_main:
+             # 添加同步到主数据库的回调
+             original_crawl = crawler.crawl
+             
+             def crawl_and_sync(*args, **kwargs):
+                 # 调用原始爬取方法
+                 result = original_crawl(*args, **kwargs)
+                 
+                 # 爬取完成后同步到主数据库
+                 try:
+                     self._sync_to_main_db(source)
+                 except Exception as e:
+                     logger.error(f"同步 {source} 到主数据库失败: {str(e)}")
+                 
+                 return result
+                 
+             # 替换爬取方法
+             crawler.crawl = crawl_and_sync
+             
          return crawler
      
-     def create_enhanced_crawler(self, source: str, db_path: str, options: Dict[str, Any] = None) -> EnhancedCrawler:
+     def _sync_to_main_db(self, source: str) -> int:
+         """
+         将指定来源的数据同步到主数据库
+         
+         Args:
+             source: 数据源名称
+             
+         Returns:
+             int: 同步的记录数
+         """
+         try:
+             # 初始化数据库连接
+             db = NewsDatabase()
+             
+             # 执行同步
+             count = db.sync_to_main_db(source)
+             
+             # 关闭数据库连接
+             db.close()
+             
+             logger.info(f"成功将 {count} 条记录从 {source} 同步到主数据库")
+             return count
+         except Exception as e:
+             logger.error(f"同步数据库失败: {str(e)}")
+             raise
+     
+     def create_enhanced_crawler(self, source: str, db_path: str = None, options: Dict[str, Any] = None) -> EnhancedCrawler:
          """
          创建增强型爬虫实例
              raise
      
-     def create_all_crawlers(self, db_dir: str, options: Dict[str, Any] = None) -> Dict[str, BaseCrawler]:
+     def create_all_crawlers(self, db_dir: str = None, options: Dict[str, Any] = None) -> Dict[str, BaseCrawler]:
          """
          创建所有支持的爬虫实例
          
          Args:
-             db_dir: 数据库目录
+             db_dir: 数据库目录，如果为None则使用默认目录
              options: 通用爬虫选项
              
          Returns:
              Dict[str, BaseCrawler]: {源名称: 爬虫实例}
          """
+         # 如果指定了db_dir，更新实例的db_dir
+         if db_dir:
+             self.db_dir = db_dir
+             self.sources_dir = os.path.join(db_dir, 'sources')
+             self.main_db_path = os.path.join(db_dir, 'finance_news.db')
+             
+             # 确保目录存在
+             for dir_path in [db_dir, self.sources_dir]:
+                 if not os.path.exists(dir_path):
+                     os.makedirs(dir_path, exist_ok=True)
+         
          crawlers = {}
          
          for source in STRATEGY_MAP.keys():
              try:
-                 # 构建数据库路径
-                 db_path = os.path.join(db_dir, f"{source}.db")
-                 
-                 # 创建爬虫实例
-                 crawler = self.create_crawler(source, db_path, options)
+                 # 创建爬虫实例，使用默认专用数据库路径
+                 crawler = self.create_crawler(source, options=options)
                  crawlers[source] = crawler
                  
          return crawlers
      
-     def create_all_enhanced_crawlers(self, db_dir: str, options: Dict[str, Any] = None) -> Dict[str, EnhancedCrawler]:
+     def create_all_enhanced_crawlers(self, db_dir: str = None, options: Dict[str, Any] = None) -> Dict[str, EnhancedCrawler]:
          """
          创建所有支持的增强型爬虫实例
              if os.path.exists(yaml_path):
                  config = self._load_config(yaml_path)
-             
-         # 使用默认配置
+         
+         # 尝试YML配置
+         if config is None:
+             yml_path = os.path.join(self.config_dir, f"{source}.yml")
+             if os.path.exists(yml_path):
+                 config = self._load_config(yml_path)
+         
+         # 如果无法加载配置，使用默认配置
          if config is None:
+             logger.warning(f"未找到 {source} 的配置文件，使用默认配置")
              config = self._get_default_config()
-             
+         
          # 缓存配置
          self.config_cache[source] = config
      def _load_config(self, config_path: str) -> Optional[Dict[str, Any]]:
          """
-         从文件加载配置
+         加载配置文件
          
          Args:
              config_path: 配置文件路径
              
          Returns:
-             Dict[str, Any]: 配置字典，失败时返回None
+             Optional[Dict[str, Any]]: 配置字典，加载失败则返回None
          """
          try:
-             # 根据文件扩展名选择解析方式
+             logger.debug(f"尝试加载配置文件: {config_path}")
+             
+             # 根据文件扩展名选择加载方式
              if config_path.endswith('.json'):
                  with open(config_path, 'r', encoding='utf-8') as f:
-                     return json.load(f)
-             elif config_path.endswith(('.yaml', '.yml')):
+                     config = json.load(f)
+             elif config_path.endswith('.yaml') or config_path.endswith('.yml'):
                  with open(config_path, 'r', encoding='utf-8') as f:
-                     return yaml.safe_load(f)
+                     config = yaml.safe_load(f)
              else:
-                 logger.warning(f"不支持的配置文件格式: {config_path}")
+                 logger.error(f"不支持的配置文件格式: {config_path}")
                  return None
+             
+             logger.debug(f"配置文件加载成功: {config_path}")
+             return config
          except Exception as e:
-             logger.error(f"加载配置文件失败 {config_path}: {str(e)}")
+             logger.error(f"加载配置文件 {config_path} 失败: {str(e)}")
              return None
      
              Dict[str, Any]: 默认配置
          """
-         return {
+         # 从环境变量获取配置值
+         from app.config import get_settings
+         settings = get_settings()
+         
+         # 构建默认配置
+         default_config = {
+             'use_enhanced': False,
+             'async_mode': True,
              'use_proxy': False,
-             'async_mode': True,
+             'timeout': 30,
              'max_workers': 5,
              'max_concurrency': 10,
-             'timeout': 30,
-             'use_enhanced': False  # 默认不使用增强型爬虫
+             'DB_DIR': settings.get('DB_DIR', os.path.join('data', 'db')),
+             'sync_to_main': True  # 默认同步到主数据库
          }
+         
+         return default_config
      
      def save_config(self, source: str, config: Dict[str, Any], format: str = 'json') -> bool:
          """
-         保存源配置到文件
+         保存配置到文件
          
          Args:
              source: 源名称
              config: 配置字典
-             format: 文件格式，'json'或'yaml'
+             format: 文件格式，支持json/yaml
              
          Returns:
-             bool: 保存是否成功
+             bool: 是否保存成功
          """
-         try:
-             # 确保配置目录存在
-             if not os.path.exists(self.config_dir):
-                 os.makedirs(self.config_dir, exist_ok=True)
-             
-             # 根据格式选择保存路径和方式
+         # 确保配置目录存在
+         if not os.path.exists(self.config_dir):
+             os.makedirs(self.config_dir, exist_ok=True)
+         
+         # 确定文件路径
+         if format.lower() == 'json':
+             config_path = os.path.join(self.config_dir, f"{source}.json")
+         elif format.lower() in ['yaml', 'yml']:
+             config_path = os.path.join(self.config_dir, f"{source}.yaml")
+         else:
+             logger.error(f"不支持的配置文件格式: {format}")
+             return False
+         
+         try:
+             # 保存配置
              if format.lower() == 'json':
-                 config_path = os.path.join(self.config_dir, f"{source}.json")
                  with open(config_path, 'w', encoding='utf-8') as f:
-                     json.dump(config, f, ensure_ascii=False, indent=4)
-             elif format.lower() in ('yaml', 'yml'):
-                 config_path = os.path.join(self.config_dir, f"{source}.yaml")
+                     json.dump(config, f, indent=4, ensure_ascii=False)
+             else:
                  with open(config_path, 'w', encoding='utf-8') as f:
-                     yaml.dump(config, f, allow_unicode=True)
-             else:
-                 logger.error(f"不支持的配置格式: {format}")
-                 return False
+                     yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
              
              # 更新缓存
              self.config_cache[source] = config
              
-             logger.info(f"已保存 {source} 配置到 {config_path}")
+             logger.info(f"配置已保存到文件: {config_path}")
              return True
          except Exception as e:
-             logger.error(f"保存配置失败: {str(e)}")
+             logger.error(f"保存配置文件 {config_path} 失败: {str(e)}")
              return False
```

---

```diff
  from app.config import get_settings
  from app.utils.logger import get_logger
+ from app.utils.database import NewsDatabase
  
  logger = get_logger(__name__)
                  'last_run': None,
                  'next_run': datetime.now()
+             },
+             'daily_backup': {
+                 'description': '每日备份数据库',
+                 'interval': 24 * 60 * 60,  # 24小时
+                 'last_run': None,
+                 'next_run': datetime.now() + timedelta(hours=1)  # 启动1小时后进行第一次备份
+             },
+             'weekly_backup': {
+                 'description': '每周备份数据库',
+                 'interval': 7 * 24 * 60 * 60,  # 7天
+                 'last_run': None,
+                 'next_run': datetime.now() + timedelta(days=1)  # 启动1天后进行第一次备份
              }
          }
              elif task_name == 'hourly_crawl':
                  self._run_hourly_crawl()
+             elif task_name == 'daily_backup':
+                 self._run_daily_backup()
+             elif task_name == 'weekly_backup':
+                 self._run_weekly_backup()
              
              # 更新任务状态
              manager.close()
      
+     def _run_daily_backup(self):
+         """执行每日备份任务"""
+         try:
+             # 创建数据库连接
+             db = NewsDatabase()
+             
+             # 备份主数据库
+             main_backup_path = db.backup_database()
+             if main_backup_path:
+                 logger.info(f"主数据库备份成功: {main_backup_path}")
+             
+             # 获取所有数据源
+             sources = db.get_available_sources()
+             logger.info(f"开始每日备份数据源，数量: {len(sources)}")
+             
+             # 备份每个数据源
+             for source in sources:
+                 try:
+                     backup_path = db.backup_database(source)
+                     if backup_path:
+                         logger.info(f"数据源 {source} 备份成功: {backup_path}")
+                 except Exception as e:
+                     logger.error(f"数据源 {source} 备份失败: {e}")
+             
+             # 清理过期备份（保留最近10个）
+             self._clean_old_backups(db, max_keep=10)
+             
+             # 关闭数据库连接
+             db.close()
+             
+             logger.info("每日备份任务完成")
+         except Exception as e:
+             logger.error(f"执行每日备份任务失败: {e}")
+     
+     def _run_weekly_backup(self):
+         """执行每周备份任务"""
+         try:
+             # 创建数据库连接
+             db = NewsDatabase()
+             
+             # 备份主数据库
+             main_backup_path = db.backup_database()
+             if main_backup_path:
+                 logger.info(f"主数据库周备份成功: {main_backup_path}")
+                 
+                 # 为周备份文件添加标记，确保不会被每日备份清理策略删除
+                 weekly_mark_path = main_backup_path + ".weekly"
+                 with open(weekly_mark_path, 'w') as f:
+                     f.write(f"周备份时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
+             
+             # 获取所有数据源
+             sources = db.get_available_sources()
+             logger.info(f"开始每周备份数据源，数量: {len(sources)}")
+             
+             # 备份每个数据源
+             for source in sources:
+                 try:
+                     backup_path = db.backup_database(source)
+                     if backup_path:
+                         logger.info(f"数据源 {source} 周备份成功: {backup_path}")
+                         
+                         # 为周备份文件添加标记
+                         weekly_mark_path = backup_path + ".weekly"
+                         with open(weekly_mark_path, 'w') as f:
+                             f.write(f"周备份时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
+                 except Exception as e:
+                     logger.error(f"数据源 {source} 周备份失败: {e}")
+             
+             # 关闭数据库连接
+             db.close()
+             
+             logger.info("每周备份任务完成")
+         except Exception as e:
+             logger.error(f"执行每周备份任务失败: {e}")
+     
+     def _clean_old_backups(self, db, max_keep=10):
+         """
+         清理过期备份，每个数据源只保留最新的几个备份
+         
+         Args:
+             db: 数据库对象
+             max_keep: 每个数据源要保留的最大备份数量
+         """
+         try:
+             # 获取所有备份
+             all_backups = db.get_backups()
+             
+             # 按数据源分组
+             backups_by_source = {}
+             for backup in all_backups:
+                 source = backup['source']
+                 if source not in backups_by_source:
+                     backups_by_source[source] = []
+                 backups_by_source[source].append(backup)
+             
+             # 对每个数据源，只保留最新的几个备份
+             for source, backups in backups_by_source.items():
+                 # 按时间降序排序
+                 backups.sort(key=lambda x: x['timestamp'], reverse=True)
+                 
+                 # 删除旧备份
+                 if len(backups) > max_keep:
+                     for old_backup in backups[max_keep:]:
+                         backup_path = old_backup['path']
+                         
+                         # 检查是否是周备份（有.weekly标记）
+                         weekly_mark_path = backup_path + ".weekly"
+                         if os.path.exists(weekly_mark_path):
+                             logger.info(f"保留周备份: {backup_path}")
+                             continue
+                             
+                         try:
+                             os.remove(backup_path)
+                             logger.info(f"删除旧备份: {backup_path}")
+                         except Exception as e:
+                             logger.error(f"删除旧备份失败 {backup_path}: {e}")
+             
+             logger.info("清理旧备份完成")
+         except Exception as e:
+             logger.error(f"清理旧备份失败: {e}")
+     
      def stop(self):
          """停止调度器"""
```

---

```diff
  import threading
  from datetime import datetime, timedelta
- from typing import List, Dict, Optional
+ from typing import List, Dict, Optional, Any, Union, Tuple
  from app.config import get_settings
  from app.utils.logger import get_logger
  from app.utils.database import NewsDatabase
  from app.crawlers.base import BaseCrawler
- from app.crawlers.eastmoney import EastMoneyCrawler
- from app.crawlers.sina import SinaCrawler
- from app.crawlers.netease import NeteaseCrawler
- from app.crawlers.ifeng import IfengCrawler
+ from app.crawlers.factory import CrawlerFactory
  import re
  import sqlite3
  
  class CrawlerManager:
-     """爬虫管理器"""
+     """爬虫管理器，负责管理所有爬虫"""
      
-     def __init__(self, settings=None):
-         """
-         初始化爬虫管理器
-         
-         Args:
-             settings: 配置字典
-         """
-         # 首先从app.config获取设置，然后用传入的settings更新它（如果有）
-         config_settings = get_settings()
-         self.settings = {} 
-         # 确保将config_settings中的设置复制到self.settings字典中
-         if hasattr(config_settings, 'settings'):
-             for key, value in config_settings.settings.items():
-                 self.settings[key] = value
-         else:
-             # 如果config_settings是字典，直接复制
-             self.settings = config_settings.copy()
-             
-         # 如果传入了settings，更新self.settings
-         if settings:
-             self.settings.update(settings)
-             
-         # 初始化logger
-         self.logger = get_logger("CrawlerManager")
-         
-         self.crawlers = {}
-         
-         # 获取数据库目录
-         db_dir = self.settings.get('DB_DIR')
-         if not db_dir:
-             # 如果在设置中没有找到DB_DIR，使用默认值
-             db_dir = os.path.join(os.getcwd(), 'data', 'db')
-             
-         # 确保目录存在
-         os.makedirs(db_dir, exist_ok=True)
-         self.logger.info(f"爬虫管理器使用数据库目录: {db_dir}")
-         
-         # 将DB_DIR保存回设置
-         self.settings['DB_DIR'] = db_dir
-         
-         # 初始化线程相关属性
-         self.crawler_threads = {}
-         self.stop_flags = {}
-         
-         # 初始化爬虫配置选项
-         self.use_proxy = self.settings.get('USE_PROXY', False)
-         self.use_source_db = True
-         
-         # 初始化数据库管理器
-         try:
-             from app.db.sqlite_manager import SQLiteManager
-             # 使用主数据库路径
-             main_db_path = os.path.join(db_dir, 'finance_news.db')
-             self.db_manager = SQLiteManager(db_path=main_db_path)
-             self.logger.info(f"初始化数据库管理器成功，使用数据库: {main_db_path}")
-         except Exception as e:
-             self.logger.error(f"初始化数据库管理器失败: {str(e)}")
-             self.db_manager = None
-             
-         # 初始化数据库连接
-         try:
-             # 尝试使用传入的db_path连接数据库
-             self.db = NewsDatabase()
-         except Exception as e:
-             self.logger.error(f"连接数据库失败: {str(e)}")
-             self.db = None
-             
-         # 导入所有爬虫类
-         try:
-             # 初始化所有爬虫
-             self._init_crawlers()
-             
-             # 添加到字典
-             self.crawlers["东方财富"] = self.eastmoney_crawler
-             self.crawlers["新浪财经"] = self.sina_crawler
-             self.crawlers["网易财经"] = self.netease_crawler
-             self.crawlers["凤凰财经"] = self.ifeng_crawler
-             
-             # 初始化stop_flags
-             for name in self.crawlers.keys():
-                 self.stop_flags[name] = threading.Event()
-             
-             self.logger.info(f"已初始化 {len(self.crawlers)} 个爬虫")
-             
-         except ImportError as e:
-             self.logger.error(f"导入爬虫模块失败: {str(e)}")
-     
-     def _init_crawlers(self):
-         """初始化所有爬虫"""
-         try:
-             from app.crawlers.eastmoney_crawler import EastMoneyCrawler
-             from app.crawlers.sina_crawler import SinaCrawler
-             from app.crawlers.netease_crawler import NeteaseCrawler
-             from app.crawlers.ifeng_crawler import IfengCrawler
-             
-             self.logger.info("开始初始化爬虫...")
+     def __init__(self):
+         """初始化爬虫管理器"""
+         self.settings = get_settings()
+         self.factory = CrawlerFactory()
+         self.crawlers_path = os.path.join(os.path.dirname(__file__), 'sites')
+         self.crawlers_cache = {}  # 缓存已创建的爬虫实例
+         self.db = NewsDatabase()  # 主数据库连接
+         
+         # 初始化爬虫目录
+         self._init_crawlers_dir()
+     
+     def _init_crawlers_dir(self):
+         """初始化爬虫目录"""
+         # 确保爬虫目录存在
+         if not os.path.exists(self.crawlers_path):
+             logger.info(f"创建爬虫目录: {self.crawlers_path}")
+             os.makedirs(self.crawlers_path, exist_ok=True)
          
-             # 初始化东方财富爬虫
-             try:
-                 self.eastmoney_crawler = EastMoneyCrawler()
-                 self.eastmoney_crawler.source = "东方财富"
-                 self.logger.info("东方财富爬虫初始化成功")
-             except Exception as e:
-                 self.logger.error(f"初始化东方财富爬虫失败: {str(e)}")
-                 self.eastmoney_crawler = None
-                 
-             # 初始化新浪财经爬虫
-             try:
-                 self.sina_crawler = SinaCrawler(
-                     db_manager=self.db_manager,
-                     use_proxy=self.use_proxy,
-                     use_source_db=self.use_source_db
-                 )
-                 self.sina_crawler.source = "新浪财经"
-                 self.logger.info("新浪财经爬虫初始化成功")
-             except Exception as e:
-                 self.logger.error(f"初始化新浪财经爬虫失败: {str(e)}")
-                 self.sina_crawler = None
-                 
-             # 初始化网易财经爬虫
-             try:
-                 self.netease_crawler = NeteaseCrawler(
-                     db_manager=self.db_manager,
-                     use_proxy=self.use_proxy,
-                     use_source_db=self.use_source_db
-                 )
-                 self.netease_crawler.source = "网易财经"
-                 self.logger.info("网易财经爬虫初始化成功")
-             except Exception as e:
-                 self.logger.error(f"初始化网易财经爬虫失败: {str(e)}")
-                 self.netease_crawler = None
-                 
-             # 初始化凤凰财经爬虫
-             try:
-                 self.ifeng_crawler = IfengCrawler(
-                     db_manager=self.db_manager,
-                     use_proxy=self.use_proxy,
-                     use_source_db=self.use_source_db
-                 )
-                 self.ifeng_crawler.source = "凤凰财经"
-                 self.logger.info("凤凰财经爬虫初始化成功")
-             except Exception as e:
-                 self.logger.error(f"初始化凤凰财经爬虫失败: {str(e)}")
-                 self.ifeng_crawler = None
-                 
-             # 设置所有爬虫的数据库路径
-             self._set_db_path()
-             
-             # 记录数据库路径
-             if self.eastmoney_crawler:
-                 self.logger.info(f"东方财富爬虫数据库路径: {self.eastmoney_crawler.db_path}")
-             if self.sina_crawler:
-                 self.logger.info(f"新浪财经爬虫数据库路径: {self.sina_crawler.db_path}")
-             if self.netease_crawler:
-                 self.logger.info(f"网易财经爬虫数据库路径: {self.netease_crawler.db_path}")
-             if self.ifeng_crawler:
-                 self.logger.info(f"凤凰财经爬虫数据库路径: {self.ifeng_crawler.db_path}")
-         except ImportError as e:
-             self.logger.error(f"导入爬虫模块失败: {str(e)}")
-             raise
- 
-     def _set_db_path(self):
-         """设置所有爬虫的数据库路径"""
-         # 确保数据库目录存在
-         db_dir = self.settings.get('DB_DIR')
-         if not db_dir:
-             db_dir = os.path.join(os.getcwd(), 'data', 'db')
-         os.makedirs(db_dir, exist_ok=True)
-         
-         # 设置东方财富爬虫的数据库路径
-         if self.eastmoney_crawler:
-             self._set_crawler_db_path(self.eastmoney_crawler, "东方财富")
-         
-         # 设置新浪财经爬虫的数据库路径
-         if self.sina_crawler:
-             self._set_crawler_db_path(self.sina_crawler, "新浪财经")
-         
-         # 设置网易财经爬虫的数据库路径
-         if self.netease_crawler:
-             self._set_crawler_db_path(self.netease_crawler, "网易财经")
-         
-         # 设置凤凰财经爬虫的数据库路径
-         if self.ifeng_crawler:
-             self._set_crawler_db_path(self.ifeng_crawler, "凤凰财经")
- 
-     def _set_crawler_db_path(self, crawler, source_name):
+         # 确保__init__.py存在
+         init_file = os.path.join(self.crawlers_path, '__init__.py')
+         if not os.path.exists(init_file):
+             logger.info(f"创建爬虫包初始化文件: {init_file}")
+             with open(init_file, 'w', encoding='utf-8') as f:
+                 f.write('# -*- coding: utf-8 -*-\n')
+                 f.write('"""爬虫实现目录"""\n')
+     
+     def get_available_crawlers(self) -> List[str]:
+         """
+         获取所有可用的爬虫列表
+         
+         Returns:
+             List[str]: 爬虫名称列表
+         """
+         crawlers = []
+         
+         # 扫描爬虫目录
+         for filename in os.listdir(self.crawlers_path):
+             if filename.endswith('.py') and filename != '__init__.py':
+                 crawler_name = filename[:-3]  # 去掉.py后缀
+                 crawlers.append(crawler_name)
+         
+         return crawlers
+     
+     def get_crawler_info(self, crawler_name: str) -> Dict[str, Any]:
          """
-         设置爬虫的数据库路径
+         获取指定爬虫的信息
          
          Args:
-             crawler: 爬虫实例
-             source_name: 来源名称
-         """
-         # 确保source属性被正确设置
-         if hasattr(crawler, 'source') and crawler.source == "未知来源":
-             crawler.source = source_name
-             self.logger.warning(f"爬虫 {crawler.__class__.__name__} 的source属性未设置，已设置为 {source_name}")
-         
-         # 确保数据库目录存在
-         db_dir = self.settings.get('DB_DIR')
-         if not db_dir:
-             db_dir = os.path.join(os.getcwd(), 'data', 'db')
-         os.makedirs(db_dir, exist_ok=True)
-         
-         # 设置数据库路径
-         db_path = os.path.join(db_dir, f"{source_name}.db")
-         if hasattr(crawler, 'db_path'):
-             crawler.db_path = db_path
-         
-         # 如果数据库不存在，则创建数据库结构
-         if not os.path.exists(db_path):
-             self.logger.info(f"数据库 {db_path} 不存在，将创建")
-             try:
-                 conn = sqlite3.connect(db_path)
-                 cursor = conn.cursor()
-                 cursor.execute('''
-                 CREATE TABLE IF NOT EXISTS news (
-                     id INTEGER PRIMARY KEY AUTOINCREMENT,
-                     title TEXT,
-                     content TEXT,
-                     url TEXT UNIQUE,
-                     publish_time TEXT,
-                     crawl_time TEXT,
-                     source TEXT,
-                     category TEXT,
-                     image_url TEXT,
-                     sentiment REAL,
-                     summary TEXT
-                 )
-                 ''')
-                 conn.commit()
-                 conn.close()
-                 self.logger.info(f"成功创建数据库 {db_path}")
-             except Exception as e:
-                 self.logger.error(f"创建数据库 {db_path} 失败: {str(e)}")
-     
-     def _verify_sources(self):
-         """验证所有爬虫的source属性是否正确设置，并统一来源名称格式"""
-         crawlers = {
-             'eastmoney_crawler': ('东方财富', self.eastmoney_crawler),
-             'sina_crawler': ('新浪财经', self.sina_crawler),
-             'netease_crawler': ('网易财经', self.netease_crawler),
-             'ifeng_crawler': ('凤凰财经', self.ifeng_crawler)
-         }
-         
-         for name, (expected_source, crawler) in crawlers.items():
-             if hasattr(crawler, 'source'):
-                 current_source = crawler.source
-                 # 标准化来源名称，处理常见变体
-                 if current_source.endswith('网') and current_source != expected_source:
-                     # 例如："东方财富网" -> "东方财富"
-                     crawler.source = expected_source
-                     logger.info(f"已统一爬虫 {name} 的source属性从 '{current_source}' 改为 '{expected_source}'")
-                 elif current_source != expected_source:
-                     logger.warning(f"爬虫 {name} 的source属性为 '{current_source}'，与预期的 '{expected_source}' 不符，已修正")
-                     crawler.source = expected_source
-             else:
-                 logger.error(f"爬虫 {name} 没有source属性")
-                 # 尝试设置source属性
-                 try:
-                     crawler.source = expected_source
-                     logger.info(f"已为爬虫 {name} 设置source属性为 '{expected_source}'")
-                 except Exception as e:
-                     logger.error(f"为爬虫 {name} 设置source属性失败: {str(e)}")
-                     
-         logger.info("已完成爬虫来源属性验证和统一格式化")
- 
-     def get_crawlers(self) -> List[BaseCrawler]:
-         """获取所有爬虫"""
-         return list(self.crawlers.values())
- 
-     def get_available_crawlers(self) -> List[str]:
-         """获取所有可用的爬虫名称"""
-         return list(self.crawlers.keys())
- 
-     def get_crawler(self, name: str) -> Optional[BaseCrawler]:
-         """获取指定爬虫"""
-         return self.crawlers.get(name)
- 
-     def start_crawler(self, name: str):
-         """启动指定爬虫"""
-         if name not in self.crawlers:
-             raise ValueError(f"爬虫 {name} 不存在")
- 
-         if name in self.crawler_threads and self.crawler_threads[name].is_alive():
-             raise RuntimeError(f"爬虫 {name} 已在运行")
- 
-         # 创建停止标志
-         self.stop_flags[name].clear()
- 
-         # 创建并启动线程
-         thread = threading.Thread(
-             target=self._crawler_worker,
-             args=(name,),
-             daemon=True
-         )
-         self.crawler_threads[name] = thread
-         thread.start()
- 
-         logger.info(f"爬虫 {name} 已启动")
- 
-     def stop_crawler(self, name: str):
-         """停止指定爬虫"""
-         if name not in self.crawlers:
-             raise ValueError(f"爬虫 {name} 不存在")
- 
-         if name not in self.crawler_threads or not self.crawler_threads[name].is_alive():
-             raise RuntimeError(f"爬虫 {name} 未在运行")
- 
-         # 设置停止标志
-         self.stop_flags[name].set()
- 
-         # 等待线程结束
-         self.crawler_threads[name].join()
- 
-         logger.info(f"爬虫 {name} 已停止")
- 
-     def start_all(self):
-         """启动所有爬虫"""
-         for name in self.crawlers:
-             try:
-                 self.start_crawler(name)
+             crawler_name: 爬虫名称
+             
+         Returns:
+             Dict: 爬虫信息字典
+         """
+         # 尝试创建爬虫实例
+         try:
+             crawler = self.factory.create_crawler(crawler_name)
+             if not crawler:
+                 return {
+                     'name': crawler_name,
+                     'status': 'error',
+                     'message': '爬虫创建失败'
+                 }
+             
+             # 获取爬虫信息
+             info = {
+                 'name': crawler_name,
+                 'source': crawler.source,
+                 'class': crawler.__class__.__name__,
+                 'description': crawler.__doc__ or '无描述',
+                 'status': 'available',
+                 'capabilities': crawler.capabilities,
+                 'config': crawler.config
+             }
+             
+             # 获取爬虫统计信息
+             source_db = NewsDatabase(source=crawler_name)
+             today_count = source_db.count_news(days=1)
+             week_count = source_db.count_news(days=7)
+             month_count = source_db.count_news(days=30)
+             total_count = source_db.count_news()
+             source_db.close()
+             
+             info['statistics'] = {
+                 'today': today_count,
+                 'week': week_count,
+                 'month': month_count,
+                 'total': total_count
+             }
+             
+             return info
+             
          except Exception as e:
-                 logger.error(f"启动爬虫 {name} 失败: {str(e)}")
- 
-     def stop_all(self):
-         """停止所有爬虫"""
-         for name in self.crawlers:
-             try:
-                 self.stop_crawler(name)
-             except Exception as e:
-                 logger.error(f"停止爬虫 {name} 失败: {str(e)}")
- 
-     def _crawler_worker(self, name: str):
-         """爬虫工作线程"""
-         crawler = self.crawlers[name]
-         stop_flag = self.stop_flags[name]
- 
-         # 为线程创建新的数据库连接
-         thread_db = NewsDatabase(use_all_dbs=True)
- 
-         while not stop_flag.is_set():
-             try:
-                 # 更新爬虫状态
-                 crawler.status = 'running'
-                 crawler.last_run = datetime.now()
- 
-                 # 执行爬取
-                 news_list = crawler.crawl()
- 
-                 # 保存数据
-                 for news in news_list:
-                     try:
-                         # 确保新闻包含正确的来源
-                         if 'source' not in news or not news['source']:
-                             news['source'] = crawler.source
-                         thread_db.add_news(news)
-                     except Exception as e:
-                         logger.error(f"保存新闻失败: {str(e)}")
+             logger.error(f"获取爬虫信息失败 {crawler_name}: {e}")
+             return {
+                 'name': crawler_name,
+                 'status': 'error',
+                 'message': str(e)
+             }
+     
+     def get_all_crawler_info(self) -> List[Dict[str, Any]]:
+         """
+         获取所有爬虫的信息
+         
+         Returns:
+             List[Dict]: 爬虫信息列表
+         """
+         crawler_list = []
+         
+         # 获取所有可用爬虫
+         available_crawlers = self.get_available_crawlers()
          
-                 # 更新爬虫状态
-                 crawler.status = 'idle'
-                 crawler.next_run = datetime.now() + timedelta(minutes=self.settings['crawler_interval'])
-                 crawler.success_count += 1
- 
-                 # 等待下一次运行
-                 while not stop_flag.is_set():
-                     if datetime.now() >= crawler.next_run:
-                         break
-                     time.sleep(1)
- 
-             except Exception as e:
-                 logger.error(f"爬虫 {name} 运行出错: {str(e)}")
-                 crawler.status = 'error'
-                 crawler.error_count += 1
-                 time.sleep(60)  # 出错后等待1分钟再重试
- 
-     def get_status(self) -> Dict:
-         """获取爬虫状态"""
-         status = {}
-         for name, crawler in self.crawlers.items():
-             status[name] = {
-                 'name': name,
-                 'display_name': crawler.source,
-                 'status': getattr(crawler, 'status', 'unknown'),
-                 'last_run': getattr(crawler, 'last_run', None).strftime('%Y-%m-%d %H:%M:%S') if getattr(crawler, 'last_run', None) else None,
-                 'next_run': getattr(crawler, 'next_run', None).strftime('%Y-%m-%d %H:%M:%S') if getattr(crawler, 'next_run', None) else None,
-                 'error_count': getattr(crawler, 'error_count', 0),
-                 'success_count': getattr(crawler, 'success_count', 0)
-             }
-         return status
- 
-     def close(self):
-         """关闭爬虫管理器"""
-         self.stop_all()
-         self.db.close()
- 
-     def run_crawler(self, crawler_name: str, **params) -> int:
-         """运行指定爬虫
+         # 获取每个爬虫的信息
+         for crawler_name in available_crawlers:
+             info = self.get_crawler_info(crawler_name)
+             crawler_list.append(info)
+         
+         return crawler_list
+     
+     def run_crawler(self, crawler_name: str, days: float = 7.0, 
+                    force_update: bool = False, 
+                    use_dedicated_db: bool = True,
+                    sync_to_main: bool = True) -> int:
+         """
+         运行指定的爬虫
          
          Args:
              crawler_name: 爬虫名称
-             **params: 爬虫参数
+             days: 爬取过去几天的新闻
+             force_update: 是否强制更新已存在的新闻
+             use_dedicated_db: 是否使用爬虫专用数据库
+             sync_to_main: 是否同步数据到主数据库
              
          Returns:
-             int: 获取的新闻数量
+             int: 成功爬取的新闻数量
          """
-         if crawler_name not in self.crawlers:
-             logger.error(f"未找到爬虫: {crawler_name}")
+         # 获取缓存中的爬虫，如果没有则创建新的
+         if crawler_name in self.crawlers_cache:
+             crawler = self.crawlers_cache[crawler_name]
+         else:
+             # 如果使用专用数据库，则传入对应数据库路径
+             db_path = None
+             if use_dedicated_db:
+                 db = NewsDatabase(source=crawler_name)
+                 db_path = db.db_path
+                 db.close()
+             
+             # 创建爬虫实例
+             crawler = self.factory.create_crawler(
+                 crawler_name, 
+                 db_path=db_path
+             )
+             
+             # 缓存爬虫实例
+             self.crawlers_cache[crawler_name] = crawler
+         
+         if not crawler:
+             logger.error(f"爬虫 {crawler_name} 创建失败")
              return 0
          
-         crawler = self.crawlers[crawler_name]
- 
+         try:
              # 运行爬虫
-         try:
-             logger.info(f"开始运行爬虫: {crawler_name}")
-             
-             # 提取days参数
-             days = params.get('days', 1)
-             
-             # 只传递days参数给爬虫的crawl方法
-             result = crawler.crawl(days=days)
-             logger.info(f"爬虫 {crawler_name} 运行完成，共获取 {len(result)} 条新闻")
+             logger.info(f"开始运行爬虫: {crawler_name}, 爬取时间范围: {days}天")
+             count = crawler.crawl(days=days, force_update=force_update)
+             logger.info(f"爬虫 {crawler_name} 运行完成，获取 {count} 条新闻")
              
-             # 创建新的数据库连接保存数据
-             from app.db.sqlite_manager import SQLiteManager
-             from app.config import get_settings
-             
-             # 获取数据库路径 - 确保使用主数据库
-             settings = get_settings()
-             db_dir = settings.get('DB_DIR', os.path.join(os.getcwd(), 'data', 'db'))
-             
-             # 1. 首先保存到源特定数据库（如果需要）
-             source_db_path = os.path.join(db_dir, f"{crawler.source}.db")
-             if os.path.exists(source_db_path):
-                 try:
-                     with SQLiteManager(db_path=source_db_path) as db:
-                         for news in result:
-                             # 确保新闻包含正确的来源
-                             if 'source' not in news or not news['source']:
-                                 news['source'] = crawler.source
-                                 
-                             # 调用保存方法
-                             self._prepare_and_save_news(db, news, crawler.source)
+             # 如果使用专用数据库且需要同步到主数据库
+             if use_dedicated_db and sync_to_main and count > 0:
+                 self._sync_to_main_db(crawler_name)
+             
+             return count
+         except Exception as e:
+             logger.error(f"爬虫 {crawler_name} 运行失败: {e}")
+             return 0
+     
+     def _sync_to_main_db(self, source: str) -> int:
+         """
+         将源数据库的数据同步到主数据库
+         
+         Args:
+             source: 数据源名称
+             
+         Returns:
+             int: 同步的记录数
+         """
+         # 创建源数据库连接
+         source_db = NewsDatabase(source=source)
+         
+         try:
+             # 获取源数据库中尚未同步到主数据库的数据
+             news_list = source_db.get_unsynchronized_news()
+             if not news_list:
+                 logger.info(f"数据源 {source} 没有需要同步的数据")
+                 return 0
+             
+             logger.info(f"开始同步数据源 {source} 到主数据库，共 {len(news_list)} 条记录")
+             
+             # 添加到主数据库
+             for news in news_list:
+                 try:
+                     # 设置同步标记
+                     news.synchronized = True
+                     # 添加到主数据库
+                     self.db.add_news(news)
                  except Exception as e:
-                     logger.error(f"保存到源特定数据库失败: {str(e)}")
+                     logger.error(f"同步记录失败 {news.id}: {e}")
              
-             # 2. 然后保存到主数据库（finance_news.db）
-             main_db_path = os.path.join(db_dir, 'finance_news.db')
-             
-             # 确保目录存在
-             os.makedirs(os.path.dirname(main_db_path), exist_ok=True)
-             
-             # 保存爬取结果到主数据库
-             saved_count = 0
-             with SQLiteManager(db_path=main_db_path) as db:
-                 for news in result:
-                     try:
-                         # 确保新闻包含正确的来源
-                         if 'source' not in news or not news['source']:
-                             news['source'] = crawler.source
-                         
-                         # 调用保存方法
-                         if self._prepare_and_save_news(db, news, crawler.source):
-                             saved_count += 1
-                             
-                     except Exception as e:
-                         logger.error(f"保存新闻到主数据库失败: {str(e)}")
-             
-             # 更新爬虫状态
-             crawler.success_count += 1
-             logger.info(f"成功保存 {saved_count} 条新闻到主数据库")
-             return saved_count
+             # 更新源数据库中的同步标记
+             source_db.mark_as_synchronized(news_list)
+             
+             logger.info(f"数据源 {source} 同步完成，共同步 {len(news_list)} 条记录")
+             return len(news_list)
+             
          except Exception as e:
-             logger.error(f"运行爬虫 {crawler_name} 失败: {e}")
-             import traceback
-             logger.error(traceback.format_exc())
-             crawler.error_count += 1
+             logger.error(f"同步数据源 {source} 失败: {e}")
              return 0
+         finally:
+             # 关闭源数据库连接
+             source_db.close()
      
-     def _prepare_and_save_news(self, db, news, source_name):
-         """准备和保存新闻数据
-         
-         Args:
-             db: 数据库连接
-             news: 新闻数据
-             source_name: 爬虫来源
+     def sync_all_to_main(self) -> Dict[str, int]:
+         """
+         将所有数据源的数据同步到主数据库
          
          Returns:
-             bool: 是否成功保存
-         """
-         # 确保新闻有ID
-         if 'id' not in news or not news['id']:
-             # 使用URL的哈希作为ID
-             import hashlib
-             if 'url' in news and news['url']:
-                 news['id'] = hashlib.md5(news['url'].encode('utf-8')).hexdigest()
-             else:
-                 # 如果没有URL，使用标题和来源生成ID
-                 title_source = f"{news.get('title', '')}_{news.get('source', '')}"
-                 news['id'] = hashlib.md5(title_source.encode('utf-8')).hexdigest()
-         
-         # 确保pub_time格式正确
-         if 'pub_time' in news and news['pub_time']:
-             try:
-                 # 尝试解析日期，确保格式一致
-                 from datetime import datetime
-                 if isinstance(news['pub_time'], str):
-                     # 尝试将常见日期格式转换为标准格式
-                     formats = [
-                         '%Y-%m-%d %H:%M:%S',
-                         '%Y/%m/%d %H:%M:%S',
-                         '%Y年%m月%d日 %H:%M:%S',
-                         '%Y-%m-%d',
-                         '%Y/%m/%d',
-                         '%Y年%m月%d日'
-                     ]
-                     
-                     for fmt in formats:
+             Dict[str, int]: 每个数据源同步的记录数
+         """
+         result = {}
+         
+         # 获取所有数据源
+         sources = self.db.get_available_sources()
+         logger.info(f"开始同步所有数据源到主数据库，共 {len(sources)} 个数据源")
+         
+         # 同步每个数据源
+         for source in sources:
              try:
-                             dt = datetime.strptime(news['pub_time'], fmt)
-                             news['pub_time'] = dt.strftime('%Y-%m-%d %H:%M:%S')
-                             break
-                         except ValueError:
-                             continue
+                 count = self._sync_to_main_db(source)
+                 result[source] = count
              except Exception as e:
-                 logger.warning(f"日期格式转换失败: {str(e)}")
-                 # 使用当前时间
-                 news['pub_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
-         else:
-             # 如果没有发布时间，使用当前时间
-             news['pub_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
-         
-         # 设置爬取时间为当前时间（确保是最新的）
-         news['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
-         
-         # 处理图片数据，确保格式正确
-         if 'images' in news:
-             # 转换图片数据为数据库可接受的格式
-             if isinstance(news['images'], list):
-                 if news['images'] and isinstance(news['images'][0], dict):
-                     # 如果是字典列表，提取url字段
-                     image_urls = [img.get('url', '') for img in news['images'] 
-                                  if isinstance(img, dict) and 'url' in img]
-                     news['images'] = ','.join(image_urls)
-                 else:
-                     # 如果是字符串列表，直接连接
-                     news['images'] = ','.join(str(img) for img in news['images'])
-             elif not isinstance(news['images'], str):
-                 news['images'] = str(news['images'])
-         
-         # 处理关键词数据，确保格式正确
-         if 'keywords' in news and news['keywords']:
-             if isinstance(news['keywords'], list):
-                 # 过滤空值并转换为字符串
-                 keywords = [k for k in news['keywords'] if k]
-                 news['keywords'] = ','.join(keywords)
-             elif not isinstance(news['keywords'], str):
-                 news['keywords'] = str(news['keywords'])
-         
-         # 处理相关股票数据，确保格式正确
-         if 'related_stocks' in news and news['related_stocks']:
-             if isinstance(news['related_stocks'], list):
-                 # 过滤空值并转换为字符串
-                 stocks = [s for s in news['related_stocks'] if s]
-                 news['related_stocks'] = ','.join(stocks)
-             elif not isinstance(news['related_stocks'], str):
-                 news['related_stocks'] = str(news['related_stocks'])
-         
-         # 保存到数据库
-         return db.save_news(news)
- 
-     def set_proxy(self, proxy_type: str, proxy_url: str):
-         """设置所有爬虫的代理
- 
-         Args:
-             proxy_type: 代理类型，如http, socks5
-             proxy_url: 代理URL
-         """
-         for crawler in self.crawlers.values():
-             if hasattr(crawler, 'proxy_type'):
-                 crawler.proxy_type = proxy_type
-             if hasattr(crawler, 'proxy_url'):
-                 crawler.proxy_url = proxy_url
-             crawler.use_proxy = True
+                 logger.error(f"同步数据源 {source} 失败: {e}")
+                 result[source] = 0
+         
+         logger.info(f"所有数据源同步完成，结果: {result}")
+         return result
+     
+     def close(self):
+         """关闭爬虫管理器，清理资源"""
+         # 关闭所有爬虫实例
+         for crawler_name, crawler in self.crawlers_cache.items():
+             try:
+                 crawler.close()
+             except Exception as e:
+                 logger.error(f"关闭爬虫 {crawler_name} 失败: {e}")
+         
+         # 清空爬虫缓存
+         self.crawlers_cache.clear()
+         
+         # 关闭数据库连接
+         self.db.close()
+         
+         logger.info("爬虫管理器已关闭")
  
```

---

```diff
  import yaml
  import logging
- from typing import Dict, List, Any, Optional
+ import sys
+ import importlib
+ import inspect
+ from typing import Dict, List, Any, Optional, Type
  
  from app.utils.logger import get_logger
  from app.crawlers.enhanced_crawler import EnhancedCrawler
  from app.utils.database import NewsDatabase
+ from app.config import get_settings
  
  logger = get_logger('crawler_factory')
  
  class CrawlerFactory:
-     """爬虫工厂类，负责创建和管理不同网站的爬虫实例"""
+     """爬虫工厂类，负责创建各种爬虫实例"""
      
-     def __init__(self, config_dir: Optional[str] = None):
-         """
-         初始化爬虫工厂
+     def __init__(self):
+         """初始化爬虫工厂"""
+         self.settings = get_settings()
+         self.crawlers_path = os.path.join(os.path.dirname(__file__), 'sites')
+         self.crawler_classes = {}  # 缓存爬虫类
+         
+         # 扫描并加载爬虫类
+         self._scan_crawlers()
+     
+     def _scan_crawlers(self):
+         """扫描并加载所有爬虫类"""
+         # 确保爬虫目录存在
+         if not os.path.exists(self.crawlers_path):
+             logger.warning(f"爬虫目录不存在: {self.crawlers_path}")
+             return
+             
+         # 确保爬虫目录可导入
+         crawler_package = 'app.crawlers.sites'
+         
+         # 扫描爬虫目录中的所有.py文件
+         for filename in os.listdir(self.crawlers_path):
+             if filename.endswith('.py') and filename != '__init__.py':
+                 crawler_name = filename[:-3]  # 去掉.py后缀
+                 try:
+                     # 动态导入模块
+                     module_name = f"{crawler_package}.{crawler_name}"
+                     module = importlib.import_module(module_name)
+                     
+                     # 查找模块中的爬虫类（BaseCrawler的子类）
+                     for name, obj in inspect.getmembers(module):
+                         if (inspect.isclass(obj) and 
+                             issubclass(obj, BaseCrawler) and 
+                             obj != BaseCrawler):
+                             # 缓存爬虫类
+                             self.crawler_classes[crawler_name] = obj
+                             logger.info(f"已加载爬虫类: {crawler_name} -> {obj.__name__}")
+                             break
+                 except ImportError as e:
+                     logger.error(f"导入爬虫模块失败 {crawler_name}: {e}")
+                 except Exception as e:
+                     logger.error(f"加载爬虫类失败 {crawler_name}: {e}")
+     
+     def get_crawler_class(self, crawler_name: str) -> Optional[Type[BaseCrawler]]:
+         """
+         获取指定爬虫的类
          
          Args:
-             config_dir: 配置文件目录路径，若为None则使用默认路径
-         """
-         self.config_dir = config_dir or os.path.join('app', 'config', 'crawlers')
-         self.config_cache = {}  # 配置缓存
-         
-         # 确保配置目录存在
-         if not os.path.exists(self.config_dir):
-             os.makedirs(self.config_dir, exist_ok=True)
-             logger.info(f"已创建爬虫配置目录: {self.config_dir}")
-         
-         # 初始化数据库目录
-         settings = self._get_default_config()
-         db_dir = settings.get('DB_DIR', os.path.join('data', 'db'))
+             crawler_name: 爬虫名称
+             
+         Returns:
+             Type[BaseCrawler]: 爬虫类，如果不存在则返回None
+         """
+         # 如果已缓存，直接返回
+         if crawler_name in self.crawler_classes:
+             return self.crawler_classes[crawler_name]
+             
+         # 尝试动态加载
+         try:
+             module_name = f"app.crawlers.sites.{crawler_name}"
+             module = importlib.import_module(module_name)
              
-         # 确保路径是绝对路径
-         if not os.path.isabs(db_dir):
-             db_dir = os.path.join(os.getcwd(), db_dir)
-             
-         # 创建数据库目录结构
-         self.db_dir = db_dir
-         self.sources_dir = os.path.join(db_dir, 'sources')
-         self.main_db_path = os.path.join(db_dir, 'finance_news.db')
-         
-         # 确保目录存在
-         for dir_path in [db_dir, self.sources_dir]:
-             if not os.path.exists(dir_path):
-                 os.makedirs(dir_path, exist_ok=True)
-                 logger.info(f"已创建数据库目录: {dir_path}")
+             # 查找模块中的爬虫类
+             for name, obj in inspect.getmembers(module):
+                 if (inspect.isclass(obj) and 
+                     issubclass(obj, BaseCrawler) and 
+                     obj != BaseCrawler):
+                     # 缓存并返回
+                     self.crawler_classes[crawler_name] = obj
+                     logger.info(f"已加载爬虫类: {crawler_name} -> {obj.__name__}")
+                     return obj
+                     
+             logger.error(f"未在模块 {module_name} 中找到爬虫类")
+             return None
+             
+         except ImportError:
+             logger.error(f"导入爬虫模块失败: {crawler_name}")
+             return None
+         except Exception as e:
+             logger.error(f"加载爬虫类失败 {crawler_name}: {e}")
+             return None
      
-     def create_crawler(self, source: str, db_path: str = None, options: Dict[str, Any] = None) -> BaseCrawler:
+     def create_crawler(self, crawler_name: str, **kwargs) -> Optional[BaseCrawler]:
          """
-         创建指定来源的爬虫实例
+         创建指定爬虫的实例
          
          Args:
-             source: 数据源名称
-             db_path: 数据库路径，如果为None则使用专用路径
-             options: 爬虫选项
+             crawler_name: 爬虫名称
+             **kwargs: 传递给爬虫构造函数的参数
              
          Returns:
-             BaseCrawler: 爬虫实例
+             BaseCrawler: 爬虫实例，如果创建失败则返回None
          """
-         # 检查是否支持该数据源
-         if source not in STRATEGY_MAP:
-             raise ValueError(f"不支持的数据源: {source}。支持的数据源: {list(STRATEGY_MAP.keys())}")
-         
-         # 获取源配置
-         source_config = self._get_source_config(source)
-         
-         # 合并配置和选项
-         crawler_options = source_config.copy()
-         if options:
-             crawler_options.update(options)
-         
-         # 确定使用哪种爬虫类型
-         use_enhanced = crawler_options.pop('use_enhanced', False)
-         
-         # 确保不会重复传递source参数
-         if 'source' in crawler_options:
-             del crawler_options['source']
-         
-         # 从选项中提取爬虫特定的参数
-         crawler_specific_options = {}
-         if 'domain_concurrency' in crawler_options:
-             crawler_specific_options['domain_concurrency'] = crawler_options.pop('domain_concurrency')
-         if 'domain_delay' in crawler_options:
-             crawler_specific_options['domain_delay'] = crawler_options.pop('domain_delay')
-         if 'concurrency' in crawler_options:
-             crawler_options['max_concurrency'] = crawler_options.pop('concurrency')
-         
-         # 确定数据库路径
-         if db_path is None:
-             # 规范化源名称
-             safe_source_name = source.replace(' ', '_').replace('/', '_')
-             # 使用源专用数据库
-             db_path = os.path.join(self.sources_dir, f"{safe_source_name}.db")
-             logger.info(f"将为爬虫 {source} 使用专用数据库: {db_path}")
-         
-         # 添加数据同步选项
-         sync_to_main = crawler_options.pop('sync_to_main', True)
+         # 获取爬虫类
+         crawler_class = self.get_crawler_class(crawler_name)
+         if not crawler_class:
+             logger.error(f"未找到爬虫类: {crawler_name}")
+             return None
              
          # 创建爬虫实例
-         if use_enhanced:
-             logger.info(f"创建增强型爬虫实例: {source}, 数据库: {db_path}")
-             # 合并特定选项
-             crawler_options.update(crawler_specific_options)
-             crawler = EnhancedCrawler(source, db_path, **crawler_options)
-         else:
-             logger.info(f"创建标准爬虫实例: {source}, 数据库: {db_path}")
-             crawler = BaseCrawler(source, db_path, **crawler_options)
-         
-         # 设置同步回调
-         if sync_to_main:
-             # 添加同步到主数据库的回调
-             original_crawl = crawler.crawl
-             
-             def crawl_and_sync(*args, **kwargs):
-                 # 调用原始爬取方法
-                 result = original_crawl(*args, **kwargs)
-                 
-                 # 爬取完成后同步到主数据库
-                 try:
-                     self._sync_to_main_db(source)
+         try:
+             # 实例化爬虫
+             crawler = crawler_class(**kwargs)
+             
+             # 确保爬虫source属性被正确设置
+             if hasattr(crawler, 'source') and not crawler.source:
+                 crawler.source = crawler_name
+                 
+             return crawler
          except Exception as e:
-                     logger.error(f"同步 {source} 到主数据库失败: {str(e)}")
-                 
-                 return result
-                 
-             # 替换爬取方法
-             crawler.crawl = crawl_and_sync
-             
-         return crawler
-     
-     def _sync_to_main_db(self, source: str) -> int:
+             logger.error(f"创建爬虫实例失败 {crawler_name}: {e}")
+             return None
+     
+     def reload_crawler(self, crawler_name: str) -> bool:
          """
-         将指定来源的数据同步到主数据库
+         重新加载指定爬虫的类
          
          Args:
-             source: 数据源名称
+             crawler_name: 爬虫名称
              
          Returns:
-             int: 同步的记录数
+             bool: 重新加载是否成功
          """
          try:
-             # 初始化数据库连接
-             db = NewsDatabase()
-             
-             # 执行同步
-             count = db.sync_to_main_db(source)
-             
-             # 关闭数据库连接
-             db.close()
-             
-             logger.info(f"成功将 {count} 条记录从 {source} 同步到主数据库")
-             return count
+             # 清除缓存
+             if crawler_name in self.crawler_classes:
+                 del self.crawler_classes[crawler_name]
+                 
+             # 重新加载模块
+             module_name = f"app.crawlers.sites.{crawler_name}"
+             if module_name in sys.modules:
+                 # 重新加载模块
+                 module = importlib.reload(sys.modules[module_name])
+             else:
+                 # 首次加载模块
+                 module = importlib.import_module(module_name)
+                 
+             # 查找爬虫类
+             for name, obj in inspect.getmembers(module):
+                 if (inspect.isclass(obj) and 
+                     issubclass(obj, BaseCrawler) and 
+                     obj != BaseCrawler):
+                     # 缓存爬虫类
+                     self.crawler_classes[crawler_name] = obj
+                     logger.info(f"已重新加载爬虫类: {crawler_name} -> {obj.__name__}")
+                     return True
+                     
+             logger.error(f"重新加载后未找到爬虫类: {crawler_name}")
+             return False
+             
          except Exception as e:
-             logger.error(f"同步数据库失败: {str(e)}")
-             raise
+             logger.error(f"重新加载爬虫类失败 {crawler_name}: {e}")
+             return False
      
      def create_enhanced_crawler(self, source: str, db_path: str = None, options: Dict[str, Any] = None) -> EnhancedCrawler:
```

---

Read file: app/crawlers/base.py

---

```diff
          # 从配置中获取设置
          self.settings = get_settings()
+         
+         # 爬虫统计信息
+         self.stats = {
+             'total_articles': 0,
+             'successful_crawls': 0,
+             'failed_crawls': 0,
+             'last_crawl_time': None,
+             'last_crawl_articles': 0,
+             'created_at': datetime.now().isoformat(),
+         }
          
          # 如果提供了db_manager，直接使用
              logger.error(f"保存图片失败: {img_url}, 错误: {str(e)}")
              return None
+ 
+     def get_info(self) -> Dict[str, Any]:
+         """
+         获取爬虫信息和统计数据
+         
+         Returns:
+             Dict[str, Any]: 爬虫信息字典
+         """
+         # 获取数据库中的文章数量
+         try:
+             article_count = self.get_news_count()
+             self.stats['total_articles'] = article_count
+         except Exception as e:
+             logger.error(f"获取文章数量失败: {str(e)}")
+         
+         # 构建爬虫信息
+         info = {
+             'source': self.source,
+             'crawler_type': self.__class__.__name__,
+             'capabilities': {
+                 'use_selenium': hasattr(self, 'use_selenium') and self.use_selenium,
+                 'use_proxy': hasattr(self, 'use_proxy') and self.use_proxy,
+             },
+             'db_path': self.db_path,
+             'stats': self.stats
+         }
+         
+         # 添加自定义属性（子类可以覆盖这个方法以添加更多信息）
+         custom_info = self._get_custom_info()
+         if custom_info:
+             info.update(custom_info)
+         
+         return info
+ 
+     def _get_custom_info(self) -> Dict[str, Any]:
+         """
+         获取爬虫的自定义信息，子类可覆盖此方法
+         
+         Returns:
+             Dict[str, Any]: 自定义爬虫信息
+         """
+         return {}
+ 
+     def update_stats(self, articles_count: int, success: bool = True):
+         """
+         更新爬虫统计信息
+         
+         Args:
+             articles_count: 本次爬取的文章数量
+             success: 爬取是否成功
+         """
+         self.stats['last_crawl_time'] = datetime.now().isoformat()
+         self.stats['last_crawl_articles'] = articles_count
+         
+         if success:
+             self.stats['successful_crawls'] += 1
+         else:
+             self.stats['failed_crawls'] += 1
  
```

---

```diff
          self.crawlers_path = os.path.join(os.path.dirname(__file__), 'sites')
          self.crawler_classes = {}  # 缓存爬虫类
+         self.crawler_instances = {}  # 缓存爬虫实例
          
          # 扫描并加载爬虫类
                          if (inspect.isclass(obj) and 
                              issubclass(obj, BaseCrawler) and 
-                             obj != BaseCrawler):
+                             obj != BaseCrawler and
+                             hasattr(obj, 'source')):
                              # 缓存爬虫类
                              self.crawler_classes[crawler_name] = obj
                  if (inspect.isclass(obj) and 
                      issubclass(obj, BaseCrawler) and 
-                     obj != BaseCrawler):
+                     obj != BaseCrawler and
+                     getattr(obj, 'source', None) == crawler_name):
                      # 缓存并返回
                      self.crawler_classes[crawler_name] = obj
              # 实例化爬虫
              crawler = crawler_class(**kwargs)
+             self.crawler_instances[crawler_name] = crawler
              
              # 确保爬虫source属性被正确设置
              if crawler_name in self.crawler_classes:
                  del self.crawler_classes[crawler_name]
+             if crawler_name in self.crawler_instances:
+                 del self.crawler_instances[crawler_name]
                  
              # 重新加载模块
                  if (inspect.isclass(obj) and 
                      issubclass(obj, BaseCrawler) and 
-                     obj != BaseCrawler):
+                     obj != BaseCrawler and
+                     getattr(obj, 'source', None) == crawler_name):
                      # 缓存爬虫类
                      self.crawler_classes[crawler_name] = obj
              logger.error(f"保存配置文件 {config_path} 失败: {str(e)}")
              return False
+     
+     def get_crawler_list(self) -> List[Dict[str, Any]]:
+         """
+         获取所有可用爬虫的简要信息列表
+         
+         Returns:
+             List[Dict[str, Any]]: 爬虫信息列表
+         """
+         result = []
+         
+         # 确保所有爬虫都已扫描
+         self._scan_crawlers()
+         
+         for source, crawler_class in self.crawler_classes.items():
+             info = {
+                 'source': source,
+                 'name': crawler_class.__name__,
+                 'module': crawler_class.__module__,
+                 'description': crawler_class.__doc__ or "无描述"
+             }
+             result.append(info)
+             
+         return result
+     
+     def get_crawler_info(self, source: str) -> Optional[Dict[str, Any]]:
+         """
+         获取指定爬虫的详细信息
+         
+         Args:
+             source: 爬虫源名称
+             
+         Returns:
+             Dict[str, Any]: 爬虫详细信息
+         """
+         # 获取或创建爬虫实例
+         crawler = self.crawler_instances.get(source)
+         if not crawler:
+             crawler = self.create_crawler(source)
+             
+         if not crawler:
+             return None
+             
+         try:
+             # 获取爬虫信息
+             return crawler.get_info()
+         except Exception as e:
+             logger.error(f"获取爬虫信息失败: {source}, 错误: {str(e)}")
+             return {
+                 'source': source,
+                 'error': str(e),
+                 'crawler_type': crawler.__class__.__name__
+             }
+     
+     def get_all_crawler_info(self) -> Dict[str, Dict[str, Any]]:
+         """
+         获取所有爬虫的详细信息
+         
+         Returns:
+             Dict[str, Dict[str, Any]]: 所有爬虫的详细信息
+         """
+         result = {}
+         
+         # 确保所有爬虫都已扫描
+         self._scan_crawlers()
+         
+         for source in self.crawler_classes.keys():
+             info = self.get_crawler_info(source)
+             if info:
+                 result[source] = info
+                 
+         return result
```

---

<details>
            <summary>Grep search for "BaseCrawler" • **30** files</summary>
        
| File | Line | Match |
|------|------|-------|
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/%E7%88%AC%E8%99%AB%E8%BF%81%E7%A7%BB%E6%80%BB%E7%BB%93.md` | L18 | `1. **基类抽象**：创建`BaseCrawler`基类，封装通用功能` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/%E7%88%AC%E8%99%AB%E8%BF%81%E7%A7%BB%E6%80%BB%E7%BB%93.md` | L32 | `- 创建了`BaseCrawler`基类，封装了通用功能如HTTP请求、数据存储、代理使用等` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/%E7%88%AC%E8%99%AB%E8%BF%81%E7%A7%BB%E6%80%BB%E7%BB%93.md` | L78 | `- 所有爬虫类都继承自`BaseCrawler`基类` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/README.md` | L169 | `2. 在文件中定义爬虫类，继承自`BaseCrawler`` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/README.md` | L176 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/README.md` | L178 | `class ExampleCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/docs/user/faq.md` | L88 | `2. 继承`BaseCrawler`类并实现必要的方法` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/docs/user/usage.md` | L136 | `如果您想开发自己的爬虫，可以通过继承 `BaseCrawler` 类来实现：` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/docs/user/usage.md` | L139 | `2. 导入 `BaseCrawler` 类` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/docs/user/usage.md` | L145 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/docs/user/usage.md` | L147 | `class MyCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/manager.py` | L15 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/manager.py` | L29 | `self.crawlers: Dict[str, BaseCrawler] = {}` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/manager.py` | L55 | `def register_crawler(self, name: str, crawler: BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/manager.py` | L60 | `def get_crawlers(self) -> List[BaseCrawler]:` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/manager.py` | L68 | `def get_crawler(self, name: str) -> Optional[BaseCrawler]:` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/ifeng.py` | L18 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/ifeng.py` | L25 | `class IfengCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/sina.py` | L18 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/sina.py` | L25 | `class SinaCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/netease.py` | L18 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/netease.py` | L25 | `class NeteaseCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/__init__.py` | L8 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/__init__.py` | L37 | `BaseCrawler: 爬虫实例` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/tencent.py` | L18 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/tencent.py` | L25 | `class TencentCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/eastmoney.py` | L20 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/eastmoney.py` | L27 | `class EastMoneyCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/base.py` | L31 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/test_declarative_crawler.py` | L28 | `from app.crawlers.core import BaseCrawler, DeclarativeCrawlerStrategy` |
| `file:///d%3A/Git/Github/NewsLook/test_declarative_crawler.py` | L78 | `crawler = BaseCrawler(source, db_path, strategy, **options)` |
| `file:///d%3A/Git/Github/NewsLook/tests/crawlers/test_crawler_base.py` | L19 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/README.md` | L190 | `- `BaseCrawler`：爬虫基类，定义通用爬取流程和接口` |
| `file:///d%3A/Git/Github/NewsLook/README.md` | L192 | `- `BaseCrawlerStrategy`：爬虫策略基类，定义站点特定的爬取规则` |
| `file:///d%3A/Git/Github/NewsLook/README.md` | L205 | `1. 在`app/crawlers/strategies/`目录下创建新的策略类，继承`BaseCrawlerStrategy`：` |
| `file:///d%3A/Git/Github/NewsLook/README.md` | L208 | `from app.crawlers.strategies.base_strategy import BaseCrawlerStrategy` |
| `file:///d%3A/Git/Github/NewsLook/README.md` | L210 | `class YourStrategy(BaseCrawlerStrategy):` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L68025 | `2025-04-24 21:06:51,886 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L68026 | `2025-04-24 21:06:51,886 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L68029 | `2025-04-24 21:06:51,886 [INFO] [NewsLook] 准备使用同步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L68030 | `2025-04-24 21:06:51,886 [INFO] [NewsLook] 准备使用同步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L69280 | `2025-04-24 21:09:16,115 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L69281 | `2025-04-24 21:09:16,115 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L69284 | `2025-04-24 21:09:16,116 [INFO] [NewsLook] 准备使用同步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L69285 | `2025-04-24 21:09:16,116 [INFO] [NewsLook] 准备使用同步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L69997 | `crawler = BaseCrawler(source, db_path, **crawler_options)` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70018 | `crawler = BaseCrawler(source, db_path, **crawler_options)` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70205 | `crawler = BaseCrawler(source, db_path, **crawler_options)` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70226 | `crawler = BaseCrawler(source, db_path, **crawler_options)` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70277 | `2025-04-24 21:10:18,131 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70278 | `2025-04-24 21:10:18,131 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70281 | `2025-04-24 21:10:18,131 [INFO] [NewsLook] 准备使用异步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70282 | `2025-04-24 21:10:18,131 [INFO] [NewsLook] 准备使用异步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70296 | `2025-04-24 21:10:18,132 [ERROR] [asyncio] Exception in callback BaseCrawler._get_urls_to_crawl.<locals>.save_result() at D:\Git\Github\NewsLook\app\crawlers\core\base_crawler.py:579` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70297 | `handle: <Handle BaseCrawler._get_urls_to_crawl.<locals>.save_result() at D:\Git\Github\NewsLook\app\crawlers\core\base_crawler.py:579>` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70351 | `2025-04-24 21:10:37,168 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70352 | `2025-04-24 21:10:37,168 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70355 | `2025-04-24 21:10:37,168 [INFO] [NewsLook] 准备使用异步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70356 | `2025-04-24 21:10:37,168 [INFO] [NewsLook] 准备使用异步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70370 | `2025-04-24 21:10:37,170 [ERROR] [asyncio] Exception in callback BaseCrawler._get_urls_to_crawl.<locals>.save_result() at D:\Git\Github\NewsLook\app\crawlers\core\base_crawler.py:579` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70371 | `handle: <Handle BaseCrawler._get_urls_to_crawl.<locals>.save_result() at D:\Git\Github\NewsLook\app\crawlers\core\base_crawler.py:579>` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70425 | `2025-04-24 21:10:38,759 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70426 | `2025-04-24 21:10:38,759 [INFO] [NewsLook] 爬虫类型: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70429 | `2025-04-24 21:10:38,759 [INFO] [NewsLook] 准备使用异步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70430 | `2025-04-24 21:10:38,759 [INFO] [NewsLook] 准备使用异步方法爬取，爬虫类: BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70444 | `2025-04-24 21:10:38,761 [ERROR] [asyncio] Exception in callback BaseCrawler._get_urls_to_crawl.<locals>.save_result() at D:\Git\Github\NewsLook\app\crawlers\core\base_crawler.py:579` |
| `file:///d%3A/Git/Github/NewsLook/logs/finance_news_crawler.log` | L70445 | `handle: <Handle BaseCrawler._get_urls_to_crawl.<locals>.save_result() at D:\Git\Github\NewsLook\app\crawlers\core\base_crawler.py:579>` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L875 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L877 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L880 | `File "D:\Git\Github\NewsLook\app\crawlers\base.py", line 760, in BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L900 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L902 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L905 | `File "D:\Git\Github\NewsLook\app\crawlers\base.py", line 760, in BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L915 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L917 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L920 | `File "D:\Git\Github\NewsLook\app\crawlers\base.py", line 760, in BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L940 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L942 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L945 | `File "D:\Git\Github\NewsLook\app\crawlers\base.py", line 760, in BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/eastmoney_simple.py` | L20 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/eastmoney_simple.py` | L26 | `class EastMoneySimpleCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/ifeng.py` | L18 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/ifeng.py` | L28 | `class IfengCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/netease.py` | L18 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/netease.py` | L28 | `class NeteaseCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/tencent.py` | L19 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/tencent.py` | L29 | `class TencentCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/__init__.py` | L8 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/__init__.py` | L38 | `BaseCrawler: 爬虫实例` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/tencent.py.bak` | L19 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/tencent.py.bak` | L29 | `class TencentCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/sina.py` | L20 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/sina.py` | L30 | `class SinaCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/manager.py` | L15 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/manager.py` | L243 | `def get_crawlers(self) -> List[BaseCrawler]:` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/manager.py` | L251 | `def get_crawler(self, name: str) -> Optional[BaseCrawler]:` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/base.py` | L33 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/eastmoney.py` | L20 | `from app.crawlers.base import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/eastmoney.py` | L28 | `class EastMoneyCrawler(BaseCrawler):` |
| `file:///d%3A/Git/Github/NewsLook/docs/database/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E8%AF%B4%E6%98%8E.md` | L79 | `所有爬虫类都继承自`BaseCrawler`，并通过以下方式与数据库交互：` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L18 | `from app.crawlers.core import BaseCrawler` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L58 | `# 查找模块中的爬虫类（BaseCrawler的子类）` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L61 | `issubclass(obj, BaseCrawler) and` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L62 | `obj != BaseCrawler and` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L73 | `def get_crawler_class(self, crawler_name: str) -> Optional[Type[BaseCrawler]]:` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L81 | `Type[BaseCrawler]: 爬虫类，如果不存在则返回None` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L95 | `issubclass(obj, BaseCrawler) and` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L96 | `obj != BaseCrawler and` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L113 | `def create_crawler(self, crawler_name: str, **kwargs) -> Optional[BaseCrawler]:` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L122 | `BaseCrawler: 爬虫实例，如果创建失败则返回None` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L174 | `issubclass(obj, BaseCrawler) and` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L175 | `obj != BaseCrawler and` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L239 | `def create_all_crawlers(self, db_dir: str = None, options: Dict[str, Any] = None) -> Dict[str, BaseCrawler]:` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/factory.py` | L248 | `Dict[str, BaseCrawler]: {源名称: 爬虫实例}` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/manager.py` | L15 | `from app.crawlers.base import BaseCrawler` |

</details>

---

Read file: app/crawlers/base.py

---

Could not find file 'app/crawlers/core.py'. Did you mean one of:
- app/crawlers/core/__init__.py
- app/crawlers/core/http_client.py
- app/crawlers/core/base_crawler.py

---

Read file: app/crawlers/core/base_crawler.py

---

Read file: app/crawlers/core/http_client.py

---

Read file: app/crawlers/strategies/base_strategy.py

---

基于我对代码库的分析，我将为您提供 NewsLook 项目中 BaseCrawler 类的详细介绍。

# NewsLook 爬虫系统架构分析

项目有两个不同的 BaseCrawler 实现:
1. `app/crawlers/base.py` 中的旧版实现
2. `app/crawlers/core/base_crawler.py` 中的新版实现

## 新版爬虫架构

新版爬虫框架采用了更模块化的设计:

### 核心组件
- **BaseCrawler 类**: 在 `core/base_crawler.py` 中实现，提供统一的爬虫框架
- **HTTP客户端**: 分为 `SyncHttpClient` 和 `AsyncHttpClient`，支持同步和异步请求
- **爬虫策略模式**: 通过 `BaseCrawlerStrategy` 基类定义网站特定的爬取规则

### BaseCrawler 特性
- 初始化时接收数据源名称和数据库路径
- 支持同步和异步操作
- 提供通用爬取流程
- 与 `DataProcessor` 配合处理数据

### 爬虫策略
`BaseCrawlerStrategy` 作为抽象基类定义爬取规则接口:
- 获取列表页 URL
- 解析新闻列表
- 提取新闻内容
- 自定义分类映射

## 使用方法

创建新爬虫时:
1. 继承 `BaseCrawler` 类
2. 实现必要的方法
3. 可以根据需要自定义爬取策略

## 技术栈
- 网络请求: requests (同步)、aiohttp (异步)
- 内容解析: BeautifulSoup
- 浏览器模拟: Selenium
- 数据存储: SQLite
- 其他工具: 情感分析、日志记录、代理管理

这种设计使爬虫系统具有高度扩展性和可维护性，新增网站爬虫只需继承基类并实现特定逻辑即可。

---

_**User**_

查看你修改的那几个文件，是否有模块导入和函数参数等问题，如果有就修复

---

_**Assistant**_

<details>
            <summary>Grep search for "class BaseCrawler" • **6** files</summary>
        
| File | Line | Match |
|------|------|-------|
| `file:///d%3A/Git/Github/NewsLook/_archived_today3_20250324200011/app/crawlers/base.py` | L31 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L877 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L902 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L917 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/logs/crawler_20250418.log` | L942 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/newslook/crawlers/base.py` | L33 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/strategies/base_strategy.py` | L12 | `class BaseCrawlerStrategy(ABC):` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/core/base_crawler.py` | L23 | `class BaseCrawler:` |
| `file:///d%3A/Git/Github/NewsLook/app/crawlers/base.py` | L39 | `class BaseCrawler:` |

</details>

---

Read file: app/crawlers/factory.py

---

Read file: app/crawlers/base.py

---

Read file: app/crawlers/core/base_crawler.py

---

Read file: newslook/crawlers/base.py

---

基于代码分析，我将为您提供BaseCrawler类的摘要：

# NewsLook系统中的BaseCrawler类分析

NewsLook系统实际上有两个主要的BaseCrawler实现：

## 1. app/crawlers/core/base_crawler.py 中的BaseCrawler

这是一个现代化的基础爬虫类，具有以下特点：

- **异步支持**：同时支持同步和异步爬取模式
- **HTTP客户端抽象**：使用`SyncHttpClient`和`AsyncHttpClient`处理请求
- **爬取策略模式**：使用`BaseCrawlerStrategy`接口，支持不同来源的自定义爬取逻辑
- **批量数据处理**：通过`DataProcessor`处理批量数据
- **资源管理**：并发控制和限流功能
- **统计信息**：详细记录爬取结果和性能数据

主要方法：
- `crawl`：爬取新闻的通用入口
- `crawl_sync`：同步模式爬取
- `crawl_async`：异步模式爬取
- `save_news_to_db`：保存新闻到数据库

## 2. newslook/crawlers/base.py 中的BaseCrawler

这是一个较传统的爬虫基类实现，提供了：

- **HTTP请求处理**：提供`fetch_page`方法封装请求逻辑
- **Selenium支持**：支持使用Selenium进行复杂页面爬取
- **数据库集成**：提供数据库连接和操作方法
- **反爬虫功能**：随机User-Agent、Referer和休眠机制
- **代理支持**：可配置使用代理服务

主要方法：
- `fetch_page`：获取页面内容的主要方法
- `fetch_page_with_requests`和`fetch_page_with_selenium`：不同的页面获取实现
- `random_sleep`：防止请求过于频繁
- `save_news`：保存新闻数据到数据库
- `get_news_by_category`、`get_news_by_date_range`、`search_news`：数据查询方法

## 系统架构设计

这两个BaseCrawler可能代表系统的不同版本或用途：
1. `core/base_crawler.py`是更现代的实现，专注于高效爬取和并发处理
2. `newslook/crawlers/base.py`提供更全面的功能，包括数据库管理和防爬虫机制

系统使用工厂模式创建爬虫实例，如`factory.py`中所示，可以根据不同的源创建相应的爬虫对象。

所有特定源的爬虫都应继承BaseCrawler类，并实现自己的爬取逻辑，依赖基类提供的通用功能，如HTTP请求、数据存储和防爬技术。

---



---

