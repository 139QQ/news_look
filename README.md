# 财经新闻爬虫系统 (NewsLook)

一个用于爬取各大财经网站新闻的系统，包括东方财富、新浪财经、腾讯财经、网易财经和凤凰财经等网站。
还有很多功能需要完善

## 功能特点

- 支持多个财经网站的新闻爬取
- 统一的爬虫接口和数据格式
- 完善的错误处理和日志记录
- 灵活的配置选项
- 支持代理使用
- 支持按时间范围爬取新闻
- 自动提取新闻标题、内容、发布时间、作者等信息
- 自动提取相关股票信息
- 支持情感分析
- 每个来源使用独立数据库，支持分布式部署

## 系统架构

```
app/
  ├── crawlers/         # 爬虫模块
  │   ├── __init__.py   # 爬虫包初始化文件，提供获取爬虫实例的接口
  │   ├── base.py       # 爬虫基类，封装通用功能
  │   ├── eastmoney.py  # 东方财富爬虫
  │   ├── sina.py       # 新浪财经爬虫
  │   ├── tencent.py    # 腾讯财经爬虫
  │   ├── netease.py    # 网易财经爬虫
  │   └── ifeng.py      # 凤凰财经爬虫
  ├── web/              # Web应用模块
  │   ├── __init__.py
  │   ├── routes.py     # 路由定义
  │   └── views.py      # 视图函数
  ├── utils/            # 工具模块
  │   ├── logger.py     # 日志工具
  │   ├── text_cleaner.py # 文本清洗工具
  │   ├── proxy.py      # 代理管理
  │   ├── ad_filter.py  # 广告过滤
  │   └── sentiment.py  # 情感分析工具
  ├── db/               # 数据库模块
  │   └── database.py   # 数据库操作
  ├── models/           # 数据模型
  │   └── news.py       # 新闻数据模型
  ├── tasks/            # 定时任务
  │   └── scheduler.py  # 任务调度器
  ├── templates/        # HTML模板
  ├── static/           # 静态资源
  ├── config.py         # 全局配置
  └── __init__.py       # 应用初始化
```

## 安装依赖

```bash
pip install -r requirements.txt
```

## 使用方法

系统提供了统一的命令行入口 `run.py`，支持三种运行模式：爬虫模式、调度模式和Web应用模式。

### 1. 爬虫模式

爬虫模式用于运行爬虫任务，支持多种爬虫类型。

```bash
# 基本使用
python run.py crawler --source <来源名称> --days <天数>

# 爬取腾讯财经最近3天的新闻
python run.py crawler --source 腾讯财经 --days 3

# 使用代理爬取新浪财经最近7天的新闻
python run.py crawler --source 新浪财经 --days 7 --use-proxy
```

### 2. 调度模式

调度模式用于定时执行爬虫任务，可以设置爬虫类型和时间间隔。

```bash
# 启动调度器
python run.py scheduler

# 使用特定配置启动调度器
python run.py scheduler --config config.json
```

### 3. Web应用模式

Web应用模式提供了一个Web界面，可以查看爬取的新闻和爬虫运行状态。

```bash
# 启动Web应用
python run.py web

# 指定端口启动Web应用
python run.py web --port 8080
```

## 数据库架构

系统使用两级数据库架构：

1. **主数据库**: `finance_news.db` - 用于存储聚合的新闻数据
2. **来源专用数据库**: 每个来源对应一个独立的数据库文件，如 `腾讯财经.db`、`新浪财经.db` 等

### 文件命名约定

来源专用数据库使用来源名称作为文件名，例如：
- 腾讯财经 -> `腾讯财经.db`
- 新浪财经 -> `新浪财经.db`
- 东方财富 -> `东方财富.db`
- 网易财经 -> `网易财经.db`
- 凤凰财经 -> `凤凰财经.db`

### 数据库实现细节

#### 1. 为每个爬虫设置专用数据库路径

在 `CrawlerManager` 类中，我们为每个爬虫实例设置了对应的数据库路径：

```python
# 为每个爬虫设置固定的数据库路径
self._set_db_path(ifeng_crawler, "凤凰财经")
self._set_db_path(sina_crawler, "新浪财经")
self._set_db_path(tencent_crawler, "腾讯财经")
self._set_db_path(netease_crawler, "网易财经")
self._set_db_path(eastmoney_crawler, "东方财富")
```

#### 2. 改进 NewsDatabase 类

`NewsDatabase` 类支持以下功能：

- 使用 `source_db_map` 属性，建立来源名称到数据库文件的映射
- 支持按来源名称选择对应的数据库进行查询
- 支持按来源统计新闻数量
- 支持在所有数据库中查找指定ID的新闻
- 从所有数据库中收集来源和分类数据

## 爬虫系统改进

### 存在的问题

1. **代码重复**：各爬虫间存在大量重复代码，如HTTP请求、数据清洗等通用功能
2. **错误处理不完善**：缺乏统一的异常处理机制，导致爬虫在遇到异常时容易崩溃
3. **日志系统不健全**：日志记录不完整，难以追踪问题
4. **配置分散**：配置信息散布在代码中，难以统一管理
5. **扩展性差**：添加新爬虫需要重复实现大量基础功能
6. **数据存储不统一**：数据存储逻辑与爬虫逻辑混合，不利于维护
7. **过度依赖浏览器自动化**：部分爬虫依赖Selenium等浏览器自动化工具，资源消耗大且稳定性差

### 改进方案

1. **基类抽象**：创建`BaseCrawler`基类，封装通用功能
2. **统一接口**：所有爬虫实现统一接口，便于调用和管理
3. **完善错误处理**：添加全面的异常捕获和处理机制
4. **增强日志系统**：实现详细的日志记录，包括调试、信息、警告和错误级别
5. **配置集中化**：将配置信息集中管理，便于修改和维护
6. **模块化设计**：将爬虫、数据处理、存储等功能模块化，降低耦合
7. **增强可扩展性**：便于添加新的爬虫源
8. **减少浏览器依赖**：尽可能使用轻量级请求库代替浏览器自动化

## 项目优化建议

### 文件组织整理

1. **测试文件移动**：将`test_*.py`文件全部移动到`tests`目录
2. **日志文件整理**：根目录下的`.log`文件应该移动到logs目录下对应的子目录
3. **数据库文件移动**：将根目录的`*.db`文件移动到`data`或`db`目录

### 运行脚本统一

1. **统一入口**: 
   - `run.py` - 主程序入口
   - `run_crawler.py` - 爬虫统一入口
   - `run_scheduler.py` - 调度任务入口

2. **CLI模块化**: 创建专门的CLI目录管理命令行接口

## 贡献

欢迎贡献代码或提出改进建议！请提交Issue或Pull Request。

## 许可

本项目使用MIT许可证。请参考LICENSE文件。

## 爬虫模式的主要参数

- `-s, --source`：爬虫来源，支持eastmoney, eastmoney_simple, sina, tencent等
- `-d, --days`：爬取最近几天的新闻，默认为1天
- `-p, --use-proxy`：是否使用代理
- `--db-path`：数据库路径，如不指定则使用默认路径
- `--source-db`：是否使用来源专用数据库
- `--log-level`：日志级别，可选值为DEBUG、INFO、WARNING、ERROR，默认为INFO
- `--log-dir`：日志存储目录，默认为./logs
- `--output-dir`：输出文件目录，默认为./data/output
- `--debug`：是否开启调试模式

### 东方财富网爬虫特有参数

- `--max-news`：每个分类最多爬取多少条新闻，默认10条
- `--categories`：要爬取的新闻分类，默认为财经和股票
- `--delay`：请求间隔时间(秒)，默认5秒
- `--max-retries`：最大重试次数，默认3次
- `--timeout`：请求超时时间(秒)，默认30秒
- `--use-selenium`：是否使用Selenium（仅适用于东方财富网爬虫）

## 爬虫支持的网站

- 东方财富网 (EastmoneyCrawler)
- 新浪财经 (SinaCrawler)
- 腾讯财经 (TencentCrawler)
- 网易财经 (NeteaseCrawler)
- 凤凰财经 (IfengCrawler)

## 详细文档

有关更详细的使用说明，请参阅以下文档：

- [使用统一入口](docs/使用统一入口.md) - 统一入口的详细使用说明
- [脚本整合说明](docs/脚本整合说明.md) - 运行脚本整合的背景和方案
- [数据库连接说明](docs/数据库连接说明.md) - 爬虫与数据库连接的详细说明

## 数据格式

爬取的新闻数据格式如下：

```json
{
    "id": "新闻ID",
    "title": "新闻标题",
    "content": "新闻内容",
    "pub_time": "发布时间",
    "author": "作者",
    "source": "来源",
    "url": "新闻URL",
    "keywords": "关键词",
    "sentiment": "情感倾向",
    "crawl_time": "爬取时间",
    "category": "新闻分类",
    "images": "图片URL",
    "related_stocks": "相关股票"
}
```

## 数据库优化

系统支持智能识别和使用多个数据库文件，确保您可以查看所有历史爬取的新闻数据。

### 多数据库支持功能

- **自动数据库发现**：系统会自动搜索可能的数据库目录，查找并使用所有可用的数据库文件
- **灵活的数据库位置**：支持多个可能的数据库位置，按优先级搜索：
  - `./data/db/` (项目根目录下的data/db目录)
  - `./db/` (项目根目录下的db目录)
  - 当前工作目录下的`data/db/`或`db/`目录
- **命令行参数**：通过`--db-dir`参数显式指定数据库目录
- **合并查询结果**：从多个数据库文件中查询并合并结果，提供完整的数据视图

### 如何使用多数据库功能

系统默认启用多数据库功能。在启动Web应用时，会自动搜索并使用所有可用的数据库文件：

```bash
# 使用自动检测的数据库目录
python run.py web

# 指定数据库目录
python run.py web --db-dir path/to/your/db
```

在控制台输出中，您可以看到系统找到了哪些数据库文件：

```
找到主数据库: /path/to/your/db/finance_news.db
找到 X 个数据库文件
```

### 数据库查找逻辑

1. 首先检查命令行参数`--db-dir`是否指定了数据库目录
2. 如果未指定，检查环境变量`DB_DIR`
3. 如果环境变量未设置，按优先级搜索可能的目录，寻找包含`.db`文件的第一个有效目录
4. 如果所有目录都不存在或不包含数据库文件，创建并使用默认目录

## 开发指南

### 添加新的爬虫

1. 在`app/crawlers/`目录下创建新的爬虫文件，如`example.py`
2. 在文件中定义爬虫类，继承自`BaseCrawler`
3. 实现必要的方法，如`crawl`、`extract_news_links`、`crawl_news_detail`等
4. 在`app/crawlers/__init__.py`中导入新的爬虫类，并添加到`CRAWLER_CLASSES`字典中

### 爬虫类模板

```python
from app.crawlers.base import BaseCrawler

class ExampleCrawler(BaseCrawler):
    """示例爬虫"""
    
    # 新闻分类URL
    CATEGORY_URLS = {
        '财经': 'https://example.com/finance',
        '股票': 'https://example.com/stock',
    }
    
    # 内容选择器
    CONTENT_SELECTOR = 'div.article-content'
    
    # 时间选择器
    TIME_SELECTOR = 'span.time'
    
    # 作者选择器
    AUTHOR_SELECTOR = 'span.author'
    
    def __init__(self, db_path=None, use_proxy=False, use_source_db=False):
        """初始化爬虫"""
        self.source = "示例网站"
        super().__init__(db_path, use_proxy, use_source_db)
    
    def crawl(self, days=1):
        """爬取新闻"""
        # 实现爬取逻辑
        pass
    
    def extract_news_links(self, html, category):
        """提取新闻链接"""
        # 实现提取新闻链接的逻辑
        pass
    
    def crawl_news_detail(self, url, category):
        """爬取新闻详情"""
        # 实现爬取新闻详情的逻辑
        pass
```



# 东方财富网爬虫

## Eastmoney爬虫

东方财富网爬虫是一个高效的财经新闻爬取工具，从各个财经类别中抓取最新的新闻和市场动态。

### 功能特点

- **多类别支持**：支持财经、股票、基金、债券、期货、外汇、黄金等多个金融新闻类别
- **高效请求**：使用requests库直接请求网页，无需浏览器自动化，性能提升300%
- **智能解析**：多级选择器和多种匹配策略，提高内容提取成功率
- **内容清洗**：精准过滤广告和无关内容，保证文本质量
- **情感分析**：对新闻内容进行情感分析，判断正面、负面或中性
- **数据存储**：支持将爬取结果存入数据库和文本文件

### 使用方法

```bash
# 爬取财经类别的新闻（默认）
python run_eastmoney.py

# 爬取指定类别的新闻
python run_eastmoney.py --categories 股票 基金

# 爬取所有类别的新闻
python run_eastmoney.py --categories all

# 指定最大爬取数量
python run_eastmoney.py --max-news 10

# 指定爬取几天内的新闻
python run_eastmoney.py --days 3

# 调试模式
python run_eastmoney.py --debug
```

### 参数说明

| 参数 | 说明 | 默认值 |
|------|------|--------|
| --categories | 要爬取的新闻类别，支持：财经、股票、基金、债券、期货、外汇、黄金 | 财经 |
| --max-news | 每个类别最大爬取的新闻数量 | 20 |
| --days | 爬取几天内的新闻 | 7 |
| --debug | 是否开启调试模式 | False |
| --output-dir | 输出目录 | ./data/output |

### 输出示例

```json
{
  "id": "a1b2c3d4e5f6g7h8i9j0",
  "url": "https://finance.eastmoney.com/a/202501010123456789.html",
  "title": "市场分析：A股有望迎来开门红",
  "content": "分析师认为，随着政策利好不断释放，A股市场有望在新年伊始迎来一波上涨行情...",
  "pub_time": "2025-01-01 08:30:00",
  "author": "东方财富网",
  "keywords": "A股,政策,利好,上涨",
  "sentiment": "正面",
  "category": "股票",
  "crawl_time": "2025-01-01 09:15:30",
  "source": "东方财富网"
}
```

## 数据结构

爬取的新闻数据包含以下字段：

- `id`: 新闻ID（唯一标识）
- `url`: 新闻URL
- `title`: 新闻标题
- `content`: 新闻内容（纯文本）
- `content_html`: 新闻内容（HTML格式）
- `pub_time`: 发布时间
- `author`: 作者
- `keywords`: 关键词
- `images`: 图片URL列表
- `related_stocks`: 相关股票
- `sentiment`: 情感倾向（正面/负面/中性）
- `classification`: 新闻分类
- `crawl_time`: 爬取时间

## 反爬机制

本爬虫实现了多种反爬机制，以应对网站的反爬措施：

1. **随机User-Agent**：每次请求使用不同的浏览器标识
2. **随机Referer**：模拟从不同网站跳转
3. **随机Cookie**：生成随机的Cookie值
4. **请求延迟**：随机延迟请求，避免频繁访问
5. **指数退避策略**：请求失败时，采用指数退避策略增加等待时间
6. **多种请求方式**：支持普通请求、移动端请求和Selenium模拟浏览器
7. **智能重试**：遇到403、429等错误时智能重试
8. **内容检测**：检测响应内容是否包含反爬关键词

### 应对403 Forbidden错误

如果遇到403 Forbidden错误，可以尝试以下方法：

1. 增加请求延迟：`--delay 10`
2. 使用Selenium模拟浏览器：`--use-selenium`
3. 减少爬取数量：`--max-news 20`
4. 使用代理：`--use-proxy`

## 常见问题

### 无法提取发布时间

有些新闻页面的发布时间格式不统一，爬虫会尝试多种方式提取：

1. 从HTML元素中提取
2. 从URL中提取日期信息
3. 如果都失败，则使用当前时间

### 遇到403 Forbidden错误

这通常是因为网站的反爬机制，可以尝试以下解决方法：

1. 使用`--use-selenium`参数启用Selenium模拟浏览器
2. 增加`--delay`参数值，减少请求频率
3. 使用`--use-proxy`参数启用代理

## 开发者文档

主要文件说明：

- `run_eastmoney.py`: 主程序入口
- `app/crawlers/eastmoney.py`: 东方财富网爬虫实现
- `app/crawlers/base.py`: 爬虫基类
- `app/db/sqlite_manager.py`: SQLite数据库管理器

## 许可证

MIT License

## 最新更新

### 2025年3月 - 参数优化与代码精简

我们对系统进行了全面的参数和命名优化，主要改进包括：

- **代码精简**：移除了冗余参数和不必要的命名，减少了代码行数
- **参数结构优化**：统一了命令行参数的结构，提高了一致性
- **启动脚本改进**：简化了启动脚本，使其更加清晰直观
- **日志系统优化**：重构了日志设置代码，减少了重复内容

这些优化使代码更加简洁和可维护，同时也提高了系统的稳定性和性能。详细改进可以查看 `docs/项目优化汇总.md` 文件中的"参数和命名优化"部分。

### 2025年4月 - 编码优化与乱码处理

为解决爬取财经网站时遇到的编码和乱码问题，我们进行了以下关键优化：

- **多重编码处理**：增强了对UTF-8、GBK、GB18030等多种编码的智能识别和转换能力
- **Unicode转义序列解码**：新增`decode_unicode_escape`函数处理类似`\u4e2d\u56fd`的Unicode转义序列
- **URL编码字符解码**：新增`decode_url_encoded`函数处理类似`%E4%B8%AD%E5%9B%BD`的URL编码字符
- **HTML实体解码增强**：扩展了`decode_html_entities`函数的能力，添加更多常见HTML实体的处理
- **编码流程优化**：在爬取、解析和存储过程中实施多阶段编码检测和转换
- **乱码自动修复**：对于特定网站（如新浪财经、腾讯财经）实现了定制化的乱码修复机制

这些优化显著提高了系统处理中文内容的能力，解决了中文乱码问题，特别是在处理财经网站混合编码的情况时。具体实现可查看`app/utils/text_cleaner.py`和`app/crawlers/sina.py`等文件。

### 2025年5月 - 数据库存储优化

为提高数据管理效率和简化系统架构，我们对数据库存储机制进行了以下优化：

- **固定数据库文件名**：改用固定命名格式`[source_name].db`（如`腾讯财经.db`），替代之前的时间戳命名方式
- **直接数据库访问**：爬虫类实现了直接使用sqlite3连接将新闻保存到数据库，减少了中间层次
- **自动表初始化**：系统运行时自动检查并创建所需的数据库表结构，确保数据一致性
- **智能路径处理**：增强了数据库路径处理逻辑，支持多种可能的数据库位置
- **出错自动重试**：优化了数据库操作的错误处理和重试机制

这些优化带来了显著改进：
1. 减少了数据库文件数量，避免了数据库文件爆炸性增长
2. 简化了数据访问流程，网页应用始终能找到最新数据
3. 减少了数据碎片化，提高了查询效率
4. 降低了系统维护难度，数据备份更加便捷

详细说明可参阅 `docs/数据库连接说明.md` 文件中的"数据库文件存储优化"部分。

## 使用说明

## 项目维护

### 修复未知来源问题

有时候系统可能会误抓取一些来源不明确的新闻，导致产生"未知来源"的记录。系统提供了以下方法处理这类问题：

1. **使用管理界面**：在系统管理界面的"系统设置"中，提供了"更新未知来源"功能，可以根据URL特征自动识别并更新正确的来源。

2. **运行修复脚本**：可以直接运行以下命令修复所有数据库中的未知来源记录：
   ```bash
   python -m app.utils.fix_unknown_sources
   ```

修复脚本会执行以下操作：
- 扫描所有数据库中的未知来源记录
- 根据URL特征自动推断并更新正确的来源
- 对于无法自动识别的记录，根据数据库名称设置默认来源
- 删除空的"未知来源"数据库文件
- 创建备份以确保数据安全

为避免产生未知来源记录，爬虫基类已增强源码验证，确保每个爬虫子类在初始化时正确设置`source`属性。爬虫管理器也会在启动时验证每个爬虫的来源设置，确保系统稳定运行。
