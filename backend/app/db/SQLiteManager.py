#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SQLiteÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÊ®°Âùó
"""

import os
import sqlite3
import logging
import json
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
import hashlib
import shutil

logger = logging.getLogger(__name__)

class SQLiteManager:
    """SQLiteÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÁ±ª"""
    
    def __init__(self, db_path: str):
        """
        ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂô® - üîß Âº∫Âà∂Ë∑ØÂæÑÁªü‰∏ÄÂåñ
        
        Args:
            db_path: Êï∞ÊçÆÂ∫ìË∑ØÂæÑ
        """
        # üîß ‰øÆÂ§çÔºöÂº∫Âà∂‰ΩøÁî®Áªü‰∏ÄË∑ØÂæÑÁÆ°ÁêÜÂô®
        from backend.newslook.core.database_path_manager import get_database_path_manager
        
        path_manager = get_database_path_manager()
        # Âº∫Âà∂‰ΩøÁî®Áªü‰∏ÄË∑ØÂæÑÔºåÂøΩÁï•‰º†ÂÖ•ÁöÑdb_path
        self.db_path = path_manager.get_main_db_path()
        self.base_dir = str(path_manager.db_dir)
        # üîß ‰øÆÂ§çÔºö‰∏çÂÜç‰ΩøÁî®sourcesÂàÜÁ¶ªÁõÆÂΩï
        self.backups_dir = str(path_manager.backup_dir)
        
        # Âè™ÂàõÂª∫backupsÁõÆÂΩïÔºå‰∏çÂÜçÂàõÂª∫sourcesÂàÜÁ¶ªÁõÆÂΩï
        os.makedirs(self.backups_dir, exist_ok=True)
        
        logger.info(f"üîß SQLiteManagerÂº∫Âà∂‰ΩøÁî®Áªü‰∏ÄË∑ØÂæÑ: {self.db_path}")
        
        # Main connection
        self.conn: Optional[sqlite3.Connection] = None 
        # Dictionary to hold connections to source-specific DBs
        self.source_connections: Dict[str, sqlite3.Connection] = {} 
        
        # Establish main connection and create tables on init
        self._connect() # Establishes self.conn
        if self.conn:
            self.create_tables(self.conn) # Create tables for main connection
        else:
            logger.error(f"Failed to establish main DB connection on init to {self.db_path}. Tables not created.")

    def __enter__(self):
        """‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂô®ÂÖ•Âè£ÊñπÊ≥ï"""
        if not self.conn:
            self._connect()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂô®ÈÄÄÂá∫ÊñπÊ≥ï"""
        self.close_all_connections()
        return False  # ‰∏çÂ§ÑÁêÜÂºÇÂ∏∏
    
    def _connect(self, db_file: Optional[str] = None) -> Optional[sqlite3.Connection]:
        """
        ËøûÊé•Âà∞Êï∞ÊçÆÂ∫ì
        
        Returns:
            sqlite3.Connection: Êï∞ÊçÆÂ∫ìËøûÊé•
        """
        target_db_path = db_file if db_file else self.db_path
        is_main_connection = (target_db_path == self.db_path)

        # Avoid reconnecting if already connected (for the main connection)
        if is_main_connection and self.conn:
            # Basic check if connection is still alive (optional)
            try:
                 self.conn.execute("SELECT 1").fetchone()
                 # logger.debug(f"Main connection to {target_db_path} is active.")
                 return self.conn
            except (sqlite3.ProgrammingError, sqlite3.OperationalError) as e:
                 logger.warning(f"Main connection seems closed ({e}). Reconnecting...")
                 self.conn = None # Force reconnect

        try:
            db_dir = os.path.dirname(target_db_path)
            if db_dir and not os.path.exists(db_dir):
                os.makedirs(db_dir)
                logger.info(f"Created directory for DB: {db_dir}")
            
            logger.debug(f"Attempting to connect to: {target_db_path}")
            conn = sqlite3.connect(target_db_path, check_same_thread=False, timeout=10)
            conn.execute("PRAGMA foreign_keys = ON")
            conn.execute("PRAGMA journal_mode = WAL")
            conn.row_factory = sqlite3.Row
            logger.info(f"Successfully connected to: {target_db_path}")
            
            if is_main_connection:
                self.conn = conn # Store the main connection
            
            return conn
        except sqlite3.Error as e:
            logger.error(f"Failed to connect to database {target_db_path}: {e}", exc_info=True)
            if is_main_connection:
                self.conn = None # Ensure main conn is None if connect failed
            return None

    def get_connection(self, db_path: Optional[str] = None) -> Optional[sqlite3.Connection]:
        """Gets or creates a connection. Manages main and source connections."""
        target_db_path = os.path.abspath(db_path) if db_path else self.db_path
        is_main = (target_db_path == self.db_path)
        source_name = None
        # üîß ‰øÆÂ§çÔºöÁßªÈô§sources_dir‰æùËµñÔºåÁªü‰∏Ä‰ΩøÁî®‰∏ªÊï∞ÊçÆÂ∫ì
        if not is_main:
             source_name = os.path.splitext(os.path.basename(target_db_path))[0]

        # 1. Check main connection cache
        if is_main and self.conn:
            try: # Quick check if alive
                self.conn.execute("SELECT 1")
                return self.conn
            except (sqlite3.ProgrammingError, sqlite3.OperationalError):
                 logger.warning("Main connection cache was stale. Reconnecting...")
                 self.conn = None # Clear stale connection

        # 2. Check source connection cache
        if source_name and source_name in self.source_connections:
             cached_conn = self.source_connections[source_name]
             try: # Quick check if alive
                  cached_conn.execute("SELECT 1")
                  return cached_conn
             except (sqlite3.ProgrammingError, sqlite3.OperationalError):
                  logger.warning(f"Source connection cache for '{source_name}' was stale. Reconnecting...")
                  self.source_connections.pop(source_name, None)
                  # Proceed to connect
        
        # 3. Establish new connection if not cached or stale
        new_conn = self._connect(db_file=target_db_path)
        if new_conn:
            # Ensure tables exist for the new connection
            self.create_tables_for_connection(new_conn)
            # Store in the appropriate cache
            if is_main:
                self.conn = new_conn
            elif source_name:
                self.source_connections[source_name] = new_conn
            else: # Should not happen if path is absolute, but handle anyway
                 logger.warning(f"Connection established to {target_db_path}, but couldn't determine if main or source.")
        return new_conn
    
    def create_tables(self, conn: sqlite3.Connection) -> None:
        """ÂàõÂª∫ÂøÖË¶ÅÁöÑÊï∞ÊçÆÂ∫ìË°®"""
        try:
            cursor = conn.cursor()
            
            # ÂàõÂª∫Êñ∞ÈóªË°®
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    content TEXT NOT NULL,
                    content_html TEXT,
                    pub_time DATETIME NOT NULL,
                    author TEXT,
                    source TEXT NOT NULL,
                    url TEXT UNIQUE NOT NULL,
                    keywords TEXT,
                    images TEXT,
                    related_stocks TEXT,
                    sentiment REAL,
                    classification TEXT,
                    category TEXT DEFAULT 'Ë¥¢Áªè',
                    crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                    summary TEXT,
                    status INTEGER DEFAULT 0,
                    
                    CONSTRAINT news_url_unique UNIQUE (url)
                )
            ''')
            
            # ÂàõÂª∫Á¥¢Âºï
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_source_time ON news(source, pub_time)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_category ON news(category)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_pub_time ON news(pub_time)')
            
            conn.commit()
            logger.info("Êï∞ÊçÆÂ∫ìË°®ÂàùÂßãÂåñÂÆåÊàê")
            
        except sqlite3.Error as e:
            logger.error(f"ÂàõÂª∫Êï∞ÊçÆÂ∫ìË°®Â§±Ë¥•: {str(e)}")
            raise
    
    def create_tables_for_connection(self, conn: sqlite3.Connection) -> None:
        """
        For a given database connection, create all necessary table structures
        (news, keywords, news_keywords, stocks, news_stocks).
        Typically used for source-specific databases.
        
        Args:
            conn: sqlite3.Connection object
        """
        cursor = None  # Initialize cursor to None
        try:
            cursor = conn.cursor()
            db_name_for_log = conn.db_name if hasattr(conn, 'db_name') else 'unknown_db'
            
            # 1. Create news Table (identical to create_tables())
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    content TEXT,
                    content_html TEXT,
                    pub_time DATETIME,
                    author TEXT,
                    source TEXT,
                    url TEXT UNIQUE NOT NULL,
                    keywords TEXT,
                    images TEXT,
                    related_stocks TEXT,
                    sentiment REAL,
                    classification TEXT,
                    category TEXT DEFAULT 'Ë¥¢Áªè',
                    crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                    summary TEXT,
                    status INTEGER DEFAULT 0,
                    CONSTRAINT news_url_unique UNIQUE (url)
                )
            ''')
            logger.info(f"Ensured 'news' table exists on connection to {db_name_for_log}.")

            # 2. Create keywords Table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS keywords (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    keyword TEXT UNIQUE NOT NULL,
                    frequency INTEGER DEFAULT 1,
                    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            logger.info(f"Ensured 'keywords' table exists on connection to {db_name_for_log}.")

            # 3. Create news_keywords Junction Table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_keywords (
                    news_id TEXT NOT NULL,
                    keyword_id INTEGER NOT NULL,
                    relevance REAL, 
                    PRIMARY KEY (news_id, keyword_id),
                    FOREIGN KEY (news_id) REFERENCES news (id) ON DELETE CASCADE,
                    FOREIGN KEY (keyword_id) REFERENCES keywords (id) ON DELETE CASCADE
                )
            ''')
            logger.info(f"Ensured 'news_keywords' table exists on connection to {db_name_for_log}.")
            
            # 4. Create stocks Table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS stocks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stock_code TEXT UNIQUE NOT NULL, -- e.g., "600519.SH" or "00700.HK"
                    stock_name TEXT,             -- e.g., "Ë¥µÂ∑ûËåÖÂè∞" or "ËÖæËÆØÊéßËÇ°"
                    market TEXT,                 -- e.g., "SH", "SZ", "HK", "US"
                    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            logger.info(f"Ensured 'stocks' table exists on connection to {db_name_for_log}.")
            
            # 5. Create news_stocks Junction Table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_stocks (
                    news_id TEXT NOT NULL,
                    stock_id INTEGER NOT NULL,
                    relevance REAL, 
                    PRIMARY KEY (news_id, stock_id),
                    FOREIGN KEY (news_id) REFERENCES news (id) ON DELETE CASCADE,
                    FOREIGN KEY (stock_id) REFERENCES stocks (id) ON DELETE CASCADE
                )
            ''')
            logger.info(f"Ensured 'news_stocks' table exists on connection to {db_name_for_log}.")

            # Apply table structure upgrades after creation
            self._upgrade_news_table_if_needed(cursor) 
            self._upgrade_keywords_table_if_needed(cursor)
            self._upgrade_stocks_table_if_needed(cursor)
            # Note: Junction tables (news_keywords, news_stocks) rarely need schema changes 
            # unless foreign keys or primary keys structure changes, which is less common.
            
            conn.commit()
            logger.info(f"Successfully created/verified all table structures on connection to {db_name_for_log}.")
            
        except sqlite3.Error as e:
            logger.error(f"Failed to create tables on connection to {db_name_for_log}: {e}")
            if conn:
                conn.rollback()
        finally:
            if cursor:
                cursor.close()
    
    def _upgrade_news_table_if_needed(self, cursor: sqlite3.Cursor) -> None:
        """
        Ê£ÄÊü•Âπ∂ÂçáÁ∫ß news Ë°®ÁªìÊûÑÔºåÊ∑ªÂä†Áº∫Â§±ÁöÑÂàó
        
        Args:
            cursor: Êï∞ÊçÆÂ∫ìÊ∏∏Ê†á
        """
        try:
            # Ëé∑ÂèñnewsË°®ÁöÑÂàó‰ø°ÊÅØ
            cursor.execute("PRAGMA table_info(news)")
            columns = {row[1]: row for row in cursor.fetchall()}
            
            # ÂÆö‰πâÊúüÊúõÁöÑÂàóÂèäÂÖ∂Ê∑ªÂä†ËØ≠Âè•
            # (Â≠óÊÆµÂêç, SQLÁ±ªÂûã, ÈªòËÆ§ÂÄº - Â¶ÇÊûúÊúâÁöÑËØù)
            expected_news_columns = {
                'content_html': "TEXT",
                'classification': "TEXT",
                'summary': "TEXT",
                'status': "INTEGER DEFAULT 0",
                # 'crawl_time' should exist due to table definition, but check anyway
                'crawl_time': "DATETIME DEFAULT CURRENT_TIMESTAMP", 
                # 'category' should exist due to table definition
                'category': "TEXT DEFAULT 'Ë¥¢Áªè'",
                 # 'sentiment' should exist
                'sentiment': "REAL",
                 # 'author' should exist
                'author': "TEXT",
                 # 'source' should exist
                'source': "TEXT", # Ensure this is NOT NULL if required by application logic elsewhere
                 # 'url' should exist and be unique
                'url': "TEXT UNIQUE NOT NULL",
                 # 'keywords' should exist
                'keywords': "TEXT",
                 # 'images' should exist
                'images': "TEXT",
                 # 'related_stocks' should exist
                'related_stocks': "TEXT"
            }

            for col_name, col_type_with_default in expected_news_columns.items():
                if col_name not in columns:
                    logger.info(f"ÂçáÁ∫ß 'news' Ë°®: Ê∑ªÂä†Âàó '{col_name}'")
                    cursor.execute(f"ALTER TABLE news ADD COLUMN {col_name} {col_type_with_default}")
            
            # Á°Æ‰øùÂÖ≥ÈîÆÁöÑ NOT NULL Á∫¶Êùü‰ªçÁÑ∂Â≠òÂú®ÊàñË¢´Ê≠£Á°ÆËÆæÁΩÆÔºàÂ¶ÇÊûúALTER TABLE‰∏çËÉΩÁõ¥Êé•‰øÆÊîπÔºâ
            # ÂØπ‰∫é SQLite, ALTER TABLE ‰∏çËÉΩÁõ¥Êé•Ê∑ªÂä† NOT NULLÁ∫¶ÊùüÂà∞Áé∞ÊúâÂàóËÄå‰∏çÊèê‰æõÈªòËÆ§ÂÄº
            # ÈÄöÂ∏∏Âú®CREATE TABLEÊó∂ÂÆö‰πâ„ÄÇÂ¶ÇÊûúÈúÄË¶Å‰øÆÊîπÔºåÂèØËÉΩÈúÄË¶ÅÊõ¥Â§çÊùÇÁöÑÊï∞ÊçÆËøÅÁßª„ÄÇ
            # ËøôÈáå‰∏ªË¶ÅÂÖ≥Ê≥®Ê∑ªÂä†Áº∫Â§±Âàó„ÄÇ

            # self.conn.commit() # Commit should be handled by the calling method (e.g., create_tables_for_connection)
            logger.debug(f"'news' Ë°®ÁªìÊûÑÊ£ÄÊü•/ÂçáÁ∫ßÂÆåÊàê (cursor: {cursor})")
        except sqlite3.Error as e:
            logger.error(f"ÂçáÁ∫ß 'news' Ë°®Â§±Ë¥•: {str(e)}")
            # self.conn.rollback() # Rollback should be handled by the calling method

    def _upgrade_keywords_table_if_needed(self, cursor: sqlite3.Cursor) -> None:
        """Checks and upgrades the keywords table schema if necessary."""
        try:
            cursor.execute("PRAGMA table_info(keywords)")
            columns = {row[1]: row for row in cursor.fetchall()}
            
            # Example: Ensure 'frequency' column exists (it should from CREATE TABLE)
            if 'frequency' not in columns:
                logger.info("ÂçáÁ∫ß 'keywords' Ë°®: Ê∑ªÂä†Âàó 'frequency' INTEGER DEFAULT 1")
                cursor.execute("ALTER TABLE keywords ADD COLUMN frequency INTEGER DEFAULT 1")

            # Example: Ensure 'last_updated' column exists
            if 'last_updated' not in columns:
                logger.info("ÂçáÁ∫ß 'keywords' Ë°®: Ê∑ªÂä†Âàó 'last_updated' DATETIME DEFAULT CURRENT_TIMESTAMP")
                cursor.execute("ALTER TABLE keywords ADD COLUMN last_updated DATETIME DEFAULT CURRENT_TIMESTAMP")
            
            # Add other checks for keywords table if needed in the future
            logger.debug(f"'keywords' Ë°®ÁªìÊûÑÊ£ÄÊü•/ÂçáÁ∫ßÂÆåÊàê (cursor: {cursor})")
        except sqlite3.Error as e:
            logger.error(f"ÂçáÁ∫ß 'keywords' Ë°®Â§±Ë¥•: {str(e)}")

    def _upgrade_stocks_table_if_needed(self, cursor: sqlite3.Cursor) -> None:
        """Checks and upgrades the stocks table schema if necessary."""
        try:
            cursor.execute("PRAGMA table_info(stocks)")
            columns = {row[1]: row for row in cursor.fetchall()}

            # Example: Ensure 'stock_code' is UNIQUE NOT NULL (defined in CREATE TABLE)
            # Example: Ensure 'market' column exists
            if 'market' not in columns:
                logger.info("ÂçáÁ∫ß 'stocks' Ë°®: Ê∑ªÂä†Âàó 'market' TEXT")
                cursor.execute("ALTER TABLE stocks ADD COLUMN market TEXT")
            
            if 'last_updated' not in columns:
                logger.info("ÂçáÁ∫ß 'stocks' Ë°®: Ê∑ªÂä†Âàó 'last_updated' DATETIME DEFAULT CURRENT_TIMESTAMP")
                cursor.execute("ALTER TABLE stocks ADD COLUMN last_updated DATETIME DEFAULT CURRENT_TIMESTAMP")

            # Add other checks for stocks table if needed in the future
            logger.debug(f"'stocks' Ë°®ÁªìÊûÑÊ£ÄÊü•/ÂçáÁ∫ßÂÆåÊàê (cursor: {cursor})")
        except sqlite3.Error as e:
            logger.error(f"ÂçáÁ∫ß 'stocks' Ë°®Â§±Ë¥•: {str(e)}")

    def save_news(self, news_data: Dict[str, Any], use_source_db: bool = False) -> bool:
        """
        ‰øùÂ≠òÊñ∞ÈóªÊï∞ÊçÆÂà∞Êï∞ÊçÆÂ∫ì
        
        Args:
            news_data: Êñ∞ÈóªÊï∞ÊçÆÂ≠óÂÖ∏
            use_source_db: ÊòØÂê¶‰ΩøÁî®Êù•Ê∫êÊï∞ÊçÆÂ∫ì
            
        Returns:
            bool: ÊòØÂê¶‰øùÂ≠òÊàêÂäü
        """
        target_conn: Optional[sqlite3.Connection] = None
        source_name_for_conn_management: Optional[str] = None

        if use_source_db:
            source = news_data.get('source')
            if not source:
                logger.error("save_news: Cannot use source DB if 'source' is not in news_data.")
                return False
            source_db_path = self.get_source_db_path(source)
            target_conn = self.get_connection(db_path=source_db_path)
            source_name_for_conn_management = source
        else:
            target_conn = self.get_connection() # Gets self.conn or establishes it
        
        if not target_conn:
            logger.error(f"save_news: Failed to get a database connection. News not saved: {news_data.get('title')}")
            return False
        
        try:
            # preprocessed_item = self._preprocess_news_data(news_data, for_source_db=use_source_db)
            # The above line was from a previous state. _preprocess_news_data should be robust.
            preprocessed_item = self._preprocess_news_data(news_data) # Assuming one preprocess fits all for now
            if not preprocessed_item.get('id'):
                 logger.error(f"News item ID missing after preprocessing. Cannot save. Title: {news_data.get('title')}")
                 return False

            self.insert_news(preprocessed_item, target_conn) # insert_news takes the connection
            return True
        except Exception as e:
            logger.error(f"save_news: Error saving news '{news_data.get('title')}': {e}", exc_info=True)
            return False
        finally:
            if use_source_db and target_conn and target_conn != self.conn:
                # Close if it was a source-specific connection different from the main one
                self.close_connection(conn=target_conn, source_name=source_name_for_conn_management)
    
    def _preprocess_news_data(self, news_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        È¢ÑÂ§ÑÁêÜÊñ∞ÈóªÊï∞ÊçÆ
        
        Args:
            news_data: ÂéüÂßãÊñ∞ÈóªÊï∞ÊçÆ
            
        Returns:
            Dict[str, Any]: Â§ÑÁêÜÂêéÁöÑÊñ∞ÈóªÊï∞ÊçÆ
        """
        # ÂàõÂª∫ÂâØÊú¨‰ª•ÈÅøÂÖç‰øÆÊîπÂéüÂßãÊï∞ÊçÆ
        processed = news_data.copy()
        
        # 1. Á°Æ‰øùidÂ≠òÂú®ÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôÊ†πÊçÆurlÁîüÊàê
        if 'id' not in processed or not processed['id']:
            if 'url' in processed and processed['url']:
                processed['id'] = hashlib.md5(processed['url'].encode('utf-8')).hexdigest()
                logger.debug(f"[preprocess] Êñ∞ÈóªIDÁº∫Â§±, Ê†πÊçÆURLÁîüÊàêID: {processed['id']} for URL: {processed['url']}")
            else:
                # Â¶ÇÊûúËøûURLÈÉΩÊ≤°ÊúâÔºåÂàôÊó†Ê≥ïÁîüÊàêIDÔºåÂêéÁª≠ÁöÑÂøÖÈúÄÂ≠óÊÆµÊ£ÄÊü•‰ºöÂ§ÑÁêÜËøô‰∏™ÈóÆÈ¢ò
                logger.warning("[preprocess] Êñ∞ÈóªIDÂíåURLÂùáÁº∫Â§±, Êó†Ê≥ïÁîüÊàêID.")

        # 2. Á°Æ‰øùÊó∂Èó¥Â≠óÊÆµÊ†ºÂºèÊ≠£Á°ÆÊàñÊúâÈªòËÆ§ÂÄº
        if 'pub_time' not in processed or not processed['pub_time']:
            # Â∞ùËØï‰ªéÂÖ∂‰ªñÂ∏∏ËßÅÊó∂Èó¥Â≠óÊÆµËé∑ÂèñÔºåÂ¶ÇÊûúÈÉΩÊ≤°ÊúâÂàôËÆæ‰∏∫ÂΩìÂâçÊó∂Èó¥
            # (Ê≠§ÈÉ®ÂàÜÂèØÊ†πÊçÆÂÆûÈôÖÊï∞ÊçÆÊ∫êÁöÑÂ≠óÊÆµÂêçËøõ‰∏ÄÊ≠•Êâ©Â±ï)
            processed['pub_time'] = processed.get('publish_time', processed.get('time', datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        
        if 'crawl_time' not in processed or not processed['crawl_time']:
            processed['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # 3. Â§ÑÁêÜÂèØËÉΩÁöÑJSONÂ≠óÊÆµ (keywords, images, related_stocks)
        for field in ['images', 'related_stocks', 'keywords']:
            if field in processed and isinstance(processed[field], (list, dict)):
                try:
                    processed[field] = json.dumps(processed[field], ensure_ascii=False)
                except TypeError as e:
                    logger.warning(f"[preprocess] JSONÂ∫èÂàóÂåñÂ§±Ë¥• for field {field}: {e}. ÂÄº: {processed[field]}. Â∞ÜËÆæÁΩÆ‰∏∫Á©∫Â≠óÁ¨¶‰∏≤.")
                    processed[field] = ""
            elif field in processed and processed[field] is None: # Â¶ÇÊûúÊòæÂºè‰º†ÂÖ•NoneÔºåËΩ¨‰∏∫Á©∫Â≠óÁ¨¶‰∏≤Êàñ‰øùÊåÅNoneÔºåÊ†πÊçÆÊï∞ÊçÆÂ∫ìÂÆö‰πâ
                 processed[field] = "" # ÊàñËÄÖ NoneÔºåÂèñÂÜ≥‰∫éË°®ÁªìÊûÑÊòØÂê¶ÂÖÅËÆ∏NULL

        # 4. Â§ÑÁêÜÊÉÖÊÑüÂÄº (sentiment)
        if 'sentiment' in processed:
            if isinstance(processed['sentiment'], str):
                sentiment_map = {
                    'ÁßØÊûÅ': 1.0,
                    '‰∏≠ÊÄß': 0.0,
                    'Ê∂àÊûÅ': -1.0
                }
                # Â¶ÇÊûúÂ≠óÁ¨¶‰∏≤‰∏çÂú®map‰∏≠Ôºå‰πüÈªòËÆ§‰∏∫‰∏≠ÊÄß0.0
                processed['sentiment'] = sentiment_map.get(processed['sentiment'], 0.0) 
            elif not isinstance(processed['sentiment'], (float, int)):
                 logger.warning(f"[preprocess] Êó†ÊïàÁöÑÊÉÖÊÑüÂÄºÁ±ªÂûã: {processed['sentiment']}. Â∞ÜËÆæÁΩÆ‰∏∫0.0")
                 processed['sentiment'] = 0.0 # Â¶ÇÊûúÁ±ªÂûã‰∏çÂØπÔºå‰πüÈªòËÆ§‰∏∫0.0
        
        # 5. ËÆæÁΩÆÂÖ∂‰ªñÂ≠óÊÆµÁöÑÈªòËÆ§ÂÄº (ÂåÖÊã¨Êñ∞Â≠óÊÆµ summary, status)
        # Ê≥®ÊÑè: author Âíå source ÈÄöÂ∏∏Áî±Áà¨Ëô´Êú¨Ë∫´Ê†πÊçÆÊù•Ê∫êËÆæÂÆöÔºåËøôÈáåÂèØ‰ª•‰Ωú‰∏∫ÊúÄÁªàÁöÑ‰øùÈöú
        defaults = {
            'author': processed.get('source', 'Êú™Áü•'), # Â¶ÇÊûúÁà¨Ëô´Ê≤°ÁªôauthorÔºåÂ∞ùËØï‰ΩøÁî®source
            'source': processed.get('source', 'Êú™Áü•Êù•Ê∫ê'), # ‰∏á‰∏ÄËøûsourceÈÉΩÊ≤°Êúâ
            'category': 'Ë¥¢Áªè',
            'classification': '', # ÈªòËÆ§‰∏∫Á©∫Â≠óÁ¨¶‰∏≤
            'content_html': '',   # ÈªòËÆ§‰∏∫Á©∫Â≠óÁ¨¶‰∏≤
            'keywords': '',       # Â¶ÇÊûú‰πãÂâçÊú™Â§ÑÁêÜÊàñÂ§ÑÁêÜÂ§±Ë¥•ÔºåÈªòËÆ§‰∏∫Á©∫Â≠óÁ¨¶‰∏≤
            'images': '',         # Âêå‰∏ä
            'related_stocks': '', # Âêå‰∏ä
            'summary': '',        # Êñ∞Â≠óÊÆµÔºåÈªòËÆ§‰∏∫Á©∫Â≠óÁ¨¶‰∏≤
            'status': 0,          # Êñ∞Â≠óÊÆµÔºåÈªòËÆ§‰∏∫0
            'sentiment': 0.0      # Â¶ÇÊûú‰∏äÈù¢Â§ÑÁêÜÂêé sentiment ‰ªç‰∏çÂ≠òÂú®ÔºåÂàôÈªòËÆ§‰∏∫0.0
        }
        
        for field, default_value in defaults.items():
            if field not in processed or processed[field] is None: # Ê£ÄÊü• None Êàñ‰∏çÂ≠òÂú®
                processed[field] = default_value
        
        # Á°Æ‰øù title, content, url, source, pub_time ‰∏ç‰∏∫ None (ÂêéÁª≠ save_news ‰∏≠ÊúâÊõ¥‰∏•Ê†ºÁöÑÊ£ÄÊü•)
        # ‰ΩÜËøôÈáåÂèØ‰ª•ÊèêÂâçÂ∞Ü None ËΩ¨‰∏∫Á©∫Â≠óÁ¨¶‰∏≤ÔºåÈÅøÂÖçÊüê‰∫õÊï∞ÊçÆÂ∫ìÊìç‰ΩúÂØπ None ÁöÑÈóÆÈ¢ò
        for critical_field in ['title', 'content', 'url', 'source', 'pub_time']:
            if critical_field in processed and processed[critical_field] is None:
                logger.warning(f"[preprocess] ÂÖ≥ÈîÆÂ≠óÊÆµ {critical_field} ‰∏∫ None, Â∞ÜËΩ¨Êç¢‰∏∫Á©∫Â≠óÁ¨¶‰∏≤.")
                processed[critical_field] = ""

        logger.debug(f"[preprocess] Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂÆåÊàê: {processed.get('title')}")
        return processed
    
    def save_to_source_db(self, news_data):
        """
        üîß ‰øÆÂ§çÔºö‰øùÂ≠òÂà∞Áªü‰∏ÄÊï∞ÊçÆÂ∫ìÁõÆÂΩïÔºåÁßªÈô§sourcesÂàÜÁ¶ª
        
        Args:
            news_data: Êñ∞ÈóªÊï∞ÊçÆÂ≠óÂÖ∏
            
        Returns:
            bool: ÊòØÂê¶‰øùÂ≠òÊàêÂäü
        """
        if 'source' not in news_data or not news_data['source']:
            logger.warning(f"‰øùÂ≠òÂ§±Ë¥•: Êñ∞ÈóªÊï∞ÊçÆÁº∫Â∞ësourceÂ≠óÊÆµ, Ê†áÈ¢ò: {news_data.get('title', 'Unknown')}")
            return False
        
        # üîß ‰øÆÂ§çÔºö‰ΩøÁî®Áªü‰∏ÄË∑ØÂæÑÁÆ°ÁêÜÂô®Ëé∑ÂèñÊï∞ÊçÆÂ∫ìË∑ØÂæÑ
        from backend.newslook.core.database_path_manager import get_database_path_manager
        
        path_manager = get_database_path_manager()
        original_source = news_data['source']
        
        # Áõ¥Êé•‰ΩøÁî®‰∏ªÊï∞ÊçÆÂ∫ìÔºåÂÆûÁé∞Ë∑ØÂæÑÁªü‰∏ÄÂåñ
        source_db_path = path_manager.get_main_db_path()
        
        logger.info(f"üîß Âº∫Âà∂‰ΩøÁî®Áªü‰∏ÄÊï∞ÊçÆÂ∫ìË∑ØÂæÑ: {source_db_path}")
        
        try:
            # üîß ‰øÆÂ§çÔºö‰ΩøÁî®Áªü‰∏ÄÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂô®ËøõË°å‰øùÂ≠òÈ™åËØÅ
            from backend.newslook.core.unified_database_manager import get_unified_database_manager
            
            unified_manager = get_unified_database_manager()
            success = unified_manager.save_news(news_data, to_source_db=False)
            
            if success:
                logger.info(f"‚úÖ ‰øùÂ≠òÈ™åËØÅÈÄöËøá: {news_data.get('title', 'Unknown')[:50]}...")
            else:
                logger.warning(f"‚ùå ‰øùÂ≠òÂ§±Ë¥•: {news_data.get('title', 'Unknown')[:50]}...")
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå ‰øùÂ≠òÂà∞Áªü‰∏ÄÊï∞ÊçÆÂ∫ìÊó∂ÂèëÁîüÈîôËØØ: {str(e)}, Ê†áÈ¢ò: {news_data.get('title', 'Unknown')}")
            return False
    
    def backup_database(self, source=None):
        """
        Â§á‰ªΩÊï∞ÊçÆÂ∫ì
        
        Args:
            source: Ë¶ÅÂ§á‰ªΩÁöÑÊù•Ê∫êÂêçÁß∞ÔºåÂ¶ÇÊûú‰∏∫NoneÂàôÂ§á‰ªΩ‰∏ªÊï∞ÊçÆÂ∫ì
            
        Returns:
            str: Â§á‰ªΩÊñá‰ª∂Ë∑ØÂæÑ
        """
        # üîß ‰øÆÂ§çÔºöÁªü‰∏Ä‰ΩøÁî®‰∏ªÊï∞ÊçÆÂ∫ìË∑ØÂæÑ
        db_path = self.db_path
        
        # ÁîüÊàêÂ§á‰ªΩÊñá‰ª∂Âêç
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        backup_filename = f"{source or 'main'}_{timestamp}.db.bak"
        backup_path = os.path.join(self.backups_dir, backup_filename)
        
        try:
            # ÂÖ≥Èó≠ËøûÊé•‰ª•Á°Æ‰øùÊï∞ÊçÆÂ∑≤ÂÜôÂÖ•Á£ÅÁõò
            if db_path in self.source_connections:
                self.source_connections[db_path].close()
                del self.source_connections[db_path]
            
            # Â§çÂà∂Êï∞ÊçÆÂ∫ìÊñá‰ª∂
            shutil.copy2(db_path, backup_path)
            logger.info(f"ÊàêÂäüÂ§á‰ªΩÊï∞ÊçÆÂ∫ì {db_path} Âà∞ {backup_path}")
            
            # ÈáçÊñ∞ËøûÊé•
            if db_path == self.db_path:
                self.conn = self._connect()
                self.source_connections[db_path] = self.conn
            
            return backup_path
        except Exception as e:
            logger.error(f"Â§á‰ªΩÊï∞ÊçÆÂ∫ìÂ§±Ë¥•: {str(e)}")
            return None
    
    def restore_database(self, backup_path, target_source=None):
        """
        ‰ªéÂ§á‰ªΩÊÅ¢Â§çÊï∞ÊçÆÂ∫ì
        
        Args:
            backup_path: Â§á‰ªΩÊñá‰ª∂Ë∑ØÂæÑ
            target_source: ÁõÆÊ†áÊù•Ê∫êÂêçÁß∞ÔºåÂ¶ÇÊûú‰∏∫NoneÂàôÊÅ¢Â§çÂà∞‰∏ªÊï∞ÊçÆÂ∫ì
            
        Returns:
            bool: ÊòØÂê¶ÊÅ¢Â§çÊàêÂäü
        """
        # üîß ‰øÆÂ§çÔºöÁªü‰∏Ä‰ΩøÁî®‰∏ªÊï∞ÊçÆÂ∫ìË∑ØÂæÑ
        target_db_path = self.db_path
        
        try:
            # ÂÖ≥Èó≠ËøûÊé•
            if target_db_path in self.source_connections:
                self.source_connections[target_db_path].close()
                del self.source_connections[target_db_path]
            
            # Â§çÂà∂Â§á‰ªΩÊñá‰ª∂Âà∞ÁõÆÊ†á‰ΩçÁΩÆ
            shutil.copy2(backup_path, target_db_path)
            logger.info(f"ÊàêÂäü‰ªéÂ§á‰ªΩ {backup_path} ÊÅ¢Â§çÊï∞ÊçÆÂ∫ì {target_db_path}")
            
            # ÈáçÊñ∞ËøûÊé•
            if target_db_path == self.db_path:
                self.conn = self._connect()
                self.source_connections[target_db_path] = self.conn
            
            return True
        except Exception as e:
            logger.error(f"ÊÅ¢Â§çÊï∞ÊçÆÂ∫ìÂ§±Ë¥•: {str(e)}")
            return False
    
    def list_backups(self, source=None):
        """
        ÂàóÂá∫Â§á‰ªΩÊñá‰ª∂
        
        Args:
            source: Êù•Ê∫êÂêçÁß∞ÔºåÂ¶ÇÊûú‰∏∫NoneÂàôÂàóÂá∫ÊâÄÊúâÂ§á‰ªΩ
            
        Returns:
            list: Â§á‰ªΩ‰ø°ÊÅØÂàóË°®ÔºåÊØèÈ°πÂåÖÂê´ {path, timestamp, source, size}
        """
        import glob
        
        # Êü•ÊâæÊâÄÊúâÂ§á‰ªΩÊñá‰ª∂
        backup_pattern = os.path.join(self.backups_dir, '*.db.bak')
        backup_files = glob.glob(backup_pattern)
        
        # Ëß£ÊûêÂ§á‰ªΩ‰ø°ÊÅØ
        backups = []
        for path in backup_files:
            filename = os.path.basename(path)
            parts = filename.split('_')
            
            if len(parts) >= 2:
                backup_source = parts[0]
                
                # Â¶ÇÊûúÊåáÂÆö‰∫ÜÊù•Ê∫êÔºåÂè™ËøîÂõûÂåπÈÖçÁöÑÂ§á‰ªΩ
                if source and backup_source != source:
                    continue
                
                # Ëß£ÊûêÊó∂Èó¥Êà≥
                timestamp_str = parts[1].split('.')[0]
                try:
                    timestamp = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S')
                    size = os.path.getsize(path)
                    
                    backups.append({
                        'path': path,
                        'timestamp': timestamp,
                        'source': backup_source,
                        'size': size
                    })
                except ValueError:
                    logger.warning(f"Êó†Ê≥ïËß£ÊûêÂ§á‰ªΩÊó∂Èó¥Êà≥: {filename}")
        
        # ÊåâÊó∂Èó¥Êà≥ÊéíÂ∫èÔºåÊúÄÊñ∞ÁöÑÂú®Ââç
        backups.sort(key=lambda x: x['timestamp'], reverse=True)
        return backups
    
    def sync_to_main_db(self, source):
        """
        Â∞ÜÊåáÂÆöÊù•Ê∫êÁöÑÊï∞ÊçÆÂêåÊ≠•Âà∞‰∏ªÊï∞ÊçÆÂ∫ì
        
        Args:
            source: Êù•Ê∫êÂêçÁß∞
            
        Returns:
            int: ÂêåÊ≠•ÁöÑÊñ∞ÈóªÊï∞Èáè
        """
        # üîß ‰øÆÂ§çÔºöÂêåÊ≠•ÂäüËÉΩÂ∑≤Â∫üÂºÉÔºåÁªü‰∏ÄÊï∞ÊçÆÂ∫ì‰∏çÈúÄË¶ÅÂêåÊ≠•
        logger.info(f"üîß ÂêåÊ≠•ÂäüËÉΩÂ∑≤Â∫üÂºÉ: ÊâÄÊúâÊï∞ÊçÆÂ∑≤Áªü‰∏ÄÂ≠òÂÇ®Âú®‰∏ªÊï∞ÊçÆÂ∫ì")
        return 0
        
        # üîß ‰øÆÂ§çÔºöÂêåÊ≠•ÂäüËÉΩÁöÑÂÆûÁé∞Â∑≤ÁßªÈô§ÔºåÂõ†‰∏∫Áªü‰∏ÄÊï∞ÊçÆÂ∫ì‰∏çÈúÄË¶ÅÂêåÊ≠•
    
    def get_available_sources(self):
        """
        üîß ‰øÆÂ§çÔºöËé∑ÂèñÁªü‰∏ÄÊï∞ÊçÆÂ∫ì‰∏≠ÁöÑÊï∞ÊçÆÊ∫ê
        
        Returns:
            list: Êï∞ÊçÆÊ∫êÂàóË°®
        """
        # üîß ‰øÆÂ§çÔºö‰ªé‰∏ªÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÊâÄÊúâÊï∞ÊçÆÊ∫ê
        try:
            cursor = self.conn.cursor()
            cursor.execute("SELECT DISTINCT source FROM news WHERE source IS NOT NULL AND source != ''")
            sources = [row[0] for row in cursor.fetchall()]
            return sorted(sources)
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÊï∞ÊçÆÊ∫êÂ§±Ë¥•: {e}")
            return []
    
    def close(self):
        """ÂÖ≥Èó≠Êï∞ÊçÆÂ∫ìËøûÊé•"""
        for path, conn in list(self.source_connections.items()):
            if conn:
                try:
                    conn.close()
                except Exception as e:
                    logger.error(f"ÂÖ≥Èó≠Êï∞ÊçÆÂ∫ìËøûÊé•Â§±Ë¥•: {str(e)}")
        
        # Ê∏ÖÁ©∫ËøûÊé•Â≠óÂÖ∏
        self.source_connections = {}
        self.conn = None
    
    def __del__(self):
        """Ensure connections are closed when the manager is garbage collected."""
        logger.debug("SQLiteManager __del__ called. Attempting to close all connections.")
        self.close_all_connections()

    def insert_news(self, news_data, conn):
        """
        ÂêëÊï∞ÊçÆÂ∫ìÊèíÂÖ•ÊàñÊõøÊç¢Êñ∞ÈóªÊï∞ÊçÆ (‰ΩøÁî®ÁâπÂÆöËøûÊé•)
        
        Args:
            news_data: Êñ∞ÈóªÊï∞ÊçÆÂ≠óÂÖ∏
            conn: Êï∞ÊçÆÂ∫ìËøûÊé•
            
        Returns:
            bool: ÊòØÂê¶ÊàêÂäüÊèíÂÖ•ÊàñÊõøÊç¢
        """
        processed_news_data = None # Define outside try
        try:
            cursor = conn.cursor()
            
            # È¢ÑÂ§ÑÁêÜÊï∞ÊçÆ
            processed_news_data = self._preprocess_news_data(news_data.copy())

            logger.debug(f"[insert_news] Attempting INSERT OR REPLACE for ID: {processed_news_data.get('id')}")
            # logger.debug(f"[insert_news] Data Dict: {processed_news_data}") # Optional: Log full dict

            # ÂáÜÂ§áÊï∞ÊçÆÂÖÉÁªÑ (ensure order matches VALUES clause)
            data_tuple = (
                processed_news_data.get('id', ''),
                processed_news_data.get('title', ''),
                processed_news_data.get('content', ''),
                processed_news_data.get('content_html', ''), 
                processed_news_data.get('pub_time', ''),
                processed_news_data.get('author', ''),
                processed_news_data.get('source', ''),
                processed_news_data.get('url', ''),
                processed_news_data.get('keywords', ''), # Expects JSON string here if preprocessing worked
                processed_news_data.get('images', ''), # Expects JSON string
                processed_news_data.get('related_stocks', ''), # Expects JSON string
                processed_news_data.get('sentiment', 0.0),
                processed_news_data.get('classification', ''),
                processed_news_data.get('category', 'Ë¥¢Áªè'), 
                processed_news_data.get('crawl_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                processed_news_data.get('summary', ''),
                processed_news_data.get('status', 0)
            )
            
            # Log the tuple right before execution
            logger.debug(f"[insert_news] Executing INSERT OR REPLACE with tuple: {data_tuple}")
            
            # ÊâßË°åÊèíÂÖ•ÊàñÊõøÊç¢
            cursor.execute('''
                INSERT OR REPLACE INTO news (
                    id, title, content, content_html, pub_time, author, source,
                    url, keywords, images, related_stocks, sentiment,
                    classification, category, crawl_time, summary, status
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', data_tuple)
            
            conn.commit()
            logger.info(f"[insert_news] Successfully inserted/replaced news ID: {processed_news_data.get('id')}, Title: {processed_news_data.get('title')[:30]}...")
            return True
            
        except sqlite3.Error as e:
            db_file_path_for_log = "Unknown DB"
            try: db_file_path_for_log = conn.execute('PRAGMA database_list;').fetchone()[2]
            except Exception: pass
            logger.error(f"[insert_news] SQLite Error during INSERT/REPLACE ({db_file_path_for_log}): {e}. Title: {processed_news_data.get('title', 'N/A') if processed_news_data else news_data.get('title', 'N/A')}", exc_info=True) # Add exc_info
            try: conn.rollback()
            except Exception as rb_err: logger.error(f"Rollback failed: {rb_err}")
            return False
        except Exception as e: # Catch other potential errors (e.g., in preprocessing)
             db_file_path_for_log = "Unknown DB"
             try: db_file_path_for_log = conn.execute('PRAGMA database_list;').fetchone()[2]
             except Exception: pass
             logger.error(f"[insert_news] Unexpected Error during insert/replace ({db_file_path_for_log}): {e}. Title: {processed_news_data.get('title', 'N/A') if processed_news_data else news_data.get('title', 'N/A')}", exc_info=True)
             try: conn.rollback()
             except Exception as rb_err: logger.error(f"Rollback failed: {rb_err}")
             return False

    def query_news(self, page=1, per_page=20, category=None, source=None, 
                   keyword=None, sort_by='pub_time', order='desc', days=None,
                   start_date=None, end_date=None, limit=None):
        """
        Êü•ËØ¢Êñ∞ÈóªÊï∞ÊçÆ
        
        Args:
            page: È°µÁ†ÅÔºå‰ªé1ÂºÄÂßã
            per_page: ÊØèÈ°µÊï∞Èáè
            category: ÂàÜÁ±ªËøáÊª§
            source: Êù•Ê∫êËøáÊª§
            keyword: ÂÖ≥ÈîÆËØçËøáÊª§
            sort_by: ÊéíÂ∫èÂ≠óÊÆµ
            order: ÊéíÂ∫èÊñπÂºèÔºå'asc'‰∏∫ÂçáÂ∫èÔºå'desc'‰∏∫ÈôçÂ∫è
            days: ÊúÄËøëÂá†Â§©ÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊûú‰∏∫NoneÂàô‰∏çÈôêÂà∂
            start_date: ÂºÄÂßãÊó•ÊúüÔºåÊ†ºÂºè‰∏∫'YYYY-MM-DD'
            end_date: ÁªìÊùüÊó•ÊúüÔºåÊ†ºÂºè‰∏∫'YYYY-MM-DD'
            limit: ÈôêÂà∂ËøîÂõûÁöÑÁªìÊûúÊï∞Èáè
            
        Returns:
            list: Êñ∞ÈóªÊï∞ÊçÆÂàóË°®
        """
        try:
            # ÊûÑÂª∫SQLÊü•ËØ¢
            query = "SELECT * FROM news WHERE 1=1"
            params = []
            
            # Ê∑ªÂä†ËøáÊª§Êù°‰ª∂
            if category:
                query += " AND category = ?"
                params.append(category)
                
            if source:
                query += " AND source = ?"
                params.append(source)
                
            if keyword:
                query += " AND (title LIKE ? OR content LIKE ? OR keywords LIKE ?)"
                keyword_param = f"%{keyword}%"
                params.extend([keyword_param, keyword_param, keyword_param])
            
            # Â§ÑÁêÜÊó•ÊúüÊù°‰ª∂
            if start_date and end_date:
                query += " AND pub_time BETWEEN ? AND ?"
                params.append(f"{start_date} 00:00:00")
                params.append(f"{end_date} 23:59:59")
            elif days:
                # ËÆ°ÁÆóÊó•ÊúüËåÉÂõ¥
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)
                query += " AND pub_time >= ?"
                params.append(start_date.strftime('%Y-%m-%d 00:00:00'))
            
            # Ê∑ªÂä†ÊéíÂ∫è
            valid_sort_fields = ['id', 'title', 'pub_time', 'source', 'category', 'crawl_time']
            if sort_by in valid_sort_fields:
                order_dir = "DESC" if order.lower() == 'desc' else "ASC"
                query += f" ORDER BY {sort_by} {order_dir}"
            else:
                query += " ORDER BY pub_time DESC"  # ÈªòËÆ§ÊéíÂ∫è
            
            # Â∫îÁî®ÂàÜÈ°µ
            if limit:
                # Â¶ÇÊûúÁõ¥Êé•ÊåáÂÆö‰∫Ülimit
                query += f" LIMIT {int(limit)}"
            else:
                # ‰ΩøÁî®ÂàÜÈ°µÂèÇÊï∞
                offset = (page - 1) * per_page
                query += f" LIMIT {int(per_page)} OFFSET {int(offset)}"
            
            # ÊâßË°åÊü•ËØ¢
            cursor = self.conn.cursor()
            cursor.execute(query, params)
            rows = cursor.fetchall()
            
            # ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏ÂàóË°®
            result = []
            for row in rows:
                news_dict = dict(row)
                result.append(news_dict)
                
            return result
            
        except sqlite3.Error as e:
            logger.error(f"Êü•ËØ¢Êñ∞ÈóªÂ§±Ë¥•: {str(e)}")
            return []
    
    def get_news_count(self, category=None, source=None, keyword=None, days=None, 
                      start_date=None, end_date=None):
        """
        Ëé∑ÂèñÁ¨¶ÂêàÊù°‰ª∂ÁöÑÊñ∞ÈóªÊï∞Èáè
        
        Args:
            category: ÂàÜÁ±ªËøáÊª§
            source: Êù•Ê∫êËøáÊª§
            keyword: ÂÖ≥ÈîÆËØçËøáÊª§
            days: ÊúÄËøëÂá†Â§©ÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊûú‰∏∫NoneÂàô‰∏çÈôêÂà∂
            start_date: ÂºÄÂßãÊó•ÊúüÔºåÊ†ºÂºè‰∏∫'YYYY-MM-DD'
            end_date: ÁªìÊùüÊó•ÊúüÔºåÊ†ºÂºè‰∏∫'YYYY-MM-DD'
            
        Returns:
            int: Êñ∞ÈóªÊï∞Èáè
        """
        try:
            # ÊûÑÂª∫SQLÊü•ËØ¢
            query = "SELECT COUNT(*) as count FROM news WHERE 1=1"
            params = []
            
            # Ê∑ªÂä†ËøáÊª§Êù°‰ª∂
            if category:
                query += " AND category = ?"
                params.append(category)
                
            if source:
                query += " AND source = ?"
                params.append(source)
                
            if keyword:
                query += " AND (title LIKE ? OR content LIKE ? OR keywords LIKE ?)"
                keyword_param = f"%{keyword}%"
                params.extend([keyword_param, keyword_param, keyword_param])
            
            # Â§ÑÁêÜÊó•ÊúüÊù°‰ª∂
            if start_date and end_date:
                query += " AND pub_time BETWEEN ? AND ?"
                params.append(f"{start_date} 00:00:00")
                params.append(f"{end_date} 23:59:59")
            elif days:
                # ËÆ°ÁÆóÊó•ÊúüËåÉÂõ¥
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)
                query += " AND pub_time >= ?"
                params.append(start_date.strftime('%Y-%m-%d 00:00:00'))
            
            # ÊâßË°åÊü•ËØ¢
            cursor = self.conn.cursor()
            cursor.execute(query, params)
            row = cursor.fetchone()
            
            return row['count'] if row else 0
            
        except sqlite3.Error as e:
            logger.error(f"Ëé∑ÂèñÊñ∞ÈóªÊï∞ÈáèÂ§±Ë¥•: {str(e)}")
            return 0
    
    def get_sources(self):
        """
        Ëé∑ÂèñÊâÄÊúâÊñ∞ÈóªÊù•Ê∫ê
        
        Returns:
            list: Êù•Ê∫êÂàóË°®
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("SELECT DISTINCT source FROM news WHERE source IS NOT NULL AND source != ''")
            sources = [row['source'] for row in cursor.fetchall()]
            return sources
        except sqlite3.Error as e:
            logger.error(f"Ëé∑ÂèñÊñ∞ÈóªÊù•Ê∫êÂ§±Ë¥•: {str(e)}")
            return []
    
    def get_categories(self):
        """
        Ëé∑ÂèñÊâÄÊúâÊñ∞ÈóªÂàÜÁ±ª
        
        Returns:
            list: ÂàÜÁ±ªÂàóË°®
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("SELECT DISTINCT category FROM news WHERE category IS NOT NULL AND category != ''")
            categories = [row['category'] for row in cursor.fetchall()]
            return categories
        except sqlite3.Error as e:
            logger.error(f"Ëé∑ÂèñÊñ∞ÈóªÂàÜÁ±ªÂ§±Ë¥•: {str(e)}")
            return []
    
    def get_news_by_id(self, news_id: str, source: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """ÈÄöËøáIDËé∑ÂèñÊñ∞ÈóªÔºåÂèØ‰ª•ÊåáÂÆöÊù•Ê∫êÊï∞ÊçÆÂ∫ì"""
        db_to_query = self.get_source_db_path(source) if source else self.db_path
        conn = self.get_connection(db_path=db_to_query)
        if not conn:
            logger.error(f"get_news_by_id: Failed to get connection for news_id '{news_id}' (source: {source}).")
            return None
        
        news_item = None
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM news WHERE id = ?", (news_id,))
            row = cursor.fetchone()
            if row:
                columns = [desc[0] for desc in cursor.description]
                news_item = dict(zip(columns, row))
                # Deserialize JSON fields if necessary (handled by _deserialize_news_item)
                news_item = self._deserialize_news_item(news_item)
        except sqlite3.Error as e:
            logger.error(f"Ëé∑ÂèñÊñ∞ÈóªÂ§±Ë¥• (ID: {news_id}, Source: {source}): {e}", exc_info=True)
        finally:
            if source and conn != self.conn: # Close if it was a source-specific, potentially temporary connection
                self.close_connection(conn=conn, source_name=source)
        return news_item

    def get_source_db_path(self, source_name: str) -> str:
        """üîß ‰øÆÂ§çÔºöËøîÂõûÁªü‰∏ÄÊï∞ÊçÆÂ∫ìË∑ØÂæÑ"""
        # üîß ‰øÆÂ§çÔºöÊâÄÊúâÊï∞ÊçÆÊ∫êÈÉΩ‰ΩøÁî®Áªü‰∏ÄÁöÑ‰∏ªÊï∞ÊçÆÂ∫ì
        return self.db_path

    def execute_query(self, query, params=None):
        """
        ÊâßË°åSQLÊü•ËØ¢
        
        Args:
            query: SQLÊü•ËØ¢Â≠óÁ¨¶‰∏≤
            params: Êü•ËØ¢ÂèÇÊï∞
            
        Returns:
            bool: ÊòØÂê¶ÊàêÂäüÊâßË°å
        """
        try:
            cursor = self.conn.cursor()
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            self.conn.commit()
            return True
        except sqlite3.Error as e:
            logger.error(f"ÊâßË°åSQLÊü•ËØ¢Â§±Ë¥•: {str(e)}")
            self.conn.rollback()
            return False

    def get_news(self, source: Optional[str] = None, days: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Ëé∑ÂèñÊñ∞ÈóªÂàóË°®
        
        Args:
            source: Êñ∞ÈóªÊù•Ê∫ê
            days: ÊúÄËøëÂá†Â§©ÁöÑÊñ∞Èóª
            
        Returns:
            List[Dict[str, Any]]: Êñ∞ÈóªÂàóË°®
        """
        try:
            cursor = self.conn.cursor()
            
            query = "SELECT * FROM news"
            params = []
            
            conditions = []
            if source:
                conditions.append("source = ?")
                params.append(source)
            
            if days:
                conditions.append("pub_time >= datetime('now', ?, 'localtime')")
                params.append(f'-{days} days')
            
            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += " ORDER BY pub_time DESC"
            
            cursor.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]
            
        except sqlite3.Error as e:
            logger.error(f"Ëé∑ÂèñÊñ∞ÈóªÂ§±Ë¥•: {str(e)}")
            return []

    def close_connection(self, conn: Optional[sqlite3.Connection] = None, source_name: Optional[str] = None):
        """Closes a specific database connection or the main connection if conn is None and source_name is None."""
        conn_to_close: Optional[sqlite3.Connection] = None
        description = "unknown connection"

        if source_name and source_name in self.source_connections:
            conn_to_close = self.source_connections.pop(source_name, None)
            description = f"source DB '{source_name}'"
        elif conn: # Explicit connection object passed
            conn_to_close = conn
            if conn == self.conn:
                description = "main DB (explicitly passed)"
                self.conn = None # Clear attribute if it was the main connection
            else:
                # Check if it was a known source connection passed directly
                for src, c_obj in list(self.source_connections.items()):
                    if c_obj == conn:
                        description = f"source DB '{src}' (explicitly passed)"
                        self.source_connections.pop(src, None)
                        break
        elif self.conn: # Default to the main connection if no other specifier
            conn_to_close = self.conn
            self.conn = None # Clear the main connection attribute
            description = "main DB (default)"
        
        if conn_to_close:
            try:
                conn_to_close.close()
                logger.info(f"SQLiteManager: Database connection closed for {description}.")
            except Exception as e:
                logger.error(f"SQLiteManager: Error closing database connection for {description}: {e}")

    def close_all_connections(self):
        """Closes all managed database connections (main and all source connections)."""
        logger.info("SQLiteManager: Closing all managed database connections.")
        # Close all source connections 
        source_keys = list(self.source_connections.keys())
        logger.debug(f"Closing {len(source_keys)} source connections...")
        for source_name in source_keys:
            conn_obj = self.source_connections.pop(source_name, None)
            if conn_obj:
                try:
                    conn_obj.close()
                    logger.info(f"SQLiteManager: Closed connection for source DB '{source_name}'.")
                except Exception as e:
                    logger.error(f"SQLiteManager: Error closing connection for source DB '{source_name}': {e}")
        self.source_connections.clear()
        
        # Close the main connection if it's active
        if self.conn:
            logger.debug("Closing main connection...")
            try:
                self.conn.close()
                logger.info("SQLiteManager: Closed main database connection (self.conn).")
                self.conn = None # Explicitly set to None after closing
            except Exception as e:
                logger.error(f"SQLiteManager: Error closing main database connection (self.conn): {e}")
        else:
            logger.debug("SQLiteManager: Main connection (self.conn) was not active or already closed.")
        logger.info("SQLiteManager: All database connection closing attempts finished.") 

    def _deserialize_news_item(self, news_item: Dict[str, Any]) -> Dict[str, Any]:
        """Helper to deserialize JSON string fields in a news item dictionary."""
        # Ensure json is imported at the top of the file: import json
        if not news_item: return news_item
        json_fields = ['tags', 'images', 'videos', 'related_stocks', 'keywords'] 
        for field in json_fields:
            if field in news_item and isinstance(news_item[field], str):
                try:
                    loaded_json = json.loads(news_item[field])
                    news_item[field] = loaded_json if isinstance(loaded_json, list) else [loaded_json]
                except json.JSONDecodeError as e:
                    logger.warning(f"_deserialize_news_item: Failed to decode JSON for field '{field}' in news ID {news_item.get('id')}. Value: \"{news_item[field][:100]}...\". Error: {e}")
                    news_item[field] = []
            elif field in news_item and news_item[field] is None:
                 news_item[field] = []
            elif field not in news_item:
                 news_item[field] = []
        return news_item 
