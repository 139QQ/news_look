#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è°ƒè¯•ä¸œæ–¹è´¢å¯Œçˆ¬è™«çš„æµ‹è¯•è„šæœ¬
"""

import os
import sys
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

def test_eastmoney_crawler():
    """æµ‹è¯•ä¸œæ–¹è´¢å¯Œçˆ¬è™«"""
    try:
        # 1. æµ‹è¯•çˆ¬è™«å¯¼å…¥
        logger.info("=== æµ‹è¯•çˆ¬è™«å¯¼å…¥ ===")
        from backend.app.crawlers.eastmoney import EastMoneyCrawler
        logger.info("âœ… ä¸œæ–¹è´¢å¯Œçˆ¬è™«å¯¼å…¥æˆåŠŸ")
        
        # 2. æµ‹è¯•çˆ¬è™«å®ä¾‹åŒ–
        logger.info("=== æµ‹è¯•çˆ¬è™«å®ä¾‹åŒ– ===")
        db_path = os.path.join('data', 'eastmoney.db')
        crawler = EastMoneyCrawler(db_path=db_path)
        logger.info(f"âœ… ä¸œæ–¹è´¢å¯Œçˆ¬è™«å®ä¾‹åŒ–æˆåŠŸï¼Œæ•°æ®åº“è·¯å¾„: {db_path}")
        
        # 3. æµ‹è¯•åŸºé‡‘åˆ†ç±»URLè·å–
        logger.info("=== æµ‹è¯•åŸºé‡‘åˆ†ç±»URLè·å– ===")
        fund_urls = crawler.get_category_url('åŸºé‡‘')
        logger.info(f"åŸºé‡‘åˆ†ç±»URLs: {fund_urls}")
        
        # 4. æµ‹è¯•ç½‘é¡µé“¾æ¥æå–
        logger.info("=== æµ‹è¯•ç½‘é¡µé“¾æ¥æå– ===")
        if fund_urls:
            test_url = fund_urls[0]
            logger.info(f"æµ‹è¯•URL: {test_url}")
            
            # æµ‹è¯•é¡µé¢è·å–
            html = crawler.fetch_page_with_requests(test_url)
            if html:
                logger.info(f"âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹ï¼Œé•¿åº¦: {len(html)} å­—ç¬¦")
                
                # æµ‹è¯•é“¾æ¥æå–
                links = crawler.extract_news_links(test_url)
                logger.info(f"æå–åˆ° {len(links)} ä¸ªæ–°é—»é“¾æ¥")
                if links:
                    logger.info("å‰5ä¸ªé“¾æ¥:")
                    for i, link in enumerate(links[:5]):
                        logger.info(f"  {i+1}. {link}")
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰æå–åˆ°æ–°é—»é“¾æ¥")
            else:
                logger.error("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
        
        # 5. æµ‹è¯•æ–°é—»è¯¦æƒ…çˆ¬å–
        logger.info("=== æµ‹è¯•æ–°é—»è¯¦æƒ…çˆ¬å– ===")
        if fund_urls and hasattr(crawler, 'extract_news_links'):
            html = crawler.fetch_page_with_requests(fund_urls[0])
            if html:
                links = crawler.extract_news_links(fund_urls[0])
                if links:
                    test_detail_url = links[0]
                    logger.info(f"æµ‹è¯•è¯¦æƒ…URL: {test_detail_url}")
                    
                    detail = crawler.crawl_news_detail(test_detail_url, 'åŸºé‡‘')
                    if detail:
                        logger.info(f"âœ… æˆåŠŸçˆ¬å–æ–°é—»è¯¦æƒ…:")
                        logger.info(f"  æ ‡é¢˜: {detail.get('title', 'N/A')}")
                        logger.info(f"  å‘å¸ƒæ—¶é—´: {detail.get('pub_time', 'N/A')}")
                        logger.info(f"  ä½œè€…: {detail.get('author', 'N/A')}")
                        logger.info(f"  å†…å®¹é•¿åº¦: {len(detail.get('content', '') or '')}")
                    else:
                        logger.error("âŒ æ— æ³•çˆ¬å–æ–°é—»è¯¦æƒ…")
        
        return True
        
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        return False
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_url_access():
    """æµ‹è¯•URLè®¿é—®èƒ½åŠ›"""
    logger.info("=== æµ‹è¯•URLè®¿é—®èƒ½åŠ› ===")
    import requests
    
    test_urls = [
        "https://fund.eastmoney.com/",
        "https://fund.eastmoney.com/news/cjjj.html",
        "https://finance.eastmoney.com/",
        "https://www.eastmoney.com/"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    for url in test_urls:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                logger.info(f"âœ… {url} - çŠ¶æ€ç : {response.status_code}, å†…å®¹é•¿åº¦: {len(response.text)}")
            else:
                logger.warning(f"âš ï¸ {url} - çŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            logger.error(f"âŒ {url} - è®¿é—®å¤±è´¥: {e}")

if __name__ == "__main__":
    logger.info("å¼€å§‹è°ƒè¯•ä¸œæ–¹è´¢å¯Œçˆ¬è™«")
    
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    os.makedirs('data', exist_ok=True)
    
    # æµ‹è¯•URLè®¿é—®
    test_url_access()
    
    # æµ‹è¯•çˆ¬è™«
    success = test_eastmoney_crawler()
    
    if success:
        logger.info("ğŸ‰ è°ƒè¯•æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("ğŸ’¥ è°ƒè¯•æµ‹è¯•å¤±è´¥ï¼")
