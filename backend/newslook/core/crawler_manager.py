#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåŒ–çˆ¬è™«ç®¡ç†å™¨
å®ç°çº¿ç¨‹å®‰å…¨ã€è¿æ¥æ± ã€é…ç½®é‡è¯•æœºåˆ¶
"""

import threading
import time
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
from contextlib import contextmanager
from backend.newslook.core.config_manager import get_config, get_connection_pool
from backend.newslook.core.error_handler import get_error_handler, auto_retry

logger = logging.getLogger(__name__)

class CrawlerManager:
    """å¼ºåŒ–çˆ¬è™«ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«ç®¡ç†å™¨"""
        self.lock = threading.Lock()  # ğŸ”§ çº¿ç¨‹å®‰å…¨é”
        self.crawlers: Dict[str, Any] = {}
        self.crawler_threads: Dict[str, threading.Thread] = {}
        self.stop_flags: Dict[str, threading.Event] = {}
        self.connection_pool = get_connection_pool()  # ğŸ”§ è¿æ¥æ± 
        self.error_handler = get_error_handler()
        self.config = None
        
        # ğŸ”§ é…ç½®é‡è¯•æœºåˆ¶
        self._load_config_with_retry()
        
        logger.info("ğŸ”§ å¼ºåŒ–çˆ¬è™«ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    @auto_retry(max_attempts=3, backoff_factor=1.0)
    def _load_config_with_retry(self):
        """é…ç½®é‡è¯•æœºåˆ¶"""
        try:
            self.config = get_config()
            logger.info("ğŸ”§ é…ç½®åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"ğŸ”§ é…ç½®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def register_crawler(self, name: str, crawler_class: Any, config: Dict[str, Any] = None):
        """æ³¨å†Œçˆ¬è™«"""
        with self.lock:
            try:
                # ğŸ”§ çº¿ç¨‹å®‰å…¨çš„çˆ¬è™«æ³¨å†Œ
                if name not in self.crawlers:
                    self.crawlers[name] = crawler_class(config or {})
                    self.stop_flags[name] = threading.Event()
                    logger.info(f"ğŸ”§ çˆ¬è™«æ³¨å†ŒæˆåŠŸ: {name}")
                else:
                    logger.warning(f"ğŸ”§ çˆ¬è™«å·²å­˜åœ¨: {name}")
            except Exception as e:
                error_response = self.error_handler.handle_crawler_error(e, name)
                logger.error(f"ğŸ”§ çˆ¬è™«æ³¨å†Œå¤±è´¥: {name}, é”™è¯¯: {error_response.message}")
                raise
    
    def start_crawler(self, name: str) -> bool:
        """å¯åŠ¨çˆ¬è™«"""
        with self.lock:
            try:
                if name not in self.crawlers:
                    raise ValueError(f"çˆ¬è™«ä¸å­˜åœ¨: {name}")
                
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿è¡Œ
                if name in self.crawler_threads and self.crawler_threads[name].is_alive():
                    logger.warning(f"ğŸ”§ çˆ¬è™«å·²åœ¨è¿è¡Œ: {name}")
                    return False
                
                # æ¸…é™¤åœæ­¢æ ‡å¿—
                self.stop_flags[name].clear()
                
                # åˆ›å»ºçˆ¬è™«çº¿ç¨‹
                crawler_thread = threading.Thread(
                    target=self._run_crawler_safely,
                    args=(name,),
                    name=f"Crawler-{name}",
                    daemon=True
                )
                
                self.crawler_threads[name] = crawler_thread
                crawler_thread.start()
                
                logger.info(f"ğŸ”§ çˆ¬è™«å¯åŠ¨æˆåŠŸ: {name}")
                return True
                
            except Exception as e:
                error_response = self.error_handler.handle_crawler_error(e, name)
                logger.error(f"ğŸ”§ çˆ¬è™«å¯åŠ¨å¤±è´¥: {name}, é”™è¯¯: {error_response.message}")
                return False
    
    def stop_crawler(self, name: str) -> bool:
        """åœæ­¢çˆ¬è™«"""
        with self.lock:
            try:
                if name not in self.stop_flags:
                    raise ValueError(f"çˆ¬è™«ä¸å­˜åœ¨: {name}")
                
                # è®¾ç½®åœæ­¢æ ‡å¿—
                self.stop_flags[name].set()
                
                # ç­‰å¾…çº¿ç¨‹ç»“æŸ
                if name in self.crawler_threads:
                    thread = self.crawler_threads[name]
                    if thread.is_alive():
                        thread.join(timeout=10)  # ç­‰å¾…æœ€å¤š10ç§’
                        if thread.is_alive():
                            logger.warning(f"ğŸ”§ çˆ¬è™«æœªèƒ½æ­£å¸¸åœæ­¢: {name}")
                            return False
                    
                    # æ¸…ç†çº¿ç¨‹
                    del self.crawler_threads[name]
                
                logger.info(f"ğŸ”§ çˆ¬è™«åœæ­¢æˆåŠŸ: {name}")
                return True
                
            except Exception as e:
                error_response = self.error_handler.handle_crawler_error(e, name)
                logger.error(f"ğŸ”§ çˆ¬è™«åœæ­¢å¤±è´¥: {name}, é”™è¯¯: {error_response.message}")
                return False
    
    def _run_crawler_safely(self, name: str):
        """å®‰å…¨è¿è¡Œçˆ¬è™«"""
        try:
            crawler = self.crawlers[name]
            stop_flag = self.stop_flags[name]
            
            # ğŸ”§ ä½¿ç”¨è¿æ¥æ± è·å–æ•°æ®åº“è¿æ¥
            db_path = self.config.database.main_db
            
            with self.connection_pool.get_connection(db_path) as conn:
                # è¿è¡Œçˆ¬è™«
                while not stop_flag.is_set():
                    try:
                        # æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
                        if hasattr(crawler, 'run'):
                            crawler.run()
                        else:
                            logger.warning(f"ğŸ”§ çˆ¬è™«æ²¡æœ‰runæ–¹æ³•: {name}")
                            break
                        
                        # æ£€æŸ¥åœæ­¢æ ‡å¿—
                        if stop_flag.wait(timeout=self.config.crawler.delay):
                            break
                            
                    except Exception as e:
                        error_response = self.error_handler.handle_crawler_error(e, name)
                        logger.error(f"ğŸ”§ çˆ¬è™«è¿è¡Œé”™è¯¯: {name}, é”™è¯¯: {error_response.message}")
                        
                        # çŸ­æš‚ä¼‘çœ åç»§ç»­
                        if not stop_flag.wait(timeout=5):
                            continue
                        else:
                            break
                            
        except Exception as e:
            error_response = self.error_handler.handle_crawler_error(e, name)
            logger.error(f"ğŸ”§ çˆ¬è™«è¿è¡Œå¼‚å¸¸: {name}, é”™è¯¯: {error_response.message}")
        
        finally:
            logger.info(f"ğŸ”§ çˆ¬è™«è¿è¡Œç»“æŸ: {name}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«çŠ¶æ€"""
        with self.lock:
            status = {
                'total_crawlers': len(self.crawlers),
                'running_crawlers': 0,
                'stopped_crawlers': 0,
                'crawlers': {}
            }
            
            for name in self.crawlers:
                is_running = (name in self.crawler_threads and 
                            self.crawler_threads[name].is_alive())
                is_stopping = self.stop_flags[name].is_set()
                
                if is_running:
                    status['running_crawlers'] += 1
                    crawler_status = 'running'
                elif is_stopping:
                    crawler_status = 'stopping'
                else:
                    status['stopped_crawlers'] += 1
                    crawler_status = 'stopped'
                
                status['crawlers'][name] = {
                    'status': crawler_status,
                    'is_running': is_running,
                    'is_stopping': is_stopping,
                    'thread_alive': name in self.crawler_threads and self.crawler_threads[name].is_alive()
                }
            
            return status
    
    def shutdown(self):
        """å…³é—­çˆ¬è™«ç®¡ç†å™¨"""
        logger.info("ğŸ”§ å¼€å§‹å…³é—­çˆ¬è™«ç®¡ç†å™¨")
        
        # åœæ­¢æ‰€æœ‰çˆ¬è™«
        for name in list(self.crawlers.keys()):
            self.stop_crawler(name)
        
        # å…³é—­è¿æ¥æ± 
        self.connection_pool.close_all()
        
        logger.info("ğŸ”§ çˆ¬è™«ç®¡ç†å™¨å…³é—­å®Œæˆ")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.shutdown()

# å…¨å±€çˆ¬è™«ç®¡ç†å™¨å®ä¾‹
_crawler_manager = None

def get_crawler_manager() -> CrawlerManager:
    """è·å–å…¨å±€çˆ¬è™«ç®¡ç†å™¨å®ä¾‹"""
    global _crawler_manager
    if _crawler_manager is None:
        _crawler_manager = CrawlerManager()
    return _crawler_manager 