#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è´¢ç»æ–°é—»çˆ¬è™«ç³»ç»Ÿ - çˆ¬è™«åŒ…åˆå§‹åŒ–æ–‡ä»¶
"""

# å¯ç”¨çš„çˆ¬è™«ç±»æ˜ å°„
CRAWLER_CLASSES = {}

# å°è¯•å¯¼å…¥å„ä¸ªçˆ¬è™«ç±»ï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
try:
    from backend.newslook.crawlers.base import BaseCrawler
    print("âœ… æˆåŠŸå¯¼å…¥ BaseCrawler")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥ BaseCrawler å¤±è´¥: {str(e)}")

try:
    from backend.newslook.crawlers.eastmoney import EastMoneyCrawler
    CRAWLER_CLASSES["ä¸œæ–¹è´¢å¯Œ"] = EastMoneyCrawler
    print("âœ… æˆåŠŸå¯¼å…¥ EastMoneyCrawler")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥ EastMoneyCrawler å¤±è´¥: {str(e)}")

try:
    from backend.newslook.crawlers.sina import SinaCrawler
    CRAWLER_CLASSES["æ–°æµªè´¢ç»"] = SinaCrawler
    print("âœ… æˆåŠŸå¯¼å…¥ SinaCrawler")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥ SinaCrawler å¤±è´¥: {str(e)}")

try:
    from backend.newslook.crawlers.tencent import TencentCrawler
    CRAWLER_CLASSES["è…¾è®¯è´¢ç»"] = TencentCrawler
    print("âœ… æˆåŠŸå¯¼å…¥ TencentCrawler")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥ TencentCrawler å¤±è´¥: {str(e)}")

try:
    from backend.newslook.crawlers.netease import NeteaseCrawler
    CRAWLER_CLASSES["ç½‘æ˜“è´¢ç»"] = NeteaseCrawler
    print("âœ… æˆåŠŸå¯¼å…¥ NeteaseCrawler")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥ NeteaseCrawler å¤±è´¥: {str(e)}")

try:
    from backend.newslook.crawlers.ifeng import IfengCrawler
    CRAWLER_CLASSES["å‡¤å‡°è´¢ç»"] = IfengCrawler
    print("âœ… æˆåŠŸå¯¼å…¥ IfengCrawler")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥ IfengCrawler å¤±è´¥: {str(e)}")

print(f"ğŸ“Š å…±æˆåŠŸå¯¼å…¥ {len(CRAWLER_CLASSES)} ä¸ªçˆ¬è™«ç±»")

def get_crawler(source, db_manager=None, use_proxy=False, use_source_db=False, db_path=None):
    """
    è·å–æŒ‡å®šæ¥æºçš„çˆ¬è™«å®ä¾‹
    
    Args:
        source: çˆ¬è™«æ¥æºåç§°
        db_manager: æ•°æ®åº“ç®¡ç†å™¨å¯¹è±¡
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        use_source_db: æ˜¯å¦ä½¿ç”¨æ¥æºä¸“ç”¨æ•°æ®åº“
        db_path: æ•°æ®åº“è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    
    Returns:
        BaseCrawler: çˆ¬è™«å®ä¾‹
    
    Raises:
        ValueError: å¦‚æœæŒ‡å®šçš„æ¥æºä¸å­˜åœ¨
    """
    if source not in CRAWLER_CLASSES:
        available_sources = ', '.join(CRAWLER_CLASSES.keys()) if CRAWLER_CLASSES else 'æ— å¯ç”¨çˆ¬è™«'
        raise ValueError(f"ä¸æ”¯æŒçš„çˆ¬è™«æ¥æº: {source}ï¼Œå¯ç”¨æ¥æº: {available_sources}")
    
    crawler_class = CRAWLER_CLASSES[source]
    return crawler_class(db_manager=db_manager, use_proxy=use_proxy, use_source_db=use_source_db, db_path=db_path)

def get_all_crawlers(db_manager=None, use_proxy=False, use_source_db=False, db_path=None):
    """
    è·å–æ‰€æœ‰çˆ¬è™«å®ä¾‹
    
    Args:
        db_manager: æ•°æ®åº“ç®¡ç†å™¨å¯¹è±¡
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        use_source_db: æ˜¯å¦ä½¿ç”¨æ¥æºä¸“ç”¨æ•°æ®åº“
        db_path: æ•°æ®åº“è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    
    Returns:
        list: çˆ¬è™«å®ä¾‹åˆ—è¡¨
    """
    crawlers = []
    for source in CRAWLER_CLASSES:
        try:
            crawler = get_crawler(source, db_manager, use_proxy, use_source_db, db_path)
            crawlers.append(crawler)
        except Exception as e:
            print(f"åˆå§‹åŒ–çˆ¬è™« {source} å¤±è´¥: {str(e)}")
    
    return crawlers

def get_crawler_sources():
    """
    è·å–æ‰€æœ‰æ”¯æŒçš„çˆ¬è™«æ¥æº
    
    Returns:
        list: çˆ¬è™«æ¥æºåˆ—è¡¨
    """
    return list(CRAWLER_CLASSES.keys())
