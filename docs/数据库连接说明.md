# 爬虫系统数据库连接说明

本文档描述了财经新闻爬虫系统中的数据库连接机制，包括各爬虫如何连接到数据库以及如何存储新闻数据。

## 数据库架构

系统使用SQLite数据库来存储爬取的新闻数据。主要通过`SQLiteManager`类提供统一的数据库操作接口。

### 数据库文件位置

数据库文件通常存储在以下位置：

- `data/db/` 目录下的`.db`文件
- `run_crawler.py`脚本运行时会自动创建新的数据库文件，文件名格式为`[source_name]_YYYYMMDD_HHMMSS.db`

其中`source_name`是爬虫来源的英文名称，例如：
- `tencent_finance` - 腾讯财经
- `sina_finance` - 新浪财经
- `eastmoney` - 东方财富
- `netease_finance` - 网易财经
- `ifeng_finance` - 凤凰财经
- `all_sources` - 所有爬虫来源（未指定具体来源时）

这种命名方式使得数据库文件一目了然，便于管理和识别不同来源的新闻数据。

### 数据表结构

主要的数据表包括：

1. **news** - 存储新闻基本信息
   - id（主键）：新闻ID
   - title：新闻标题
   - content：新闻内容（纯文本）
   - content_html：新闻内容（HTML格式）
   - pub_time：发布时间
   - author：作者
   - source：来源
   - url：新闻URL
   - keywords：关键词
   - images：图片URL
   - related_stocks：相关股票
   - sentiment：情感倾向
   - classification：新闻分类
   - category：新闻类别
   - crawl_time：爬取时间

2. **keywords** - 存储关键词
   - id（主键）：关键词ID
   - keyword：关键词
   - count：出现次数
   - last_updated：最后更新时间

3. **news_keywords** - 新闻和关键词的关联表
   - news_id：新闻ID
   - keyword_id：关键词ID

4. **stocks** - 存储股票信息
   - code（主键）：股票代码
   - name：股票名称
   - count：出现次数
   - last_updated：最后更新时间

5. **news_stocks** - 新闻和股票的关联表
   - news_id：新闻ID
   - stock_code：股票代码

## 数据库连接机制

### SQLiteManager 类

`SQLiteManager`类（位于`app/db/sqlite_manager.py`）是系统的核心数据库管理类，它提供了以下功能：

- 初始化数据库连接和表结构
- 提供新闻保存接口
- 提供新闻查询接口
- 支持关键词和股票信息的提取和关联

### 爬虫与数据库的集成

所有爬虫类都继承自`BaseCrawler`，并通过以下方式与数据库交互：

1. **初始化连接**：
   - 爬虫类初始化时可以传入一个`db_manager`参数或`db_path`参数
   - 如果提供了`db_manager`，爬虫会使用该管理器进行数据库操作
   - 如果提供了`db_path`，爬虫会创建一个指向该路径的数据库连接
   - 如果都没有提供，爬虫会使用默认路径

2. **保存数据**：
   - 爬虫通过调用`save_news_to_db`方法将新闻保存到数据库
   - 优先使用`sqlite_manager`属性保存数据
   - 如果没有该属性，则使用父类的`save_news`方法保存到内存

3. **爬取完成后的保存**：
   - 爬虫的`crawl`方法结束时会检查内存中的新闻列表
   - 将所有新闻保存到数据库
   - 记录成功保存的数量

## 各爬虫的数据库连接

系统中的所有爬虫（东方财富、新浪财经、腾讯财经、网易财经、凤凰财经）都使用相同的数据库连接机制：

1. **创建SQLiteManager实例**：
   ```python
   if db_manager and not isinstance(db_manager, SQLiteManager):
       if hasattr(db_manager, 'db_path'):
           self.sqlite_manager = SQLiteManager(db_manager.db_path)
       else:
           self.sqlite_manager = SQLiteManager(db_path or self.db_path)
   elif not db_manager:
       self.sqlite_manager = SQLiteManager(db_path or self.db_path)
   else:
       self.sqlite_manager = db_manager
   ```

2. **保存新闻方法**：
   ```python
   def save_news_to_db(self, news):
       try:
           if hasattr(self, 'sqlite_manager') and self.sqlite_manager:
               return self.sqlite_manager.save_news(news)
           return super().save_news(news)
       except Exception as e:
           logger.error(f"保存新闻到数据库失败: {news.get('title', '未知标题')}, 错误: {str(e)}")
           return False
   ```

3. **爬取完成后的批量保存**：
   ```python
   # 爬取结束后，确保数据被保存到数据库
   if hasattr(self, 'news_data') and self.news_data:
       saved_count = 0
       for news in self.news_data:
           if self.save_news_to_db(news):
               saved_count += 1
       logger.info(f"成功保存 {saved_count}/{len(self.news_data)} 条新闻到数据库")
   ```

## 多数据库支持

系统支持使用多个数据库文件，具有以下特点：

1. **自动数据库发现**：系统会自动搜索可能的数据库目录，查找并使用所有可用的数据库文件
2. **灵活的数据库位置**：支持多个可能的数据库位置
3. **命令行参数**：通过`--db-dir`参数显式指定数据库目录
4. **合并查询结果**：从多个数据库文件中查询并合并结果

## 运行时使用

在运行爬虫时，可以通过以下方式指定数据库：

```bash
# 使用默认数据库
python run_crawler.py --source "腾讯财经" --days 1

# 指定数据库路径
python run_crawler.py --source "腾讯财经" --days 1 --db-path /path/to/your/database.db
```

## 数据库接口

### 保存新闻

```python
# 使用SQLiteManager保存新闻
db_manager = SQLiteManager('path/to/db.db')
db_manager.save_news({
    'id': 'unique_id',
    'title': '新闻标题',
    'content': '新闻内容',
    'pub_time': '2025-03-22 10:00:00',
    'author': '作者',
    'source': '新闻来源',
    'url': 'https://example.com/news',
    'keywords': ['关键词1', '关键词2'],
    'images': ['https://example.com/image1.jpg'],
    'related_stocks': ['000001:平安银行', '600000:浦发银行'],
    'sentiment': '正面',
    'classification': '财经',
    'category': '股票',
})
```

### 查询新闻

```python
# 获取指定ID的新闻
news = db_manager.get_news('unique_id')

# 获取新闻列表
news_list = db_manager.get_news_list(limit=10, category='财经')

# 获取新闻数量
count = db_manager.get_news_count(category='财经')
```

## 最佳实践

1. **使用统一的数据库接口**：所有爬虫都应使用`SQLiteManager`类进行数据库操作
2. **异常处理**：所有数据库操作都应包含适当的异常处理
3. **日志记录**：记录所有数据库操作的结果
4. **数据验证**：保存前验证新闻数据的完整性
5. **定期备份**：定期备份数据库文件，以防数据丢失
6. **使用事务**：对于批量操作，使用事务来确保数据一致性 

## 数据库文件存储优化

为了优化数据库存储和访问，系统进行了以下更新：

### 固定数据库文件名

系统已从原来每次爬取都创建新的数据库文件（使用时间戳命名）改为为每个爬虫源使用固定的数据库文件名：

- 旧命名方式：`[source_name]_YYYYMMDD_HHMMSS.db`（例如：`tencent_finance_20250321_102233.db`）
- 新命名方式：`[source_name].db`（例如：`腾讯财经.db`）

这种改进带来以下优势：
1. **减少数据库文件数量**：不再为每次爬取创建新文件，避免数据库文件爆炸性增长
2. **简化数据访问**：网页应用始终能找到最新的数据，而不必搜索多个数据库文件
3. **减少数据碎片**：所有来自同一源的数据都集中存储在同一个文件中
4. **便于管理与备份**：固定的文件名使管理和备份操作更加简单直接

### 直接数据库访问

爬虫类现在实现了直接使用`sqlite3`连接将新闻保存到数据库的功能，减少了中间层次，确保数据更可靠地存储：

```python
def save_news_to_db(self, news):
    """保存新闻到数据库"""
    try:
        # 直接使用sqlite3连接将新闻保存到数据库
        import sqlite3
        
        # 连接数据库
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 执行INSERT语句
        sql = """
        INSERT OR REPLACE INTO news (
            id, title, content, pub_time, author, source, url, 
            keywords, sentiment, crawl_time, category, images, 
            related_stocks, content_html, classification
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        
        # 执行并提交
        cursor.execute(sql, values)
        conn.commit()
        conn.close()
        
        return True
    except Exception as e:
        logger.error(f"保存新闻到数据库失败: {news.get('title', '未知标题')}, 错误: {str(e)}")
        return False
```

### 数据库表初始化

系统运行时自动检查并创建所需的数据库表结构：

```python
# 确保数据库表已创建
if hasattr(crawler, 'db_manager') and hasattr(crawler.db_manager, 'init_db'):
    try:
        if hasattr(crawler, 'conn'):
            crawler.db_manager.init_db(crawler.conn)
            logger.info("数据库表初始化完成")
        else:
            logger.warning("爬虫没有数据库连接")
    except Exception as e:
        logger.error(f"初始化数据库表失败: {str(e)}")
```

### 路径处理优化

Web应用中的`NewsDatabase`类增加了对相对路径的智能处理，可以在多个可能的位置查找数据库文件：

```python
# 使用相对路径或绝对路径查找数据库
possible_paths = [
    self.db_dir,
    os.path.join(os.getcwd(), self.db_dir),
    os.path.join(os.getcwd(), 'data', 'db'),
    os.path.join('data', 'db')
]
for path in possible_paths:
    if os.path.exists(path):
        pattern = os.path.join(path, '*.db')
        db_files.extend(glob.glob(pattern))
```

这些优化确保了数据的更高效存储和更可靠的访问，同时简化了系统的维护和管理。 