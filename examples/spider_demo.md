# NewsLook 爬虫示例脚本使用指南

## 概述

`spider_demo.py` 是 NewsLook 金融新闻爬虫系统的示例脚本，用于演示如何使用系统中的爬虫组件抓取财经新闻数据。该脚本支持三种爬取模式（同步、异步和增强型异步），并将结果保存到 SQLite 数据库和 JSON 文件中。

## 功能特点

- 支持三种爬取模式的性能对比：同步、异步和增强型异步
- 自动处理数据存储，同时保存到数据库和 JSON 文件
- 记录详细的爬取统计信息，包括耗时、成功/失败计数等
- 智能处理不同环境下的异步执行条件
- 支持多个财经新闻源：东方财富、新浪财经、网易财经、凤凰财经等

## 使用方法

### 命令行参数

脚本支持以下命令行参数：

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `--mode` | 爬取模式: `sync`=同步, `async`=异步, `enhanced`=增强型, `all`=全部 | `all` |
| `--source` | 新闻源名称，支持：东方财富、新浪财经、网易财经、凤凰财经 | 新浪财经 |
| `--days` | 要爬取的天数 | 1 |
| `--max` | 每个源最大爬取的新闻数量 | 20 |
| `--category` | 要爬取的新闻分类，多个分类用逗号分隔 | 无（所有分类） |

### 使用示例

1. **使用默认参数运行**（爬取新浪财经最近1天的新闻，最多20篇）：
   ```
   python examples/spider_demo.py
   ```

2. **爬取指定新闻源**：
   ```
   python examples/spider_demo.py --source 东方财富
   ```

3. **仅使用同步模式爬取**：
   ```
   python examples/spider_demo.py --mode sync --source 网易财经
   ```

4. **使用增强型爬虫爬取更多天数的新闻**：
   ```
   python examples/spider_demo.py --mode enhanced --source 凤凰财经 --days 3 --max 50
   ```

5. **爬取特定分类的新闻**：
   ```
   python examples/spider_demo.py --source 新浪财经 --category 财经,科技
   ```

## 输出内容

脚本运行后会生成以下输出：

1. **数据库文件**：
   - 同步模式：`data/spider_example/{source}_sync.db`
   - 异步模式：`data/spider_example/{source}_async.db`
   - 增强型异步：`data/spider_example/{source}_enhanced.db`

2. **JSON 结果文件**：
   - 同步模式：`data/spider_example/{source}_sync_{timestamp}.json`
   - 异步模式：`data/spider_example/{source}_async_{timestamp}.json`
   - 增强型异步：`data/spider_example/{source}_enhanced_{timestamp}.json`

3. **控制台日志**：
   - 爬取过程的详细日志
   - 三种模式的性能对比结果（如果使用 `--mode all`）

## 爬虫模式说明

### 同步爬虫

同步爬虫按顺序发送请求、处理响应，优点是实现简单，缺点是速度较慢。适合小规模爬取任务或对目标网站负载敏感的场景。

### 异步爬虫

异步爬虫利用 `asyncio` 并行发送请求，大幅提高爬取速度。适合需要快速获取大量数据的场景。

### 增强型异步爬虫

增强型异步爬虫在标准异步爬虫基础上增加了：
- 智能并发控制：限制总并发请求数和对单一域名的并发请求数
- 域名请求延迟：为每个域名设置请求间隔，避免触发反爬机制
- 高级错误处理：更好地处理网络波动和临时性错误

增强型爬虫同样利用异步机制提高速度，但更加稳定，适合生产环境中的大规模爬取任务。

## 注意事项

1. 首次运行脚本会在项目根目录下自动创建 `data/spider_example` 目录用于存储输出结果
2. 在某些环境下（如已有事件循环运行时），`asyncio.run()` 可能会失败，脚本会自动切换到传统的事件循环创建方法
3. 每次运行脚本都会生成新的 JSON 结果文件（使用时间戳区分），但数据库文件会被覆盖
4. 爬取大量数据或增加爬取天数可能会导致耗时增加，请耐心等待脚本完成
5. 爬取速度受网络状况和目标网站响应速度影响，实际性能可能有所波动 