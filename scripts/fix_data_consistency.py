#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
NewsLookæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®å¤è„šæœ¬
è§£å†³æ•°æ®æ¦‚è§ˆä¸æ–°é—»åˆ—è¡¨æ˜¾ç¤ºæ•°é‡ä¸ä¸€è‡´çš„é—®é¢˜
"""

import os
import sys
import sqlite3
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Set, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataConsistencyFixer:
    """æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®å¤å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨"""
        self.project_root = project_root
        self.data_dir = project_root / 'data' / 'db'
        self.main_db = self.data_dir / 'finance_news.db'
        self.sources_dir = self.data_dir / 'sources'
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.sources_dir.mkdir(exist_ok=True)
        
        self.report = {
            'timestamp': datetime.now().isoformat(),
            'main_db_stats': {},
            'source_db_stats': {},
            'duplicate_analysis': {},
            'recommendations': []
        }
    
    def check_database_files(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶çŠ¶æ€"""
        logger.info("æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶çŠ¶æ€...")
        
        file_status = {
            'main_db_exists': self.main_db.exists(),
            'main_db_size': self.main_db.stat().st_size if self.main_db.exists() else 0,
            'source_dbs': []
        }
        
        # æ£€æŸ¥æºæ•°æ®åº“
        if self.sources_dir.exists():
            for db_file in self.sources_dir.glob('*.db'):
                file_status['source_dbs'].append({
                    'name': db_file.stem,
                    'path': str(db_file),
                    'size': db_file.stat().st_size
                })
        
        # æ£€æŸ¥dataç›®å½•ä¸‹çš„æ—§æ•°æ®åº“æ–‡ä»¶
        old_dbs = []
        for db_file in self.data_dir.parent.glob('*.db'):
            if db_file.parent == self.data_dir.parent:
                old_dbs.append({
                    'name': db_file.stem,
                    'path': str(db_file),
                    'size': db_file.stat().st_size
                })
        
        file_status['old_dbs'] = old_dbs
        
        logger.info(f"ä¸»æ•°æ®åº“: {'å­˜åœ¨' if file_status['main_db_exists'] else 'ä¸å­˜åœ¨'}")
        logger.info(f"æºæ•°æ®åº“: {len(file_status['source_dbs'])} ä¸ª")
        logger.info(f"æ—§æ•°æ®åº“: {len(old_dbs)} ä¸ª")
        
        return file_status
    
    def analyze_data_duplicates(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®é‡å¤æƒ…å†µ"""
        logger.info("åˆ†ææ•°æ®é‡å¤æƒ…å†µ...")
        
        all_urls = {}  # URL -> [æ•°æ®åº“æ¥æºåˆ—è¡¨]
        db_stats = {}  # æ•°æ®åº“ -> ç»Ÿè®¡ä¿¡æ¯
        
        # åˆ†æä¸»æ•°æ®åº“
        if self.main_db.exists():
            main_stats = self._analyze_single_db(str(self.main_db), 'ä¸»æ•°æ®åº“')
            db_stats['main'] = main_stats
            
            for url in main_stats['urls']:
                if url not in all_urls:
                    all_urls[url] = []
                all_urls[url].append('ä¸»æ•°æ®åº“')
        
        # åˆ†ææºæ•°æ®åº“
        if self.sources_dir.exists():
            for db_file in self.sources_dir.glob('*.db'):
                source_name = db_file.stem
                source_stats = self._analyze_single_db(str(db_file), source_name)
                db_stats[source_name] = source_stats
                
                for url in source_stats['urls']:
                    if url not in all_urls:
                        all_urls[url] = []
                    all_urls[url].append(source_name)
        
        # åˆ†æé‡å¤æƒ…å†µ
        duplicates = {}
        unique_urls = set()
        
        for url, sources in all_urls.items():
            if len(sources) > 1:
                duplicates[url] = sources
            unique_urls.add(url)
        
        analysis = {
            'total_urls': len(all_urls),
            'unique_urls': len(unique_urls),
            'duplicate_urls': len(duplicates),
            'duplicate_details': duplicates,
            'db_stats': db_stats
        }
        
        logger.info(f"æ€»URLæ•°: {analysis['total_urls']}")
        logger.info(f"å”¯ä¸€URLæ•°: {analysis['unique_urls']}")
        logger.info(f"é‡å¤URLæ•°: {analysis['duplicate_urls']}")
        
        return analysis
    
    def _analyze_single_db(self, db_path: str, db_name: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ•°æ®åº“"""
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news'")
            if not cursor.fetchone():
                logger.warning(f"{db_name} ä¸­æ²¡æœ‰newsè¡¨")
                conn.close()
                return {
                    'total_count': 0,
                    'urls': [],
                    'sources': set(),
                    'has_table': False
                }
            
            # è·å–æ‰€æœ‰æ–°é—»è®°å½•
            cursor.execute("SELECT url, source, title FROM news WHERE url IS NOT NULL AND url != ''")
            rows = cursor.fetchall()
            
            urls = [row[0] for row in rows]
            sources = set(row[1] for row in rows if row[1])
            
            stats = {
                'total_count': len(rows),
                'unique_url_count': len(set(urls)),
                'urls': urls,
                'sources': sources,
                'has_table': True
            }
            
            conn.close()
            logger.info(f"{db_name}: {stats['total_count']} æ¡è®°å½•, {stats['unique_url_count']} ä¸ªå”¯ä¸€URL")
            
            return stats
            
        except Exception as e:
            logger.error(f"åˆ†ææ•°æ®åº“ {db_name} å¤±è´¥: {e}")
            return {
                'total_count': 0,
                'urls': [],
                'sources': set(),
                'has_table': False,
                'error': str(e)
            }
    
    def test_api_consistency(self) -> Dict[str, Any]:
        """æµ‹è¯•APIæ•°æ®ä¸€è‡´æ€§"""
        logger.info("æµ‹è¯•APIæ•°æ®ä¸€è‡´æ€§...")
        
        try:
            # æµ‹è¯•ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨
            from backend.newslook.core.unified_database_manager import get_unified_database_manager
            
            db_manager = get_unified_database_manager()
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = db_manager.get_database_stats()
            
            # è·å–æ–°é—»æ•°é‡
            total_count = db_manager.get_news_count()
            
            # è·å–æ–°é—»åˆ—è¡¨
            news_list = db_manager.query_news(use_all_sources=True, limit=1000)
            
            api_test = {
                'database_stats': stats,
                'news_count_method': total_count,
                'news_list_count': len(news_list),
                'consistency_check': {
                    'stats_vs_count': stats['total_news'] == total_count,
                    'stats_vs_list': stats['total_news'] == len(news_list),
                    'count_vs_list': total_count == len(news_list)
                }
            }
            
            logger.info(f"æ•°æ®åº“ç»Ÿè®¡API: {stats['total_news']} æ¡")
            logger.info(f"è®¡æ•°æ–¹æ³•API: {total_count} æ¡")
            logger.info(f"æ–°é—»åˆ—è¡¨API: {len(news_list)} æ¡")
            
            # æ£€æŸ¥ä¸€è‡´æ€§
            if api_test['consistency_check']['stats_vs_count'] and \
               api_test['consistency_check']['stats_vs_list'] and \
               api_test['consistency_check']['count_vs_list']:
                logger.info("âœ… APIæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
            else:
                logger.warning("âŒ APIæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥")
            
            return api_test
            
        except Exception as e:
            logger.error(f"APIä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
            return {
                'error': str(e),
                'consistency_check': {
                    'stats_vs_count': False,
                    'stats_vs_list': False,
                    'count_vs_list': False
                }
            }
    
    def generate_recommendations(self, file_status: Dict, duplicates: Dict, api_test: Dict) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥æ•°æ®é‡å¤
        if duplicates['duplicate_urls'] > 0:
            recommendations.append(
                f"å‘ç° {duplicates['duplicate_urls']} ä¸ªé‡å¤URLï¼Œå»ºè®®æ¸…ç†é‡å¤æ•°æ®"
            )
        
        # æ£€æŸ¥APIä¸€è‡´æ€§
        if not all(api_test.get('consistency_check', {}).values()):
            recommendations.append(
                "APIæ•°æ®ä¸ä¸€è‡´ï¼Œå·²é€šè¿‡ä»£ç ä¿®å¤ï¼Œå»ºè®®é‡å¯åº”ç”¨éªŒè¯"
            )
        
        # æ£€æŸ¥æ—§æ•°æ®åº“æ–‡ä»¶
        if file_status.get('old_dbs'):
            recommendations.append(
                f"å‘ç° {len(file_status['old_dbs'])} ä¸ªæ—§æ•°æ®åº“æ–‡ä»¶ï¼Œå»ºè®®è¿ç§»åˆ°ç»Ÿä¸€ä½ç½®"
            )
        
        # æ£€æŸ¥æºæ•°æ®åº“
        if not file_status.get('source_dbs'):
            recommendations.append(
                "æœªå‘ç°æºæ•°æ®åº“ï¼Œå»ºè®®æ£€æŸ¥çˆ¬è™«é…ç½®"
            )
        
        return recommendations
    
    def run_full_check(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥"""
        logger.info("å¼€å§‹æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥...")
        
        # 1. æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
        file_status = self.check_database_files()
        self.report['file_status'] = file_status
        
        # 2. åˆ†ææ•°æ®é‡å¤
        duplicates = self.analyze_data_duplicates()
        self.report['duplicate_analysis'] = duplicates
        
        # 3. æµ‹è¯•APIä¸€è‡´æ€§
        api_test = self.test_api_consistency()
        self.report['api_consistency'] = api_test
        
        # 4. ç”Ÿæˆå»ºè®®
        recommendations = self.generate_recommendations(file_status, duplicates, api_test)
        self.report['recommendations'] = recommendations
        
        return self.report
    
    def save_report(self, filename: str = None):
        """ä¿å­˜æ£€æŸ¥æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"data_consistency_report_{timestamp}.json"
        
        report_path = self.project_root / filename
        
        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"æ£€æŸ¥æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path
    
    def print_summary(self):
        """æ‰“å°æ£€æŸ¥æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š NewsLookæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥æŠ¥å‘Š")
        print("="*60)
        
        # æ–‡ä»¶çŠ¶æ€
        file_status = self.report.get('file_status', {})
        print(f"\nğŸ“ æ•°æ®åº“æ–‡ä»¶çŠ¶æ€:")
        print(f"   ä¸»æ•°æ®åº“: {'âœ…' if file_status.get('main_db_exists') else 'âŒ'}")
        print(f"   æºæ•°æ®åº“: {len(file_status.get('source_dbs', []))} ä¸ª")
        print(f"   æ—§æ•°æ®åº“: {len(file_status.get('old_dbs', []))} ä¸ª")
        
        # é‡å¤åˆ†æ
        duplicates = self.report.get('duplicate_analysis', {})
        print(f"\nğŸ” æ•°æ®é‡å¤åˆ†æ:")
        print(f"   æ€»URLæ•°: {duplicates.get('total_urls', 0)}")
        print(f"   å”¯ä¸€URLæ•°: {duplicates.get('unique_urls', 0)}")
        print(f"   é‡å¤URLæ•°: {duplicates.get('duplicate_urls', 0)}")
        
        # APIä¸€è‡´æ€§
        api_test = self.report.get('api_consistency', {})
        consistency = api_test.get('consistency_check', {})
        print(f"\nğŸ”§ APIä¸€è‡´æ€§æ£€æŸ¥:")
        print(f"   ç»Ÿè®¡API: {api_test.get('database_stats', {}).get('total_news', 'N/A')} æ¡")
        print(f"   è®¡æ•°API: {api_test.get('news_count_method', 'N/A')} æ¡")
        print(f"   åˆ—è¡¨API: {api_test.get('news_list_count', 'N/A')} æ¡")
        print(f"   ä¸€è‡´æ€§: {'âœ…' if all(consistency.values()) else 'âŒ'}")
        
        # å»ºè®®
        recommendations = self.report.get('recommendations', [])
        print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        else:
            print("   âœ… æœªå‘ç°éœ€è¦ä¿®å¤çš„é—®é¢˜")
        
        print("\n" + "="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ NewsLookæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å·¥å…·")
    print("è§£å†³æ•°æ®æ¦‚è§ˆä¸æ–°é—»åˆ—è¡¨æ˜¾ç¤ºæ•°é‡ä¸ä¸€è‡´çš„é—®é¢˜")
    print("-" * 50)
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    fixer = DataConsistencyFixer()
    
    try:
        # è¿è¡Œå®Œæ•´æ£€æŸ¥
        report = fixer.run_full_check()
        
        # æ‰“å°æ‘˜è¦
        fixer.print_summary()
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = fixer.save_report()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é—®é¢˜
        api_consistency = report.get('api_consistency', {}).get('consistency_check', {})
        if not all(api_consistency.values()):
            print("\nâš ï¸  å‘ç°APIæ•°æ®ä¸ä¸€è‡´é—®é¢˜ï¼")
            print("ğŸ”§ å·²é€šè¿‡ä»£ç ä¿®å¤ç»Ÿè®¡é€»è¾‘ï¼Œè¯·é‡å¯åº”ç”¨åé‡æ–°æµ‹è¯•")
        else:
            print("\nâœ… æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ï¼")
        
        return True
        
    except Exception as e:
        logger.error(f"æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        print(f"\nâŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 