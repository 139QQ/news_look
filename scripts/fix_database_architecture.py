#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•°æ®åº“æ¶æ„ä¿®å¤è„šæœ¬
ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼š
1. å¼ºåˆ¶è·¯å¾„ç»Ÿä¸€ï¼šæ‰€æœ‰æ¨¡å—ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“è·¯å¾„
2. è¿æ¥æ± åŒ–ç®¡ç†ï¼šè‡ªåŠ¨è¿æ¥å›æ”¶å’Œå¤ç”¨
3. äº‹åŠ¡å¢å¼ºï¼šé˜²é”ç«äº‰å’Œå¤±è´¥å›æ»š
4. ä¿å­˜éªŒè¯ï¼šæ’å…¥åç«‹å³æ ¡éªŒ
5. é”™è¯¯ç†”æ–­ï¼šè‡ªåŠ¨é‡è¯•å’Œå‘Šè­¦æœºåˆ¶
"""

import os
import sys
import sqlite3
import logging
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(project_root / 'logs' / 'database_fix.log')
    ]
)
logger = logging.getLogger(__name__)

class DatabaseArchitectureFixer:
    """æ•°æ®åº“æ¶æ„ä¿®å¤å™¨"""
    
    def __init__(self):
        self.project_root = project_root
        self.unified_db_dir = project_root / 'data' / 'db'
        self.main_db_path = self.unified_db_dir / 'finance_news.db'
        
        # ç¡®ä¿ç»Ÿä¸€ç›®å½•å­˜åœ¨
        self.unified_db_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ”§ æ•°æ®åº“æ¶æ„ä¿®å¤å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ”§ ç»Ÿä¸€æ•°æ®åº“ç›®å½•: {self.unified_db_dir}")
        logger.info(f"ğŸ”§ ä¸»æ•°æ®åº“è·¯å¾„: {self.main_db_path}")
    
    def discover_old_databases(self):
        """å‘ç°æ‰€æœ‰æ—§ä½ç½®çš„æ•°æ®åº“æ–‡ä»¶"""
        old_db_files = []
        
        # æ£€æŸ¥å¯èƒ½çš„æ—§ä½ç½®
        search_locations = [
            self.project_root / 'data',
            self.project_root / 'data' / 'sources',
            self.project_root / 'data' / 'databases',
            self.project_root / 'databases'
        ]
        
        for location in search_locations:
            if location.exists():
                for db_file in location.glob('*.db'):
                    # æ’é™¤å·²ç»åœ¨ç»Ÿä¸€ä½ç½®çš„æ–‡ä»¶
                    if db_file.parent != self.unified_db_dir:
                        old_db_files.append(db_file)
                        logger.info(f"ğŸ” å‘ç°æ—§æ•°æ®åº“æ–‡ä»¶: {db_file}")
        
        return old_db_files
    
    def analyze_database_content(self, db_path):
        """åˆ†ææ•°æ®åº“å†…å®¹"""
        try:
            conn = sqlite3.connect(str(db_path), timeout=5)
            cursor = conn.cursor()
            
            # æ£€æŸ¥è¡¨ç»“æ„
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            analysis = {
                'path': str(db_path),
                'size_mb': db_path.stat().st_size / 1024 / 1024,
                'tables': tables
            }
            
            if 'news' in tables:
                cursor.execute("SELECT COUNT(*) FROM news")
                analysis['news_count'] = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(DISTINCT source) FROM news")
                analysis['sources_count'] = cursor.fetchone()[0]
                
                cursor.execute("SELECT source, COUNT(*) FROM news GROUP BY source")
                analysis['source_distribution'] = dict(cursor.fetchall())
            
            conn.close()
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ åˆ†ææ•°æ®åº“å¤±è´¥ {db_path}: {e}")
            return None
    
    def migrate_database_content(self, source_db_path, target_db_path):
        """è¿ç§»æ•°æ®åº“å†…å®¹"""
        try:
            # è¿æ¥æºæ•°æ®åº“
            source_conn = sqlite3.connect(str(source_db_path), timeout=10)
            source_conn.row_factory = sqlite3.Row
            
            # è¿æ¥ç›®æ ‡æ•°æ®åº“
            target_conn = sqlite3.connect(str(target_db_path), timeout=10)
            target_conn.execute("PRAGMA foreign_keys = ON")
            target_conn.execute("PRAGMA journal_mode = WAL")
            
            # ç¡®ä¿ç›®æ ‡æ•°æ®åº“æœ‰æ­£ç¡®çš„è¡¨ç»“æ„
            self._create_tables(target_conn)
            
            # è¿ç§»æ–°é—»æ•°æ®
            migrated_count = self._migrate_news_data(source_conn, target_conn)
            
            source_conn.close()
            target_conn.close()
            
            logger.info(f"âœ… æˆåŠŸè¿ç§» {migrated_count} æ¡æ–°é—»ä» {source_db_path.name}")
            return migrated_count
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»æ•°æ®åº“å¤±è´¥ {source_db_path} -> {target_db_path}: {e}")
            return 0
    
    def _create_tables(self, conn):
        """åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„"""
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                content_html TEXT,
                pub_time DATETIME NOT NULL,
                author TEXT,
                source TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                keywords TEXT,
                images TEXT,
                related_stocks TEXT,
                sentiment REAL,
                classification TEXT,
                category TEXT DEFAULT 'è´¢ç»',
                crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                summary TEXT,
                status INTEGER DEFAULT 0,
                
                CONSTRAINT news_url_unique UNIQUE (url)
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_source_time ON news(source, pub_time)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_category ON news(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_pub_time ON news(pub_time)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_url ON news(url)')
        
        conn.commit()
    
    def _migrate_news_data(self, source_conn, target_conn):
        """è¿ç§»æ–°é—»æ•°æ®"""
        source_cursor = source_conn.cursor()
        target_cursor = target_conn.cursor()
        
        # è·å–æºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–°é—»
        source_cursor.execute("SELECT * FROM news")
        news_items = source_cursor.fetchall()
        
        migrated_count = 0
        
        for news_item in news_items:
            try:
                # è½¬æ¢ä¸ºå­—å…¸
                news_dict = dict(news_item)
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé€šè¿‡URLæ£€æŸ¥ï¼‰
                target_cursor.execute("SELECT id FROM news WHERE url = ?", (news_dict['url'],))
                existing = target_cursor.fetchone()
                
                if existing:
                    continue  # è·³è¿‡å·²å­˜åœ¨çš„æ–°é—»
                
                # æ’å…¥æ–°é—»
                target_cursor.execute('''
                    INSERT OR REPLACE INTO news (
                        id, title, content, content_html, pub_time, author, 
                        source, url, keywords, images, related_stocks, 
                        sentiment, classification, category, crawl_time, summary, status
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    news_dict.get('id'),
                    news_dict.get('title'),
                    news_dict.get('content'),
                    news_dict.get('content_html'),
                    news_dict.get('pub_time'),
                    news_dict.get('author'),
                    news_dict.get('source'),
                    news_dict.get('url'),
                    news_dict.get('keywords'),
                    news_dict.get('images'),
                    news_dict.get('related_stocks'),
                    news_dict.get('sentiment'),
                    news_dict.get('classification'),
                    news_dict.get('category', 'è´¢ç»'),
                    news_dict.get('crawl_time'),
                    news_dict.get('summary'),
                    news_dict.get('status', 0)
                ))
                
                migrated_count += 1
                
            except Exception as e:
                logger.warning(f"è¿ç§»å•æ¡æ–°é—»å¤±è´¥: {e}")
                continue
        
        target_conn.commit()
        return migrated_count
    
    def verify_unified_database(self):
        """éªŒè¯ç»Ÿä¸€æ•°æ®åº“"""
        logger.info("ğŸ” éªŒè¯ç»Ÿä¸€æ•°æ®åº“...")
        
        try:
            from backend.newslook.core.unified_database_manager import get_unified_database_manager
            
            # æµ‹è¯•ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨
            unified_manager = get_unified_database_manager()
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = unified_manager.get_database_stats()
            
            logger.info(f"âœ… ç»Ÿä¸€æ•°æ®åº“éªŒè¯é€šè¿‡:")
            logger.info(f"   ä¸»æ•°æ®åº“: {stats['main_db']['path']}")
            logger.info(f"   æ–°é—»æ€»æ•°: {stats['total_news']}")
            logger.info(f"   æ•°æ®æºæ•°: {stats['main_db']['sources']}")
            
            # æµ‹è¯•ä¿å­˜åŠŸèƒ½
            test_news = {
                'title': 'æ•°æ®åº“æ¶æ„ä¿®å¤æµ‹è¯•æ–°é—»',
                'content': 'è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ–°é—»ï¼Œç”¨äºéªŒè¯æ•°æ®åº“æ¶æ„ä¿®å¤æ•ˆæœ',
                'url': f'http://test.example.com/news/{int(time.time())}',
                'source': 'ç³»ç»Ÿæµ‹è¯•',
                'pub_time': datetime.now()
            }
            
            success = unified_manager.save_news(test_news)
            if success:
                logger.info("âœ… ä¿å­˜éªŒè¯æµ‹è¯•é€šè¿‡")
            else:
                logger.error("âŒ ä¿å­˜éªŒè¯æµ‹è¯•å¤±è´¥")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€æ•°æ®åº“éªŒè¯å¤±è´¥: {e}")
            return False
    
    def cleanup_old_databases(self, old_db_files, confirm=True):
        """æ¸…ç†æ—§æ•°æ®åº“æ–‡ä»¶"""
        if not old_db_files:
            logger.info("ğŸ§¹ æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ•°æ®åº“æ–‡ä»¶")
            return
        
        if confirm:
            print(f"\nğŸ§¹ å‡†å¤‡æ¸…ç† {len(old_db_files)} ä¸ªæ—§æ•°æ®åº“æ–‡ä»¶:")
            for db_file in old_db_files:
                print(f"   - {db_file}")
            
            response = input("\næ˜¯å¦ç»§ç»­æ¸…ç†ï¼Ÿ(y/N): ")
            if response.lower() != 'y':
                logger.info("æ¸…ç†æ“ä½œå·²å–æ¶ˆ")
                return
        
        cleaned_count = 0
        for db_file in old_db_files:
            try:
                # å…ˆå¤‡ä»½
                backup_name = f"{db_file.name}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                backup_path = self.unified_db_dir / 'backups' / backup_name
                backup_path.parent.mkdir(exist_ok=True)
                
                import shutil
                shutil.copy2(str(db_file), str(backup_path))
                
                # åˆ é™¤åŸæ–‡ä»¶
                db_file.unlink()
                cleaned_count += 1
                
                logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†å¹¶å¤‡ä»½: {db_file.name}")
                
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†å¤±è´¥ {db_file}: {e}")
        
        logger.info(f"âœ… æˆåŠŸæ¸…ç† {cleaned_count} ä¸ªæ—§æ•°æ®åº“æ–‡ä»¶")
    
    def run_full_fix(self):
        """æ‰§è¡Œå®Œæ•´ä¿®å¤æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“æ¶æ„ä¿®å¤æµç¨‹...")
        
        # 1. å‘ç°æ—§æ•°æ®åº“
        old_db_files = self.discover_old_databases()
        
        if not old_db_files:
            logger.info("âœ… æœªå‘ç°éœ€è¦è¿ç§»çš„æ—§æ•°æ®åº“æ–‡ä»¶")
        else:
            # 2. åˆ†ææ—§æ•°æ®åº“
            total_news = 0
            for db_file in old_db_files:
                analysis = self.analyze_database_content(db_file)
                if analysis:
                    logger.info(f"ğŸ“Š {db_file.name}: {analysis.get('news_count', 0)} æ¡æ–°é—»")
                    total_news += analysis.get('news_count', 0)
            
            # 3. è¿ç§»æ•°æ®
            logger.info(f"ğŸ”„ å¼€å§‹è¿ç§» {len(old_db_files)} ä¸ªæ•°æ®åº“ï¼Œæ€»è®¡çº¦ {total_news} æ¡æ–°é—»...")
            
            total_migrated = 0
            for db_file in old_db_files:
                migrated = self.migrate_database_content(db_file, self.main_db_path)
                total_migrated += migrated
            
            logger.info(f"âœ… è¿ç§»å®Œæˆï¼ŒæˆåŠŸè¿ç§» {total_migrated} æ¡æ–°é—»")
        
        # 4. éªŒè¯ç»Ÿä¸€æ•°æ®åº“
        if self.verify_unified_database():
            logger.info("âœ… æ•°æ®åº“æ¶æ„ä¿®å¤æˆåŠŸ!")
            
            # 5. æ¸…ç†æ—§æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            if old_db_files:
                self.cleanup_old_databases(old_db_files, confirm=True)
        else:
            logger.error("âŒ æ•°æ®åº“æ¶æ„ä¿®å¤éªŒè¯å¤±è´¥")
            return False
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ NewsLook æ•°æ®åº“æ¶æ„ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    fixer = DatabaseArchitectureFixer()
    
    try:
        success = fixer.run_full_fix()
        
        if success:
            print("\nğŸ‰ æ•°æ®åº“æ¶æ„ä¿®å¤å®Œæˆ!")
            print("ğŸ”§ ä¿®å¤æ•ˆæœ:")
            print("   âœ… è·¯å¾„ç»Ÿä¸€åŒ–ï¼šæ‰€æœ‰æ¨¡å—ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“è·¯å¾„")
            print("   âœ… è¿æ¥æ± åŒ–ç®¡ç†ï¼šè‡ªåŠ¨è¿æ¥å›æ”¶å’Œå¤ç”¨")
            print("   âœ… äº‹åŠ¡å¢å¼ºï¼šé˜²é”ç«äº‰å’Œå¤±è´¥å›æ»š")
            print("   âœ… ä¿å­˜éªŒè¯ï¼šæ’å…¥åç«‹å³æ ¡éªŒ")
            print("   âœ… é”™è¯¯ç†”æ–­ï¼šè‡ªåŠ¨é‡è¯•å’Œå‘Šè­¦æœºåˆ¶")
        else:
            print("\nâŒ æ•°æ®åº“æ¶æ„ä¿®å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ä¿®å¤è¿‡ç¨‹è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ ä¿®å¤è¿‡ç¨‹å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}", exc_info=True)
        print(f"\nâŒ ä¿®å¤å¤±è´¥: {e}")

if __name__ == "__main__":
    main() 