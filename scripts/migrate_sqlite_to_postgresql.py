#!/usr/bin/env python3
"""
SQLiteåˆ°PostgreSQLæ•°æ®è¿ç§»è„šæœ¬
å®ç°å¤šæºSQLiteæ•°æ®åº“åˆ°ç»Ÿä¸€PostgreSQLçš„è¿ç§»

ä½¿ç”¨æ–¹æ³•:
python scripts/migrate_sqlite_to_postgresql.py

åŠŸèƒ½:
1. æ‰«ææ‰€æœ‰SQLiteæ•°æ®åº“æ–‡ä»¶
2. æå–å’Œè½¬æ¢æ•°æ®
3. æ‰¹é‡è¿ç§»åˆ°PostgreSQL
4. éªŒè¯æ•°æ®å®Œæ•´æ€§
5. ç”Ÿæˆè¿ç§»æŠ¥å‘Š
"""

import os
import sys
import sqlite3
import asyncio
import time
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timezone
import hashlib
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from backend.newslook.databases.postgresql_manager import (
    PostgreSQLManager, DatabaseConfig, NewsRecord
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('data/logs/migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SQLiteExtractor:
    """SQLiteæ•°æ®æå–å™¨"""
    
    def __init__(self):
        self.source_mapping = {
            "eastmoney": "eastmoney",
            "sina": "sina", 
            "netease": "netease",
            "ifeng": "ifeng",
            "tencent": "tencent",
            "è…¾è®¯è´¢ç»": "tencent",
            "ä¸œæ–¹è´¢å¯Œ": "eastmoney",
            "æ–°æµªè´¢ç»": "sina",
            "ç½‘æ˜“è´¢ç»": "netease",
            "å‡¤å‡°è´¢ç»": "ifeng"
        }
        
    def discover_sqlite_files(self) -> List[Tuple[str, str]]:
        """å‘ç°SQLiteæ–‡ä»¶å¹¶æ˜ å°„åˆ°æºä»£ç """
        data_dir = project_root / "data"
        discovered = []
        
        # æœç´¢æ¨¡å¼
        patterns = ["*.db", "*.sqlite", "*.sqlite3"]
        
        for pattern in patterns:
            for db_file in data_dir.rglob(pattern):
                # è·³è¿‡å¤‡ä»½å’Œæµ‹è¯•æ–‡ä»¶
                if any(skip in str(db_file).lower() for skip in ['backup', 'bak', 'temp', 'tmp', 'test']):
                    continue
                    
                # å°è¯•ä»æ–‡ä»¶åæ¨æ–­æºä»£ç 
                file_name = db_file.stem.lower()
                source_code = None
                
                for keyword, code in self.source_mapping.items():
                    if keyword.lower() in file_name:
                        source_code = code
                        break
                
                if not source_code:
                    # é»˜è®¤å¤„ç†
                    if 'finance' in file_name:
                        source_code = 'unknown'
                    else:
                        source_code = file_name.replace('_', '').replace('-', '')
                
                discovered.append((str(db_file), source_code))
                
        return discovered
        
    def extract_from_sqlite(self, db_path: str, source_code: str) -> List[Dict]:
        """ä»SQLiteæ•°æ®åº“æå–æ•°æ®"""
        try:
            conn = sqlite3.connect(db_path, timeout=30)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # æ£€æŸ¥è¡¨ç»“æ„
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            logger.info(f"æ•°æ®åº“ {db_path} åŒ…å«è¡¨: {tables}")
            
            # å°è¯•ä¸åŒçš„è¡¨åæ¨¡å¼
            news_table = None
            for table_name in ['news', 'finance_news', 'articles', 'data']:
                if table_name in tables:
                    news_table = table_name
                    break
                    
            if not news_table:
                logger.warning(f"åœ¨æ•°æ®åº“ {db_path} ä¸­æœªæ‰¾åˆ°æ–°é—»è¡¨")
                return []
                
            # æ£€æŸ¥è¡¨ç»“æ„
            cursor.execute(f"PRAGMA table_info({news_table})")
            columns = {row[1]: row[2] for row in cursor.fetchall()}
            logger.info(f"è¡¨ {news_table} çš„åˆ—: {list(columns.keys())}")
            
            # æ„å»ºæŸ¥è¯¢SQL
            select_columns = []
            column_mapping = {
                'title': ['title', 'headline', 'subject'],
                'content': ['content', 'body', 'text', 'article'],
                'url': ['url', 'link', 'source_url'],
                'published_at': ['published_at', 'pub_time', 'publish_time', 'date', 'created_at'],
                'author': ['author', 'writer'],
                'category': ['category', 'type', 'section'],
                'summary': ['summary', 'abstract', 'description']
            }
            
            field_map = {}
            for target, candidates in column_mapping.items():
                for candidate in candidates:
                    if candidate in columns:
                        field_map[target] = candidate
                        break
                        
            # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
            required_fields = ['title', 'url']
            for field in required_fields:
                if field not in field_map:
                    logger.error(f"ç¼ºå°‘å¿…éœ€å­—æ®µ {field} åœ¨è¡¨ {news_table}")
                    return []
                    
            # æ„å»ºæŸ¥è¯¢
            query = f"SELECT * FROM {news_table} ORDER BY rowid"
            cursor.execute(query)
            
            records = []
            for row in cursor.fetchall():
                row_dict = dict(row)
                
                # æ•°æ®è½¬æ¢å’Œæ¸…ç†
                record = {
                    'title': str(row_dict.get(field_map.get('title', 'title'), '')).strip(),
                    'content': str(row_dict.get(field_map.get('content', 'content'), '')).strip(),
                    'url': str(row_dict.get(field_map.get('url', 'url'), '')).strip(),
                    'source_code': source_code,
                    'published_at': self._parse_datetime(row_dict.get(field_map.get('published_at'))),
                    'crawled_at': datetime.now(timezone.utc),
                    'author': str(row_dict.get(field_map.get('author', 'author'), '') or '').strip(),
                    'category': str(row_dict.get(field_map.get('category', 'category'), 'è´¢ç»')).strip(),
                    'summary': str(row_dict.get(field_map.get('summary', 'summary'), '') or '').strip(),
                    'raw_data': row_dict  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºè°ƒè¯•
                }
                
                # æ•°æ®éªŒè¯
                if not record['title'] or not record['url']:
                    continue
                    
                # URLæ¸…ç†
                if not record['url'].startswith('http'):
                    continue
                    
                records.append(record)
                
            logger.info(f"ä» {db_path} æå–äº† {len(records)} æ¡è®°å½•")
            return records
            
        except Exception as e:
            logger.error(f"æå–æ•°æ®å¤±è´¥ {db_path}: {e}")
            return []
        finally:
            if 'conn' in locals():
                conn.close()
                
    def _parse_datetime(self, dt_value) -> datetime:
        """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
        if not dt_value:
            return datetime.now(timezone.utc)
            
        if isinstance(dt_value, (int, float)):
            # Unixæ—¶é—´æˆ³
            try:
                return datetime.fromtimestamp(dt_value, tz=timezone.utc)
            except (ValueError, OSError):
                return datetime.now(timezone.utc)
                
        if isinstance(dt_value, str):
            # å¸¸è§æ—¥æœŸæ ¼å¼
            formats = [
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d %H:%M:%S.%f',
                '%Y-%m-%d',
                '%Y/%m/%d %H:%M:%S',
                '%Y/%m/%d',
                '%m/%d/%Y %H:%M:%S',
                '%m/%d/%Y'
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(dt_value, fmt)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    return dt
                except ValueError:
                    continue
                    
        return datetime.now(timezone.utc)

class MigrationManager:
    """è¿ç§»ç®¡ç†å™¨"""
    
    def __init__(self):
        self.extractor = SQLiteExtractor()
        self.pg_manager: Optional[PostgreSQLManager] = None
        self.migration_stats = {
            'start_time': None,
            'end_time': None,
            'sqlite_files_processed': 0,
            'total_records_found': 0,
            'records_migrated': 0,
            'records_skipped': 0,
            'errors': []
        }
        
    async def initialize_postgresql(self):
        """åˆå§‹åŒ–PostgreSQLè¿æ¥"""
        config = DatabaseConfig(
            host=os.getenv('PG_HOST', 'localhost'),
            port=int(os.getenv('PG_PORT', 5432)),
            database=os.getenv('PG_DATABASE', 'newslook'),
            username=os.getenv('PG_USERNAME', 'newslook'),
            password=os.getenv('PG_PASSWORD', 'newslook123')
        )
        
        self.pg_manager = PostgreSQLManager(config)
        await self.pg_manager.initialize()
        logger.info("PostgreSQLè¿æ¥åˆå§‹åŒ–å®Œæˆ")
        
    async def migrate_all_data(self):
        """è¿ç§»æ‰€æœ‰æ•°æ®"""
        self.migration_stats['start_time'] = time.time()
        
        try:
            # 1. åˆå§‹åŒ–PostgreSQL
            await self.initialize_postgresql()
            
            # 2. å‘ç°SQLiteæ–‡ä»¶
            print("ğŸ” æ­£åœ¨å‘ç°SQLiteæ•°æ®åº“æ–‡ä»¶...")
            sqlite_files = self.extractor.discover_sqlite_files()
            
            if not sqlite_files:
                print("âŒ æœªå‘ç°ä»»ä½•SQLiteæ•°æ®åº“æ–‡ä»¶")
                return
                
            print(f"âœ… å‘ç° {len(sqlite_files)} ä¸ªSQLiteæ•°æ®åº“:")
            for db_path, source_code in sqlite_files:
                size_mb = Path(db_path).stat().st_size / 1024 / 1024
                print(f"   - {db_path} [{source_code}] ({size_mb:.2f} MB)")
                
            # 3. é€ä¸ªè¿ç§»æ•°æ®åº“
            print("\nğŸš€ å¼€å§‹æ•°æ®è¿ç§»...")
            
            for db_path, source_code in sqlite_files:
                await self._migrate_single_database(db_path, source_code)
                self.migration_stats['sqlite_files_processed'] += 1
                
            # 4. éªŒè¯è¿ç§»ç»“æœ
            await self._verify_migration()
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            await self._generate_migration_report()
            
        except Exception as e:
            logger.error(f"è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.migration_stats['errors'].append(str(e))
            
        finally:
            self.migration_stats['end_time'] = time.time()
            if self.pg_manager:
                await self.pg_manager.close()
                
    async def _migrate_single_database(self, db_path: str, source_code: str):
        """è¿ç§»å•ä¸ªæ•°æ®åº“"""
        print(f"\nğŸ“¦ æ­£åœ¨è¿ç§»: {Path(db_path).name} [{source_code}]")
        
        try:
            # æå–æ•°æ®
            records = self.extractor.extract_from_sqlite(db_path, source_code)
            self.migration_stats['total_records_found'] += len(records)
            
            if not records:
                print(f"   âš ï¸  æ— æœ‰æ•ˆæ•°æ®")
                return
                
            # è·å–æºID
            source_id = self.pg_manager.get_source_id(source_code)
            
            # è½¬æ¢ä¸ºNewsRecordå¯¹è±¡
            news_records = []
            for record in records:
                try:
                    news_record = NewsRecord(
                        title=record['title'],
                        content=record['content'],
                        url=record['url'],
                        source_id=source_id,
                        published_at=record['published_at'],
                        crawled_at=record['crawled_at'],
                        category=record['category'],
                        author=record['author'] if record['author'] else None,
                        summary=record['summary'] if record['summary'] else None,
                        metadata={'migrated_from': db_path, 'original_data': record['raw_data']}
                    )
                    news_records.append(news_record)
                except Exception as e:
                    logger.warning(f"è®°å½•è½¬æ¢å¤±è´¥: {e}")
                    self.migration_stats['records_skipped'] += 1
                    
            # æ‰¹é‡æ’å…¥
            if news_records:
                print(f"   ğŸ“ æ­£åœ¨æ’å…¥ {len(news_records)} æ¡è®°å½•...")
                
                # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜é—®é¢˜
                batch_size = 500
                for i in range(0, len(news_records), batch_size):
                    batch = news_records[i:i + batch_size]
                    await self.pg_manager.batch_insert_news(batch)
                    
                self.migration_stats['records_migrated'] += len(news_records)
                print(f"   âœ… æˆåŠŸè¿ç§» {len(news_records)} æ¡è®°å½•")
            else:
                print(f"   âš ï¸  æ— æœ‰æ•ˆè®°å½•å¯è¿ç§»")
                
        except Exception as e:
            error_msg = f"è¿ç§»æ•°æ®åº“å¤±è´¥ {db_path}: {e}"
            logger.error(error_msg)
            self.migration_stats['errors'].append(error_msg)
            print(f"   âŒ è¿ç§»å¤±è´¥: {e}")
            
    async def _verify_migration(self):
        """éªŒè¯è¿ç§»ç»“æœ"""
        print("\nğŸ” æ­£åœ¨éªŒè¯è¿ç§»ç»“æœ...")
        
        try:
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = await self.pg_manager.get_statistics(days=365)
            
            total_in_pg = sum(stat['total_count'] for stat in stats['daily_stats'])
            
            print(f"   ğŸ“Š PostgreSQLä¸­æ€»è®°å½•æ•°: {total_in_pg}")
            print(f"   ğŸ“Š é¢„æœŸè¿ç§»è®°å½•æ•°: {self.migration_stats['records_migrated']}")
            
            if total_in_pg >= self.migration_stats['records_migrated'] * 0.95:  # å…è®¸5%è¯¯å·®
                print("   âœ… æ•°æ®éªŒè¯é€šè¿‡")
            else:
                print("   âš ï¸  æ•°æ®éªŒè¯å‘ç°å·®å¼‚ï¼Œè¯·æ£€æŸ¥")
                
        except Exception as e:
            logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            
    async def _generate_migration_report(self):
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        duration = self.migration_stats['end_time'] - self.migration_stats['start_time']
        
        report = f"""
======================================
        SQLiteåˆ°PostgreSQLè¿ç§»æŠ¥å‘Š
======================================

è¿ç§»æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ€»è€—æ—¶: {duration:.2f} ç§’

æ–‡ä»¶ç»Ÿè®¡:
- å¤„ç†çš„SQLiteæ–‡ä»¶æ•°: {self.migration_stats['sqlite_files_processed']}
- å‘ç°çš„è®°å½•æ€»æ•°: {self.migration_stats['total_records_found']}

æ•°æ®ç»Ÿè®¡:
- æˆåŠŸè¿ç§»è®°å½•æ•°: {self.migration_stats['records_migrated']}
- è·³è¿‡è®°å½•æ•°: {self.migration_stats['records_skipped']}
- è¿ç§»æˆåŠŸç‡: {(self.migration_stats['records_migrated'] / max(self.migration_stats['total_records_found'], 1) * 100):.1f}%

æ€§èƒ½æŒ‡æ ‡:
- è¿ç§»é€Ÿåº¦: {(self.migration_stats['records_migrated'] / max(duration, 1)):.1f} è®°å½•/ç§’

é”™è¯¯ä¿¡æ¯:
{chr(10).join(self.migration_stats['errors']) if self.migration_stats['errors'] else 'æ— é”™è¯¯'}

======================================
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = project_root / "data/logs/migration_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(report)
        print(f"ğŸ“„ è¿ç§»æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ NewsLook SQLiteåˆ°PostgreSQLè¿ç§»å·¥å…·")
    print("=" * 60)
    
    migration_manager = MigrationManager()
    
    # æ£€æŸ¥PostgreSQLè¿æ¥
    print("ğŸ”§ æ­£åœ¨æ£€æŸ¥PostgreSQLè¿æ¥...")
    try:
        await migration_manager.initialize_postgresql()
        print("âœ… PostgreSQLè¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ PostgreSQLè¿æ¥å¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·ç¡®ä¿:")
        print("   1. PostgreSQLæœåŠ¡å·²å¯åŠ¨")
        print("   2. æ•°æ®åº“é…ç½®æ­£ç¡®")
        print("   3. ç”¨æˆ·æƒé™å……è¶³")
        return
        
    # æ‰§è¡Œè¿ç§»
    await migration_manager.migrate_all_data()
    
    print("\nğŸ‰ æ•°æ®è¿ç§»å®Œæˆï¼")
    print("ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("   1. éªŒè¯åº”ç”¨åŠŸèƒ½")
    print("   2. æ›´æ–°åº”ç”¨é…ç½®ä½¿ç”¨PostgreSQL")
    print("   3. å¤‡ä»½SQLiteæ–‡ä»¶åˆ°å½’æ¡£ç›®å½•")

if __name__ == "__main__":
    asyncio.run(main()) 