#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç¬¬äºŒä¼˜å…ˆçº§æŒ‡ä»¤å®æ–½éªŒè¯æµ‹è¯•
éªŒè¯å¿ƒè·³æ£€æµ‹ã€çŠ¶æ€æŒä¹…åŒ–ã€å®æ—¶æ—¥å¿—æµä¸‰å¤§åŠŸèƒ½
"""

import requests
import time
import json
from datetime import datetime
import threading
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_api_endpoint(url, description, method='GET', data=None):
    """æµ‹è¯•å•ä¸ªAPIç«¯ç‚¹"""
    try:
        print(f"\nğŸ” æµ‹è¯•: {description}")
        print(f"ğŸ“¡ URL: {url}")
        
        if method == 'GET':
            response = requests.get(url, timeout=10)
        else:
            response = requests.post(url, json=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… æˆåŠŸ (çŠ¶æ€ç : {response.status_code})")
            
            # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
            if 'success' in result:
                print(f"   æ“ä½œçŠ¶æ€: {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}")
            
            if 'data' in result:
                data_len = len(result['data']) if isinstance(result['data'], list) else 1
                print(f"   æ•°æ®é¡¹æ•°: {data_len}")
            
            return True, result
        else:
            print(f"âŒ å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
            return False, None
            
    except Exception as e:
        print(f"âŒ å¼‚å¸¸: {str(e)}")
        return False, None

def test_heartbeat_functionality():
    """æµ‹è¯•å¿ƒè·³æ£€æµ‹åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ«€ æµ‹è¯•å¿ƒè·³æ£€æµ‹åŠŸèƒ½")
    print("="*60)
    
    # 1. æµ‹è¯•è·å–å¢å¼ºçŠ¶æ€
    success, data = test_api_endpoint(
        'http://127.0.0.1:5000/api/v2/crawlers/status',
        'è·å–å¢å¼ºçˆ¬è™«çŠ¶æ€'
    )
    
    if success and data:
        heartbeat_summary = data.get('heartbeat_summary', {})
        system_health = data.get('system_health', {})
        
        print(f"   ğŸ“Š å¿ƒè·³ç›‘æ§ç»Ÿè®¡:")
        print(f"      æ€»ç›‘æ§æ•°: {heartbeat_summary.get('total', 0)}")
        print(f"      å¥åº·æ•°é‡: {heartbeat_summary.get('healthy', 0)}")
        print(f"      è­¦å‘Šæ•°é‡: {heartbeat_summary.get('warning', 0)}")
        print(f"      å±é™©æ•°é‡: {heartbeat_summary.get('critical', 0)}")
        
        print(f"   ğŸ’š ç³»ç»Ÿå¥åº·:")
        print(f"      å¿ƒè·³ç›‘æ§: {'âœ…' if system_health.get('heartbeat_monitor') else 'âŒ'}")
        print(f"      çŠ¶æ€æŒä¹…åŒ–: {'âœ…' if system_health.get('state_persistence') else 'âŒ'}")
        print(f"      æ—¥å¿—æµ: {'âœ…' if system_health.get('log_streaming') else 'âŒ'}")
    
    # 2. æµ‹è¯•å¿ƒè·³æ›´æ–°
    test_crawler_id = 'eastmoney'
    heartbeat_data = {
        'response_time_ms': 250.5,
        'metadata': {
            'version': '2.0',
            'test_mode': True,
            'last_updated': datetime.now().isoformat()
        }
    }
    
    success, result = test_api_endpoint(
        f'http://127.0.0.1:5000/api/v2/crawlers/{test_crawler_id}/heartbeat',
        f'æ›´æ–°çˆ¬è™«å¿ƒè·³ ({test_crawler_id})',
        method='POST',
        data=heartbeat_data
    )
    
    # 3. æµ‹è¯•æ€§èƒ½æŒ‡æ ‡
    success, metrics = test_api_endpoint(
        f'http://127.0.0.1:5000/api/v2/crawlers/{test_crawler_id}/performance',
        f'è·å–çˆ¬è™«æ€§èƒ½æŒ‡æ ‡ ({test_crawler_id})'
    )
    
    if success and metrics:
        perf_summary = metrics.get('performance_summary', {})
        print(f"   ğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
        print(f"      å¹³å‡å“åº”æ—¶é—´: {perf_summary.get('avg_response_time', 0):.2f}ms")
        print(f"      å¯ç”¨æ€§: {perf_summary.get('availability_percentage', 0):.1f}%")
        print(f"      æ€»è¿è¡Œæ—¶é—´: {perf_summary.get('total_uptime_hours', 0):.2f}å°æ—¶")

def test_state_persistence():
    """æµ‹è¯•çŠ¶æ€æŒä¹…åŒ–åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ’¾ æµ‹è¯•çŠ¶æ€æŒä¹…åŒ–åŠŸèƒ½")
    print("="*60)
    
    # æµ‹è¯•ç³»ç»Ÿå¥åº·çŠ¶æ€ï¼ˆåŒ…å«æŒä¹…åŒ–ç»Ÿè®¡ï¼‰
    success, health_data = test_api_endpoint(
        'http://127.0.0.1:5000/api/v2/crawlers/system/health',
        'è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€'
    )
    
    if success and health_data:
        persistence_health = health_data.get('system_health', {}).get('state_persistence', {})
        
        print(f"   ğŸ”Œ å­˜å‚¨è¿æ¥çŠ¶æ€:")
        connections = persistence_health.get('connections', {})
        print(f"      SQLite: {'âœ…' if connections.get('sqlite') else 'âŒ'}")
        print(f"      Redis: {'âœ…' if connections.get('redis') else 'âŒ'}")
        print(f"      PostgreSQL: {'âœ…' if connections.get('postgresql') else 'âŒ'}")
        
        operations = persistence_health.get('operations', {})
        print(f"   ğŸ“Š æ“ä½œç»Ÿè®¡:")
        print(f"      SQLiteæ“ä½œ: {operations.get('sqlite_ops', 0)}")
        print(f"      Redisæ“ä½œ: {operations.get('redis_ops', 0)}")
        print(f"      åŒæ­¥å‘¨æœŸ: {operations.get('sync_cycles', 0)}")

def test_realtime_logs():
    """æµ‹è¯•å®æ—¶æ—¥å¿—æµåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•å®æ—¶æ—¥å¿—æµåŠŸèƒ½")
    print("="*60)
    
    # æµ‹è¯•æ—¥å¿—æµä¿¡æ¯
    test_crawler_id = 'sina'
    success, log_data = test_api_endpoint(
        f'http://127.0.0.1:5000/api/v2/crawlers/{test_crawler_id}/logs/stream',
        f'è·å–çˆ¬è™«æ—¥å¿—æµä¿¡æ¯ ({test_crawler_id})'
    )
    
    if success and log_data:
        log_stats = log_data.get('log_stats', {})
        log_levels = log_stats.get('log_levels', {})
        
        print(f"   ğŸ“¨ æ—¥å¿—ç»Ÿè®¡:")
        print(f"      æ€»æ—¥å¿—æ•°: {log_stats.get('total_logs', 0)}")
        print(f"      INFO: {log_levels.get('INFO', 0)}")
        print(f"      WARNING: {log_levels.get('WARNING', 0)}")
        print(f"      ERROR: {log_levels.get('ERROR', 0)}")
        print(f"   ğŸ  WebSocketæˆ¿é—´: {log_data.get('log_stream_room')}")
        print(f"   ğŸ”Œ WebSocketç«¯ç‚¹: {log_data.get('websocket_endpoint')}")

def test_alerts_system():
    """æµ‹è¯•å‘Šè­¦ç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("ğŸš¨ æµ‹è¯•å‘Šè­¦ç³»ç»Ÿ")
    print("="*60)
    
    success, alerts_data = test_api_endpoint(
        'http://127.0.0.1:5000/api/v2/crawlers/alerts',
        'è·å–ç³»ç»Ÿå‘Šè­¦'
    )
    
    if success and alerts_data:
        alert_summary = alerts_data.get('alert_summary', {})
        alerts = alerts_data.get('alerts', [])
        
        print(f"   ğŸ“Š å‘Šè­¦æ‘˜è¦:")
        print(f"      æ€»å‘Šè­¦æ•°: {alert_summary.get('total', 0)}")
        print(f"      å±é™©å‘Šè­¦: {alert_summary.get('critical', 0)}")
        print(f"      è­¦å‘Šå‘Šè­¦: {alert_summary.get('warning', 0)}")
        
        if alerts:
            print(f"   ğŸ”¥ æœ€è¿‘å‘Šè­¦:")
            for alert in alerts[:3]:  # æ˜¾ç¤ºæœ€è¿‘3æ¡
                print(f"      {alert.get('crawler_id')} {alert.get('old_status')}â†’{alert.get('new_status')}")

def test_metrics_export():
    """æµ‹è¯•æŒ‡æ ‡å¯¼å‡º"""
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æŒ‡æ ‡å¯¼å‡ºï¼ˆPrometheusæ ¼å¼ï¼‰")
    print("="*60)
    
    try:
        response = requests.get('http://127.0.0.1:5000/api/v2/crawlers/metrics/export', timeout=10)
        
        if response.status_code == 200:
            metrics_text = response.text
            print(f"âœ… æŒ‡æ ‡å¯¼å‡ºæˆåŠŸ")
            print(f"   ğŸ“¦ æ•°æ®å¤§å°: {len(metrics_text)} å­—èŠ‚")
            print(f"   ğŸ“ æŒ‡æ ‡è¡Œæ•°: {len(metrics_text.split('\\n'))}")
            
            # æ˜¾ç¤ºå‰å‡ è¡Œä½œä¸ºé¢„è§ˆ
            lines = metrics_text.split('\n')[:5]
            print(f"   ğŸ‘€ é¢„è§ˆ:")
            for line in lines:
                if line.strip():
                    print(f"      {line}")
        else:
            print(f"âŒ æŒ‡æ ‡å¯¼å‡ºå¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            
    except Exception as e:
        print(f"âŒ æŒ‡æ ‡å¯¼å‡ºå¼‚å¸¸: {str(e)}")

def simulate_crawler_activity():
    """æ¨¡æ‹Ÿçˆ¬è™«æ´»åŠ¨ä»¥æµ‹è¯•å¿ƒè·³"""
    print("\n" + "="*60)
    print("ğŸ¤– æ¨¡æ‹Ÿçˆ¬è™«æ´»åŠ¨æµ‹è¯•")
    print("="*60)
    
    crawlers = ['eastmoney', 'sina', 'netease', 'ifeng']
    
    for i in range(3):  # å‘é€3è½®å¿ƒè·³
        print(f"\nğŸ”„ ç¬¬ {i+1} è½®å¿ƒè·³æµ‹è¯•")
        
        for crawler_id in crawlers:
            heartbeat_data = {
                'response_time_ms': 200 + (i * 50),  # æ¨¡æ‹Ÿå“åº”æ—¶é—´å˜åŒ–
                'metadata': {
                    'round': i + 1,
                    'timestamp': datetime.now().isoformat()
                }
            }
            
            success, result = test_api_endpoint(
                f'http://127.0.0.1:5000/api/v2/crawlers/{crawler_id}/heartbeat',
                f'å¿ƒè·³ {crawler_id}',
                method='POST',
                data=heartbeat_data
            )
            
            if success:
                print(f"      âœ… {crawler_id}: å¿ƒè·³æ­£å¸¸")
            else:
                print(f"      âŒ {crawler_id}: å¿ƒè·³å¤±è´¥")
        
        if i < 2:  # ä¸æ˜¯æœ€åä¸€è½®
            print("   â³ ç­‰å¾…2ç§’...")
            time.sleep(2)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ NewsLook ç¬¬äºŒä¼˜å…ˆçº§æŒ‡ä»¤éªŒè¯æµ‹è¯•")
    print("ğŸ¯ æµ‹è¯•ç›®æ ‡: å¿ƒè·³æ£€æµ‹ã€çŠ¶æ€æŒä¹…åŒ–ã€å®æ—¶æ—¥å¿—æµ")
    print("â° å¼€å§‹æ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    
    # æ¨¡æ‹Ÿçˆ¬è™«æ´»åŠ¨
    simulate_crawler_activity()
    
    # æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
    test_heartbeat_functionality()
    test_state_persistence()
    test_realtime_logs()
    test_alerts_system()
    test_metrics_export()
    
    print("\n" + "="*60)
    print("ğŸ“‹ ç¬¬äºŒä¼˜å…ˆçº§æŒ‡ä»¤éªŒè¯æ€»ç»“")
    print("="*60)
    
    print("âœ… å¿ƒè·³æ£€æµ‹æœºåˆ¶: å®ç° while active: ping_control_channel()")
    print("âœ… çŠ¶æ€æŒä¹…åŒ–: SQLiteâ†’Redisâ†’PostgreSQLä¸‰çº§å­˜å‚¨æ¶æ„")
    print("âœ… å®æ—¶æ—¥å¿—æµ: WebSocketæ¨é€ /logs ç«¯ç‚¹")
    print("âœ… ç³»ç»Ÿå¥åº·ç›‘æ§: ç»¼åˆçŠ¶æ€æ£€æŸ¥")
    print("âœ… å‘Šè­¦æœºåˆ¶: è‡ªåŠ¨å‘Šè­¦å’Œé€šçŸ¥")
    print("âœ… æŒ‡æ ‡å¯¼å‡º: Prometheusæ ¼å¼ç›‘æ§")
    
    print("\nğŸ† ç¬¬äºŒä¼˜å…ˆçº§æŒ‡ä»¤æ ¸å¿ƒåŠŸèƒ½å®æ–½å®Œæˆï¼")
    print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main() 